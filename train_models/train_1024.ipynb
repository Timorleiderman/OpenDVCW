{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/OpenDVCW/train_models'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/OpenDVCW\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/ubu-admin/Developer/tensorflow-wavelets\n",
    "%cd /workspaces/OpenDVCW\n",
    "import OpenDVCW\n",
    "import numpy as np\n",
    "import load\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import DataGen\n",
    "import Callbacks\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 700\n",
    "STEPS_PER_EPOCH = 200\n",
    "Height = 240\n",
    "Width = 240\n",
    "Channel = 3\n",
    "lmbda = 1024\n",
    "lr_init = 1e-4\n",
    "early_stop = 15\n",
    "I_QP=27\n",
    "\n",
    "args = OpenDVCW.Arguments()\n",
    "last = 0\n",
    "checkponts_last_path = \"checkpoints_wavelets_L_{}_{}_{}x{}/\".format(lmbda, last, Width, Height)\n",
    "checkponts_new_path = \"checkpoints_wavelets_L_{}_{}_{}x{}/\".format(lmbda, last+1, Width, Height)\n",
    "save_name = \"model_save_wavelets_L_{}_{}_{}x{}\".format(lmbda, last+1, Width, Height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 04:59:21.246492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 04:59:21.297814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 04:59:21.298643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 04:59:21.306268: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-05 04:59:21.317112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 04:59:21.318247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 04:59:21.319059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 04:59:24.958494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 04:59:24.959547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 04:59:24.960391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 04:59:24.961506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10244 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:00:09.0, compute capability: 8.6\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "* [Loading dataset]...\n"
     ]
    }
   ],
   "source": [
    "model = OpenDVCW.OpenDVC(width=Width, height=Height, batch_size=BATCH_SIZE, num_filters=128, lmbda=lmbda)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_init),)\n",
    "print(\"* [Model compiled]...\")\n",
    "\n",
    "print(\"* [Loading dataset]...\")\n",
    "data = DataGen.DataVimeo90kGenerator(\"folder_cloud_test.npy\", \n",
    "                                    BATCH_SIZE,\n",
    "                                    (Height,Width,Channel),\n",
    "                                    Channel,\n",
    "                                    True, \n",
    "                                    I_QP,\n",
    "                                    True)\n",
    "\n",
    "# print(\"Loading weights\")\n",
    "# model.load_weights(checkponts_last_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].trainable = False\n",
    "# model.layers[1].trainable = False\n",
    "# model.layers[2].trainable = True\n",
    "# model.layers[3].trainable = True\n",
    "# model.layers[4].trainable = False\n",
    "# model.layers[5].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv_analysis True\n",
      "mv_synthesis True\n",
      "res_analysis True\n",
      "res_synthesis True\n",
      "wavelets_optical_flow True\n",
      "motion_compensation True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 05:00:00.235806: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-04-05 05:00:43.603944: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/200 [..............................] - ETA: 56s - loss: 62.1167 - bpp: 5.3752 - mse: 0.0554WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2777s vs `on_train_batch_end` time: 0.2894s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2777s vs `on_train_batch_end` time: 0.2894s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA: 0s - loss: 16.3186 - bpp: 5.3043 - mse: 0.0108[MemoryCallback]:  4838424\n",
      "\n",
      "Epoch 1: loss improved from inf to 16.31858, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 119s 265ms/step - loss: 16.3186 - bpp: 5.3043 - mse: 0.0108\n",
      "Epoch 2/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 7.8212 - bpp: 5.1647 - mse: 0.0026[MemoryCallback]:  4935084\n",
      "\n",
      "Epoch 2: loss improved from 16.31858 to 7.82117, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 56s 276ms/step - loss: 7.8212 - bpp: 5.1647 - mse: 0.0026\n",
      "Epoch 3/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 7.1008 - bpp: 5.0292 - mse: 0.0020[MemoryCallback]:  5027096\n",
      "\n",
      "Epoch 3: loss improved from 7.82117 to 7.10078, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 55s 270ms/step - loss: 7.1008 - bpp: 5.0292 - mse: 0.0020\n",
      "Epoch 4/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 6.3670 - bpp: 4.8961 - mse: 0.0014[MemoryCallback]:  5138028\n",
      "\n",
      "Epoch 4: loss improved from 7.10078 to 6.36705, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 52s 260ms/step - loss: 6.3670 - bpp: 4.8961 - mse: 0.0014\n",
      "Epoch 5/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 6.0233 - bpp: 4.7680 - mse: 0.0012[MemoryCallback]:  5198576\n",
      "\n",
      "Epoch 5: loss improved from 6.36705 to 6.02327, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 6.0233 - bpp: 4.7680 - mse: 0.0012\n",
      "Epoch 6/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.6510 - bpp: 4.6410 - mse: 9.8632e-04[MemoryCallback]:  5198576\n",
      "\n",
      "Epoch 6: loss improved from 6.02327 to 5.65099, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 53s 264ms/step - loss: 5.6510 - bpp: 4.6410 - mse: 9.8632e-04\n",
      "Epoch 7/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.4356 - bpp: 4.5155 - mse: 8.9855e-04[MemoryCallback]:  5234984\n",
      "\n",
      "Epoch 7: loss improved from 5.65099 to 5.43557, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 50s 248ms/step - loss: 5.4356 - bpp: 4.5155 - mse: 8.9855e-04\n",
      "Epoch 8/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.2137 - bpp: 4.3923 - mse: 8.0219e-04[MemoryCallback]:  5294908\n",
      "\n",
      "Epoch 8: loss improved from 5.43557 to 5.21370, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 5.2137 - bpp: 4.3923 - mse: 8.0219e-04\n",
      "Epoch 9/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.0643 - bpp: 4.2724 - mse: 7.7339e-04[MemoryCallback]:  5381760\n",
      "\n",
      "Epoch 9: loss improved from 5.21370 to 5.06430, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 5.0643 - bpp: 4.2724 - mse: 7.7339e-04\n",
      "Epoch 10/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.9047 - bpp: 4.1537 - mse: 7.3338e-04[MemoryCallback]:  5479492\n",
      "\n",
      "Epoch 10: loss improved from 5.06430 to 4.90469, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 4.9047 - bpp: 4.1537 - mse: 7.3338e-04\n",
      "Epoch 11/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.8280 - bpp: 4.0384 - mse: 7.7108e-04[MemoryCallback]:  5534912\n",
      "\n",
      "Epoch 11: loss improved from 4.90469 to 4.82802, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 47s 232ms/step - loss: 4.8280 - bpp: 4.0384 - mse: 7.7108e-04\n",
      "Epoch 12/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.5305 - bpp: 3.9235 - mse: 5.9275e-04[MemoryCallback]:  5584600\n",
      "\n",
      "Epoch 12: loss improved from 4.82802 to 4.53050, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 48s 237ms/step - loss: 4.5305 - bpp: 3.9235 - mse: 5.9275e-04\n",
      "Epoch 13/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.5270 - bpp: 3.8132 - mse: 6.9707e-04[MemoryCallback]:  5584828\n",
      "\n",
      "Epoch 13: loss improved from 4.53050 to 4.52704, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 47s 231ms/step - loss: 4.5270 - bpp: 3.8132 - mse: 6.9707e-04\n",
      "Epoch 14/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.2519 - bpp: 3.7029 - mse: 5.3616e-04[MemoryCallback]:  5585092\n",
      "\n",
      "Epoch 14: loss improved from 4.52704 to 4.25188, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 4.2519 - bpp: 3.7029 - mse: 5.3616e-04\n",
      "Epoch 15/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.1471 - bpp: 3.5951 - mse: 5.3910e-04[MemoryCallback]:  5634188\n",
      "\n",
      "Epoch 15: loss improved from 4.25188 to 4.14711, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 47s 232ms/step - loss: 4.1471 - bpp: 3.5951 - mse: 5.3910e-04\n",
      "Epoch 16/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.0160 - bpp: 3.4888 - mse: 5.1485e-04[MemoryCallback]:  5634528\n",
      "\n",
      "Epoch 16: loss improved from 4.14711 to 4.01603, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 4.0160 - bpp: 3.4888 - mse: 5.1485e-04\n",
      "Epoch 17/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.8789 - bpp: 3.3831 - mse: 4.8417e-04[MemoryCallback]:  5634532\n",
      "\n",
      "Epoch 17: loss improved from 4.01603 to 3.87894, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 3.8789 - bpp: 3.3831 - mse: 4.8417e-04\n",
      "Epoch 18/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.7175 - bpp: 3.2817 - mse: 4.2565e-04[MemoryCallback]:  5684684\n",
      "\n",
      "Epoch 18: loss improved from 3.87894 to 3.71753, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 3.7175 - bpp: 3.2817 - mse: 4.2565e-04\n",
      "Epoch 19/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.6499 - bpp: 3.1833 - mse: 4.5566e-04[MemoryCallback]:  5733248\n",
      "\n",
      "Epoch 19: loss improved from 3.71753 to 3.64989, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 3.6499 - bpp: 3.1833 - mse: 4.5566e-04\n",
      "Epoch 20/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.5605 - bpp: 3.0848 - mse: 4.6454e-04[MemoryCallback]:  5783220\n",
      "\n",
      "Epoch 20: loss improved from 3.64989 to 3.56049, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 3.5605 - bpp: 3.0848 - mse: 4.6454e-04\n",
      "Epoch 21/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.4204 - bpp: 2.9865 - mse: 4.2369e-04[MemoryCallback]:  5785956\n",
      "\n",
      "Epoch 21: loss improved from 3.56049 to 3.42037, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 3.4204 - bpp: 2.9865 - mse: 4.2369e-04\n",
      "Epoch 22/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2946 - bpp: 2.8894 - mse: 3.9578e-04[MemoryCallback]:  5833924\n",
      "\n",
      "Epoch 22: loss improved from 3.42037 to 3.29465, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 43s 212ms/step - loss: 3.2946 - bpp: 2.8894 - mse: 3.9578e-04\n",
      "Epoch 23/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2082 - bpp: 2.7976 - mse: 4.0098e-04[MemoryCallback]:  5885704\n",
      "\n",
      "Epoch 23: loss improved from 3.29465 to 3.20824, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 3.2082 - bpp: 2.7976 - mse: 4.0098e-04\n",
      "Epoch 24/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.1432 - bpp: 2.7147 - mse: 4.1853e-04[MemoryCallback]:  5923828\n",
      "\n",
      "Epoch 24: loss improved from 3.20824 to 3.14324, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 3.1432 - bpp: 2.7147 - mse: 4.1853e-04\n",
      "Epoch 25/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.9993 - bpp: 2.6167 - mse: 3.7364e-04[MemoryCallback]:  5983752\n",
      "\n",
      "Epoch 25: loss improved from 3.14324 to 2.99927, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 2.9993 - bpp: 2.6167 - mse: 3.7364e-04\n",
      "Epoch 26/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.9383 - bpp: 2.5324 - mse: 3.9642e-04[MemoryCallback]:  5983800\n",
      "\n",
      "Epoch 26: loss improved from 2.99927 to 2.93834, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 2.9383 - bpp: 2.5324 - mse: 3.9642e-04\n",
      "Epoch 27/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.8481 - bpp: 2.4474 - mse: 3.9129e-04[MemoryCallback]:  5998052\n",
      "\n",
      "Epoch 27: loss improved from 2.93834 to 2.84812, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 2.8481 - bpp: 2.4474 - mse: 3.9129e-04\n",
      "Epoch 28/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.4146 - bpp: 2.3842 - mse: 0.0010[MemoryCallback]:  5998344\n",
      "\n",
      "Epoch 28: loss did not improve from 2.84812\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 3.4146 - bpp: 2.3842 - mse: 0.0010\n",
      "Epoch 29/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.0228 - bpp: 2.3099 - mse: 6.9617e-04[MemoryCallback]:  5998344\n",
      "\n",
      "Epoch 29: loss did not improve from 2.84812\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 3.0228 - bpp: 2.3099 - mse: 6.9617e-04\n",
      "Epoch 30/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.6223 - bpp: 2.2075 - mse: 4.0506e-04[MemoryCallback]:  5998344\n",
      "\n",
      "Epoch 30: loss improved from 2.84812 to 2.62230, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 217ms/step - loss: 2.6223 - bpp: 2.2075 - mse: 4.0506e-04\n",
      "Epoch 31/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.5530 - bpp: 2.1288 - mse: 4.1420e-04[MemoryCallback]:  6047216\n",
      "\n",
      "Epoch 31: loss improved from 2.62230 to 2.55297, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 2.5530 - bpp: 2.1288 - mse: 4.1420e-04\n",
      "Epoch 32/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.5659 - bpp: 2.0661 - mse: 4.8800e-04[MemoryCallback]:  6047900\n",
      "\n",
      "Epoch 32: loss did not improve from 2.55297\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 2.5659 - bpp: 2.0661 - mse: 4.8800e-04\n",
      "Epoch 33/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.3605 - bpp: 1.9793 - mse: 3.7229e-04[MemoryCallback]:  6047900\n",
      "\n",
      "Epoch 33: loss improved from 2.55297 to 2.36050, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 47s 233ms/step - loss: 2.3605 - bpp: 1.9793 - mse: 3.7229e-04\n",
      "Epoch 34/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.3287 - bpp: 1.9108 - mse: 4.0817e-04[MemoryCallback]:  6048020\n",
      "\n",
      "Epoch 34: loss improved from 2.36050 to 2.32872, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 217ms/step - loss: 2.3287 - bpp: 1.9108 - mse: 4.0817e-04\n",
      "Epoch 35/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.1775 - bpp: 1.8378 - mse: 3.3175e-04[MemoryCallback]:  6048704\n",
      "\n",
      "Epoch 35: loss improved from 2.32872 to 2.17748, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 2.1775 - bpp: 1.8378 - mse: 3.3175e-04\n",
      "Epoch 36/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.1278 - bpp: 1.7705 - mse: 3.4894e-04[MemoryCallback]:  6049000\n",
      "\n",
      "Epoch 36: loss improved from 2.17748 to 2.12778, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 43s 214ms/step - loss: 2.1278 - bpp: 1.7705 - mse: 3.4894e-04\n",
      "Epoch 37/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0649 - bpp: 1.7082 - mse: 3.4828e-04[MemoryCallback]:  6096924\n",
      "\n",
      "Epoch 37: loss improved from 2.12778 to 2.06487, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 2.0649 - bpp: 1.7082 - mse: 3.4828e-04\n",
      "Epoch 38/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.9824 - bpp: 1.6425 - mse: 3.3192e-04[MemoryCallback]:  6098804\n",
      "\n",
      "Epoch 38: loss improved from 2.06487 to 1.98240, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 1.9824 - bpp: 1.6425 - mse: 3.3192e-04\n",
      "Epoch 39/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.9041 - bpp: 1.5801 - mse: 3.1643e-04[MemoryCallback]:  6147340\n",
      "\n",
      "Epoch 39: loss improved from 1.98240 to 1.90407, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 1.9041 - bpp: 1.5801 - mse: 3.1643e-04\n",
      "Epoch 40/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8662 - bpp: 1.5178 - mse: 3.4023e-04[MemoryCallback]:  6148616\n",
      "\n",
      "Epoch 40: loss improved from 1.90407 to 1.86619, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 48s 236ms/step - loss: 1.8662 - bpp: 1.5178 - mse: 3.4023e-04\n",
      "Epoch 41/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8500 - bpp: 1.4809 - mse: 3.6042e-04[MemoryCallback]:  6149364\n",
      "\n",
      "Epoch 41: loss improved from 1.86619 to 1.85001, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 1.8500 - bpp: 1.4809 - mse: 3.6042e-04\n",
      "Epoch 42/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7655 - bpp: 1.4141 - mse: 3.4314e-04[MemoryCallback]:  6149524\n",
      "\n",
      "Epoch 42: loss improved from 1.85001 to 1.76552, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 46s 225ms/step - loss: 1.7655 - bpp: 1.4141 - mse: 3.4314e-04\n",
      "Epoch 43/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.5993 - bpp: 1.4184 - mse: 0.0012[MemoryCallback]:  6197084\n",
      "\n",
      "Epoch 43: loss did not improve from 1.76552\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 2.5993 - bpp: 1.4184 - mse: 0.0012\n",
      "Epoch 44/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.4176 - bpp: 1.3668 - mse: 0.0010[MemoryCallback]:  6197084\n",
      "\n",
      "Epoch 44: loss did not improve from 1.76552\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 2.4176 - bpp: 1.3668 - mse: 0.0010\n",
      "Epoch 45/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7996 - bpp: 1.3090 - mse: 4.7906e-04[MemoryCallback]:  6197084\n",
      "\n",
      "Epoch 45: loss did not improve from 1.76552\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 1.7996 - bpp: 1.3090 - mse: 4.7906e-04\n",
      "Epoch 46/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6085 - bpp: 1.2328 - mse: 3.6693e-04[MemoryCallback]:  6197084\n",
      "\n",
      "Epoch 46: loss improved from 1.76552 to 1.60849, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 1.6085 - bpp: 1.2328 - mse: 3.6693e-04\n",
      "Epoch 47/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6340 - bpp: 1.2102 - mse: 4.1388e-04[MemoryCallback]:  6245780\n",
      "\n",
      "Epoch 47: loss did not improve from 1.60849\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 1.6340 - bpp: 1.2102 - mse: 4.1388e-04\n",
      "Epoch 48/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5411 - bpp: 1.1561 - mse: 3.7595e-04[MemoryCallback]:  6245780\n",
      "\n",
      "Epoch 48: loss improved from 1.60849 to 1.54107, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 1.5411 - bpp: 1.1561 - mse: 3.7595e-04\n",
      "Epoch 49/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4497 - bpp: 1.1069 - mse: 3.3481e-04[MemoryCallback]:  6248212\n",
      "\n",
      "Epoch 49: loss improved from 1.54107 to 1.44970, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 46s 225ms/step - loss: 1.4497 - bpp: 1.1069 - mse: 3.3481e-04\n",
      "Epoch 50/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4268 - bpp: 1.0728 - mse: 3.4571e-04[MemoryCallback]:  6248692\n",
      "\n",
      "Epoch 50: loss improved from 1.44970 to 1.42678, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 47s 231ms/step - loss: 1.4268 - bpp: 1.0728 - mse: 3.4571e-04\n",
      "Epoch 51/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5169 - bpp: 1.0749 - mse: 4.3159e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 51: loss did not improve from 1.42678\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 1.5169 - bpp: 1.0749 - mse: 4.3159e-04\n",
      "Epoch 52/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3642 - bpp: 1.0021 - mse: 3.5360e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 52: loss improved from 1.42678 to 1.36422, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 1.3642 - bpp: 1.0021 - mse: 3.5360e-04\n",
      "Epoch 53/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3566 - bpp: 0.9772 - mse: 3.7045e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 53: loss improved from 1.36422 to 1.35657, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 1.3566 - bpp: 0.9772 - mse: 3.7045e-04\n",
      "Epoch 54/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4441 - bpp: 0.9907 - mse: 4.4278e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 54: loss did not improve from 1.35657\n",
      "200/200 [==============================] - 46s 222ms/step - loss: 1.4441 - bpp: 0.9907 - mse: 4.4278e-04\n",
      "Epoch 55/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3214 - bpp: 0.9227 - mse: 3.8938e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 55: loss improved from 1.35657 to 1.32143, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 49s 246ms/step - loss: 1.3214 - bpp: 0.9227 - mse: 3.8938e-04\n",
      "Epoch 56/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2945 - bpp: 0.9014 - mse: 3.8387e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 56: loss improved from 1.32143 to 1.29451, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 1.2945 - bpp: 0.9014 - mse: 3.8387e-04\n",
      "Epoch 57/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3060 - bpp: 0.8862 - mse: 4.1000e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 57: loss did not improve from 1.29451\n",
      "200/200 [==============================] - 43s 214ms/step - loss: 1.3060 - bpp: 0.8862 - mse: 4.1000e-04\n",
      "Epoch 58/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2023 - bpp: 0.8474 - mse: 3.4666e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 58: loss improved from 1.29451 to 1.20233, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 46s 225ms/step - loss: 1.2023 - bpp: 0.8474 - mse: 3.4666e-04\n",
      "Epoch 59/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2313 - bpp: 0.8399 - mse: 3.8223e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 59: loss did not improve from 1.20233\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 1.2313 - bpp: 0.8399 - mse: 3.8223e-04\n",
      "Epoch 60/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2085 - bpp: 0.8125 - mse: 3.8666e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 60: loss did not improve from 1.20233\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 1.2085 - bpp: 0.8125 - mse: 3.8666e-04\n",
      "Epoch 61/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2505 - bpp: 0.8046 - mse: 4.3546e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 61: loss did not improve from 1.20233\n",
      "200/200 [==============================] - 42s 209ms/step - loss: 1.2505 - bpp: 0.8046 - mse: 4.3546e-04\n",
      "Epoch 62/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2009 - bpp: 0.7830 - mse: 4.0802e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 62: loss improved from 1.20233 to 1.20086, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 1.2009 - bpp: 0.7830 - mse: 4.0802e-04\n",
      "Epoch 63/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2032 - bpp: 0.7657 - mse: 4.2723e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 63: loss did not improve from 1.20086\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 1.2032 - bpp: 0.7657 - mse: 4.2723e-04\n",
      "Epoch 64/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1848 - bpp: 0.7491 - mse: 4.2547e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 64: loss improved from 1.20086 to 1.18481, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 1.1848 - bpp: 0.7491 - mse: 4.2547e-04\n",
      "Epoch 65/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1075 - bpp: 0.7171 - mse: 3.8125e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 65: loss improved from 1.18481 to 1.10749, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 50s 250ms/step - loss: 1.1075 - bpp: 0.7171 - mse: 3.8125e-04\n",
      "Epoch 66/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1569 - bpp: 0.7240 - mse: 4.2274e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 66: loss did not improve from 1.10749\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 1.1569 - bpp: 0.7240 - mse: 4.2274e-04\n",
      "Epoch 67/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2053 - bpp: 0.7157 - mse: 4.7814e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 67: loss did not improve from 1.10749\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 1.2053 - bpp: 0.7157 - mse: 4.7814e-04\n",
      "Epoch 68/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1234 - bpp: 0.6969 - mse: 4.1649e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 68: loss did not improve from 1.10749\n",
      "200/200 [==============================] - 48s 235ms/step - loss: 1.1234 - bpp: 0.6969 - mse: 4.1649e-04\n",
      "Epoch 69/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4921 - bpp: 0.7337 - mse: 7.4065e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 69: loss did not improve from 1.10749\n",
      "200/200 [==============================] - 47s 235ms/step - loss: 1.4921 - bpp: 0.7337 - mse: 7.4065e-04\n",
      "Epoch 70/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2552 - bpp: 0.6980 - mse: 5.4419e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 70: loss did not improve from 1.10749\n",
      "200/200 [==============================] - 47s 235ms/step - loss: 1.2552 - bpp: 0.6980 - mse: 5.4419e-04\n",
      "Epoch 71/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1500 - bpp: 0.6791 - mse: 4.5989e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 71: loss did not improve from 1.10749\n",
      "200/200 [==============================] - 48s 238ms/step - loss: 1.1500 - bpp: 0.6791 - mse: 4.5989e-04\n",
      "Epoch 72/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0080 - bpp: 0.6211 - mse: 3.7783e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 72: loss improved from 1.10749 to 1.00801, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 48s 236ms/step - loss: 1.0080 - bpp: 0.6211 - mse: 3.7783e-04\n",
      "Epoch 73/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0596 - bpp: 0.6437 - mse: 4.0610e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 73: loss did not improve from 1.00801\n",
      "200/200 [==============================] - 47s 232ms/step - loss: 1.0596 - bpp: 0.6437 - mse: 4.0610e-04\n",
      "Epoch 74/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0816 - bpp: 0.6317 - mse: 4.3936e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 74: loss did not improve from 1.00801\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 1.0816 - bpp: 0.6317 - mse: 4.3936e-04\n",
      "Epoch 75/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1246 - bpp: 0.6285 - mse: 4.8453e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 75: loss did not improve from 1.00801\n",
      "200/200 [==============================] - 50s 249ms/step - loss: 1.1246 - bpp: 0.6285 - mse: 4.8453e-04\n",
      "Epoch 76/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9999 - bpp: 0.5977 - mse: 3.9272e-04[MemoryCallback]:  6248708\n",
      "\n",
      "Epoch 76: loss improved from 1.00801 to 0.99986, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 47s 233ms/step - loss: 0.9999 - bpp: 0.5977 - mse: 3.9272e-04\n",
      "Epoch 77/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1689 - bpp: 0.6279 - mse: 5.2837e-04[MemoryCallback]:  6296140\n",
      "\n",
      "Epoch 77: loss did not improve from 0.99986\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 1.1689 - bpp: 0.6279 - mse: 5.2837e-04\n",
      "Epoch 78/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9774 - bpp: 0.5722 - mse: 3.9576e-04[MemoryCallback]:  6296140\n",
      "\n",
      "Epoch 78: loss improved from 0.99986 to 0.97743, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 0.9774 - bpp: 0.5722 - mse: 3.9576e-04\n",
      "Epoch 79/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0380 - bpp: 0.5839 - mse: 4.4348e-04[MemoryCallback]:  6296220\n",
      "\n",
      "Epoch 79: loss did not improve from 0.97743\n",
      "200/200 [==============================] - 48s 237ms/step - loss: 1.0380 - bpp: 0.5839 - mse: 4.4348e-04\n",
      "Epoch 80/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9817 - bpp: 0.5623 - mse: 4.0956e-04[MemoryCallback]:  6296220\n",
      "\n",
      "Epoch 80: loss did not improve from 0.97743\n",
      "200/200 [==============================] - 47s 235ms/step - loss: 0.9817 - bpp: 0.5623 - mse: 4.0956e-04\n",
      "Epoch 81/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9864 - bpp: 0.5555 - mse: 4.2079e-04[MemoryCallback]:  6296220\n",
      "\n",
      "Epoch 81: loss did not improve from 0.97743\n",
      "200/200 [==============================] - 47s 235ms/step - loss: 0.9864 - bpp: 0.5555 - mse: 4.2079e-04\n",
      "Epoch 82/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9283 - bpp: 0.5311 - mse: 3.8787e-04[MemoryCallback]:  6296220\n",
      "\n",
      "Epoch 82: loss improved from 0.97743 to 0.92829, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 48s 236ms/step - loss: 0.9283 - bpp: 0.5311 - mse: 3.8787e-04\n",
      "Epoch 83/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0067 - bpp: 0.5481 - mse: 4.4781e-04[MemoryCallback]:  6303440\n",
      "\n",
      "Epoch 83: loss did not improve from 0.92829\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 1.0067 - bpp: 0.5481 - mse: 4.4781e-04\n",
      "Epoch 84/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9464 - bpp: 0.5243 - mse: 4.1221e-04[MemoryCallback]:  6303440\n",
      "\n",
      "Epoch 84: loss did not improve from 0.92829\n",
      "200/200 [==============================] - 51s 251ms/step - loss: 0.9464 - bpp: 0.5243 - mse: 4.1221e-04\n",
      "Epoch 85/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9390 - bpp: 0.5186 - mse: 4.1056e-04[MemoryCallback]:  6303440\n",
      "\n",
      "Epoch 85: loss did not improve from 0.92829\n",
      "200/200 [==============================] - 49s 242ms/step - loss: 0.9390 - bpp: 0.5186 - mse: 4.1056e-04\n",
      "Epoch 86/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0223 - bpp: 0.5337 - mse: 4.7715e-04[MemoryCallback]:  6303440\n",
      "\n",
      "Epoch 86: loss did not improve from 0.92829\n",
      "200/200 [==============================] - 50s 246ms/step - loss: 1.0223 - bpp: 0.5337 - mse: 4.7715e-04\n",
      "Epoch 87/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9111 - bpp: 0.5082 - mse: 3.9346e-04[MemoryCallback]:  6303440\n",
      "\n",
      "Epoch 87: loss improved from 0.92829 to 0.91113, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 48s 237ms/step - loss: 0.9111 - bpp: 0.5082 - mse: 3.9346e-04\n",
      "Epoch 88/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9826 - bpp: 0.5112 - mse: 4.6043e-04[MemoryCallback]:  6303440\n",
      "\n",
      "Epoch 88: loss did not improve from 0.91113\n",
      "200/200 [==============================] - 49s 241ms/step - loss: 0.9826 - bpp: 0.5112 - mse: 4.6043e-04\n",
      "Epoch 89/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9706 - bpp: 0.5117 - mse: 4.4818e-04[MemoryCallback]:  6303440\n",
      "\n",
      "Epoch 89: loss did not improve from 0.91113\n",
      "200/200 [==============================] - 49s 239ms/step - loss: 0.9706 - bpp: 0.5117 - mse: 4.4818e-04\n",
      "Epoch 90/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9172 - bpp: 0.4996 - mse: 4.0780e-04[MemoryCallback]:  6303440\n",
      "\n",
      "Epoch 90: loss did not improve from 0.91113\n",
      "200/200 [==============================] - 50s 248ms/step - loss: 0.9172 - bpp: 0.4996 - mse: 4.0780e-04\n",
      "Epoch 91/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9903 - bpp: 0.5087 - mse: 4.7038e-04[MemoryCallback]:  6303440\n",
      "\n",
      "Epoch 91: loss did not improve from 0.91113\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 0.9903 - bpp: 0.5087 - mse: 4.7038e-04\n",
      "Epoch 92/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0100 - bpp: 0.5094 - mse: 4.8890e-04[MemoryCallback]:  6303440\n",
      "\n",
      "Epoch 92: loss did not improve from 0.91113\n",
      "200/200 [==============================] - 50s 248ms/step - loss: 1.0100 - bpp: 0.5094 - mse: 4.8890e-04\n",
      "Epoch 93/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8719 - bpp: 0.4697 - mse: 3.9275e-04[MemoryCallback]:  6303440\n",
      "\n",
      "Epoch 93: loss improved from 0.91113 to 0.87189, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 53s 266ms/step - loss: 0.8719 - bpp: 0.4697 - mse: 3.9275e-04\n",
      "Epoch 94/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8769 - bpp: 0.4729 - mse: 3.9446e-04[MemoryCallback]:  6310204\n",
      "\n",
      "Epoch 94: loss did not improve from 0.87189\n",
      "200/200 [==============================] - 54s 268ms/step - loss: 0.8769 - bpp: 0.4729 - mse: 3.9446e-04\n",
      "Epoch 95/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9616 - bpp: 0.4869 - mse: 4.6350e-04[MemoryCallback]:  6310204\n",
      "\n",
      "Epoch 95: loss did not improve from 0.87189\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 0.9616 - bpp: 0.4869 - mse: 4.6350e-04\n",
      "Epoch 96/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9626 - bpp: 0.4844 - mse: 4.6707e-04[MemoryCallback]:  6310204\n",
      "\n",
      "Epoch 96: loss did not improve from 0.87189\n",
      "200/200 [==============================] - 50s 248ms/step - loss: 0.9626 - bpp: 0.4844 - mse: 4.6707e-04\n",
      "Epoch 97/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8315 - bpp: 0.4486 - mse: 3.7395e-04[MemoryCallback]:  6310204\n",
      "\n",
      "Epoch 97: loss improved from 0.87189 to 0.83149, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 52s 257ms/step - loss: 0.8315 - bpp: 0.4486 - mse: 3.7395e-04\n",
      "Epoch 98/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8329 - bpp: 0.4560 - mse: 3.6806e-04[MemoryCallback]:  6360132\n",
      "\n",
      "Epoch 98: loss did not improve from 0.83149\n",
      "200/200 [==============================] - 51s 255ms/step - loss: 0.8329 - bpp: 0.4560 - mse: 3.6806e-04\n",
      "Epoch 99/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8690 - bpp: 0.4554 - mse: 4.0388e-04[MemoryCallback]:  6360132\n",
      "\n",
      "Epoch 99: loss did not improve from 0.83149\n",
      "200/200 [==============================] - 49s 242ms/step - loss: 0.8690 - bpp: 0.4554 - mse: 4.0388e-04\n",
      "Epoch 100/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8551 - bpp: 0.4419 - mse: 4.0351e-04[MemoryCallback]:  6360132\n",
      "\n",
      "Epoch 100: loss did not improve from 0.83149\n",
      "200/200 [==============================] - 50s 246ms/step - loss: 0.8551 - bpp: 0.4419 - mse: 4.0351e-04\n",
      "Epoch 101/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8103 - bpp: 0.4269 - mse: 3.7440e-04[MemoryCallback]:  6360132\n",
      "\n",
      "Epoch 101: loss improved from 0.83149 to 0.81029, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 55s 275ms/step - loss: 0.8103 - bpp: 0.4269 - mse: 3.7440e-04\n",
      "Epoch 102/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9464 - bpp: 0.4504 - mse: 4.8433e-04[MemoryCallback]:  6365612\n",
      "\n",
      "Epoch: 101. Reducing Learning Rate from 9.999999747378752e-05 to 9.899999713525176e-05\n",
      "\n",
      "Epoch 102: loss did not improve from 0.81029\n",
      "200/200 [==============================] - 52s 257ms/step - loss: 0.9464 - bpp: 0.4504 - mse: 4.8433e-04\n",
      "Epoch 103/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9137 - bpp: 0.4468 - mse: 4.5597e-04[MemoryCallback]:  6365612\n",
      "\n",
      "Epoch: 102. Reducing Learning Rate from 9.899999713525176e-05 to 9.801000123843551e-05\n",
      "\n",
      "Epoch 103: loss did not improve from 0.81029\n",
      "200/200 [==============================] - 52s 257ms/step - loss: 0.9137 - bpp: 0.4468 - mse: 4.5597e-04\n",
      "Epoch 104/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9056 - bpp: 0.4465 - mse: 4.4842e-04[MemoryCallback]:  6365612\n",
      "\n",
      "Epoch: 103. Reducing Learning Rate from 9.801000123843551e-05 to 9.702990064397454e-05\n",
      "\n",
      "Epoch 104: loss did not improve from 0.81029\n",
      "200/200 [==============================] - 57s 282ms/step - loss: 0.9056 - bpp: 0.4465 - mse: 4.4842e-04\n",
      "Epoch 105/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8540 - bpp: 0.4418 - mse: 4.0253e-04[MemoryCallback]:  6365612\n",
      "\n",
      "Epoch: 104. Reducing Learning Rate from 9.702990064397454e-05 to 9.605960076441988e-05\n",
      "\n",
      "Epoch 105: loss did not improve from 0.81029\n",
      "200/200 [==============================] - 51s 251ms/step - loss: 0.8540 - bpp: 0.4418 - mse: 4.0253e-04\n",
      "Epoch 106/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8605 - bpp: 0.4270 - mse: 4.2335e-04[MemoryCallback]:  6365612\n",
      "\n",
      "Epoch: 105. Reducing Learning Rate from 9.605960076441988e-05 to 9.509900701232255e-05\n",
      "\n",
      "Epoch 106: loss did not improve from 0.81029\n",
      "200/200 [==============================] - 56s 277ms/step - loss: 0.8605 - bpp: 0.4270 - mse: 4.2335e-04\n",
      "Epoch 107/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8351 - bpp: 0.4276 - mse: 3.9798e-04[MemoryCallback]:  6365612\n",
      "\n",
      "Epoch: 106. Reducing Learning Rate from 9.509900701232255e-05 to 9.414801752427593e-05\n",
      "\n",
      "Epoch 107: loss did not improve from 0.81029\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 0.8351 - bpp: 0.4276 - mse: 3.9798e-04\n",
      "Epoch 108/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9111 - bpp: 0.4322 - mse: 4.6767e-04[MemoryCallback]:  6365612\n",
      "\n",
      "Epoch: 107. Reducing Learning Rate from 9.414801752427593e-05 to 9.320653771283105e-05\n",
      "\n",
      "Epoch 108: loss did not improve from 0.81029\n",
      "200/200 [==============================] - 51s 254ms/step - loss: 0.9111 - bpp: 0.4322 - mse: 4.6767e-04\n",
      "Epoch 109/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8619 - bpp: 0.4308 - mse: 4.2098e-04[MemoryCallback]:  6365612\n",
      "\n",
      "Epoch: 108. Reducing Learning Rate from 9.320653771283105e-05 to 9.227447299053892e-05\n",
      "\n",
      "Epoch 109: loss did not improve from 0.81029\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 0.8619 - bpp: 0.4308 - mse: 4.2098e-04\n",
      "Epoch 110/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8562 - bpp: 0.4255 - mse: 4.2055e-04[MemoryCallback]:  6365612\n",
      "\n",
      "Epoch: 109. Reducing Learning Rate from 9.227447299053892e-05 to 9.135172876995057e-05\n",
      "\n",
      "Epoch 110: loss did not improve from 0.81029\n",
      "200/200 [==============================] - 50s 249ms/step - loss: 0.8562 - bpp: 0.4255 - mse: 4.2055e-04\n",
      "Epoch 111/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8710 - bpp: 0.4264 - mse: 4.3418e-04[MemoryCallback]:  6365612\n",
      "\n",
      "Epoch: 110. Reducing Learning Rate from 9.135172876995057e-05 to 9.0438210463617e-05\n",
      "\n",
      "Epoch 111: loss did not improve from 0.81029\n",
      "200/200 [==============================] - 53s 261ms/step - loss: 0.8710 - bpp: 0.4264 - mse: 4.3418e-04\n",
      "Epoch 112/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7546 - bpp: 0.4001 - mse: 3.4620e-04[MemoryCallback]:  6365612\n",
      "\n",
      "Epoch: 111. Reducing Learning Rate from 9.0438210463617e-05 to 8.953383076004684e-05\n",
      "\n",
      "Epoch 112: loss improved from 0.81029 to 0.75464, saving model to checkpoints_wavelets_L_1024_1_240x240/\n",
      "200/200 [==============================] - 51s 252ms/step - loss: 0.7546 - bpp: 0.4001 - mse: 3.4620e-04\n",
      "Epoch 113/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8251 - bpp: 0.4152 - mse: 4.0032e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 112. Reducing Learning Rate from 8.953383076004684e-05 to 8.863849507179111e-05\n",
      "\n",
      "Epoch 113: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 50s 246ms/step - loss: 0.8251 - bpp: 0.4152 - mse: 4.0032e-04\n",
      "Epoch 114/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8406 - bpp: 0.4167 - mse: 4.1392e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 113. Reducing Learning Rate from 8.863849507179111e-05 to 8.775210881140083e-05\n",
      "\n",
      "Epoch 114: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 50s 246ms/step - loss: 0.8406 - bpp: 0.4167 - mse: 4.1392e-04\n",
      "Epoch 115/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7733 - bpp: 0.3972 - mse: 3.6726e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 114. Reducing Learning Rate from 8.775210881140083e-05 to 8.687459194334224e-05\n",
      "\n",
      "Epoch 115: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 50s 247ms/step - loss: 0.7733 - bpp: 0.3972 - mse: 3.6726e-04\n",
      "Epoch 116/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8052 - bpp: 0.4059 - mse: 3.8996e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 115. Reducing Learning Rate from 8.687459194334224e-05 to 8.600584988016635e-05\n",
      "\n",
      "Epoch 116: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 0.8052 - bpp: 0.4059 - mse: 3.8996e-04\n",
      "Epoch 117/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8399 - bpp: 0.4125 - mse: 4.1733e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 116. Reducing Learning Rate from 8.600584988016635e-05 to 8.51457953103818e-05\n",
      "\n",
      "Epoch 117: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 0.8399 - bpp: 0.4125 - mse: 4.1733e-04\n",
      "Epoch 118/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8930 - bpp: 0.4264 - mse: 4.5566e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 117. Reducing Learning Rate from 8.51457953103818e-05 to 8.429434092249721e-05\n",
      "\n",
      "Epoch 118: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 50s 248ms/step - loss: 0.8930 - bpp: 0.4264 - mse: 4.5566e-04\n",
      "Epoch 119/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0617 - bpp: 0.4554 - mse: 5.9209e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 118. Reducing Learning Rate from 8.429434092249721e-05 to 8.345139940502122e-05\n",
      "\n",
      "Epoch 119: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 50s 247ms/step - loss: 1.0617 - bpp: 0.4554 - mse: 5.9209e-04\n",
      "Epoch 120/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7694 - bpp: 0.3967 - mse: 3.6399e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 119. Reducing Learning Rate from 8.345139940502122e-05 to 8.261688344646245e-05\n",
      "\n",
      "Epoch 120: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 53s 262ms/step - loss: 0.7694 - bpp: 0.3967 - mse: 3.6399e-04\n",
      "Epoch 121/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7928 - bpp: 0.3974 - mse: 3.8612e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 120. Reducing Learning Rate from 8.261688344646245e-05 to 8.179071301128715e-05\n",
      "\n",
      "Epoch 121: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 50s 249ms/step - loss: 0.7928 - bpp: 0.3974 - mse: 3.8612e-04\n",
      "Epoch 122/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7801 - bpp: 0.3934 - mse: 3.7758e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 121. Reducing Learning Rate from 8.179071301128715e-05 to 8.097280806396157e-05\n",
      "\n",
      "Epoch 122: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 50s 250ms/step - loss: 0.7801 - bpp: 0.3934 - mse: 3.7758e-04\n",
      "Epoch 123/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8863 - bpp: 0.4198 - mse: 4.5556e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 122. Reducing Learning Rate from 8.097280806396157e-05 to 8.016308129299432e-05\n",
      "\n",
      "Epoch 123: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 49s 242ms/step - loss: 0.8863 - bpp: 0.4198 - mse: 4.5556e-04\n",
      "Epoch 124/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7761 - bpp: 0.3881 - mse: 3.7889e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 123. Reducing Learning Rate from 8.016308129299432e-05 to 7.936145266285166e-05\n",
      "\n",
      "Epoch 124: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 0.7761 - bpp: 0.3881 - mse: 3.7889e-04\n",
      "Epoch 125/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8838 - bpp: 0.4127 - mse: 4.6004e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 124. Reducing Learning Rate from 7.936145266285166e-05 to 7.856784213799983e-05\n",
      "\n",
      "Epoch 125: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 0.8838 - bpp: 0.4127 - mse: 4.6004e-04\n",
      "Epoch 126/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7751 - bpp: 0.3888 - mse: 3.7721e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 125. Reducing Learning Rate from 7.856784213799983e-05 to 7.778216240694746e-05\n",
      "\n",
      "Epoch 126: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 47s 235ms/step - loss: 0.7751 - bpp: 0.3888 - mse: 3.7721e-04\n",
      "Epoch 127/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8361 - bpp: 0.3994 - mse: 4.2647e-04[MemoryCallback]:  6421704\n",
      "\n",
      "Epoch: 126. Reducing Learning Rate from 7.778216240694746e-05 to 7.700434071011841e-05\n",
      "\n",
      "Epoch 127: loss did not improve from 0.75464\n",
      "200/200 [==============================] - 47s 233ms/step - loss: 0.8361 - bpp: 0.3994 - mse: 4.2647e-04\n"
     ]
    }
   ],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "hist = model.fit(x=data, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE,\n",
    "                callbacks=[\n",
    "                    Callbacks.MemoryCallback(),\n",
    "                    Callbacks.LearningRateReducer(),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(filepath=checkponts_new_path, save_weights_only=True, save_freq='epoch', monitor=\"loss\", mode='min',  save_best_only=True, verbose=1), \n",
    "                    tf.keras.callbacks.TerminateOnNaN(),\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=early_stop),\n",
    "                    tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, update_freq=\"epoch\"),            \n",
    "                    ],\n",
    "\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/WindowsDev/DataSets/vimeo_septuplet/sequences/00056/0723/im1.png\n",
      "compress\n",
      "in the compress\n",
      "decompress\n",
      "in decompress\n"
     ]
    }
   ],
   "source": [
    "path = load.load_random_path(\"folder_cloud_test.npy\")\n",
    "i=0\n",
    "out_bin = \"Test_com/test{}.bin\".format(i)\n",
    "out_decom = \"Test_com/testdcom{}.png\".format(i)\n",
    "p_on_test = \"Test_com/test_p_frame{}.png\".format(i)\n",
    "i_on_test = \"Test_com/test_i_frame{}.png\".format(i)\n",
    "\n",
    "i_frame = path + 'im1' + '.png'\n",
    "p_frame = path + 'im2' + '.png'\n",
    "print(i_frame)\n",
    "\n",
    "OpenDVCW.write_png(p_on_test, OpenDVCW.read_png_crop(p_frame, 240, 240))\n",
    "OpenDVCW.write_png(i_on_test, OpenDVCW.read_png_crop(i_frame, 240, 240))\n",
    "\n",
    "OpenDVCW.compress(model, i_frame, p_frame, out_bin, 240, 240)\n",
    "OpenDVCW.decompress(model, i_frame, out_bin, out_decom, 240, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the compress\n",
      "in decompress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as optical_flow_loss_layer_call_fn, optical_flow_loss_layer_call_and_return_conditional_losses, dwt_layer_call_fn, dwt_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_wavelets_L_1024_1_240x240/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_wavelets_L_1024_1_240x240/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(save_name, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
