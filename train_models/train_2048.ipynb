{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/OpenDVCW/train_models'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/OpenDVCW\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/ubu-admin/Developer/tensorflow-wavelets\n",
    "%cd /workspaces/OpenDVCW\n",
    "import OpenDVCW\n",
    "import numpy as np\n",
    "import load\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import DataGen\n",
    "import Callbacks\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 800\n",
    "STEPS_PER_EPOCH = 100\n",
    "Height = 240\n",
    "Width = 240\n",
    "Channel = 3\n",
    "lmbda = 2048\n",
    "lr_init = 1e-4\n",
    "early_stop = 100\n",
    "I_QP=27\n",
    "wavelet_name = \"haar\"\n",
    "\n",
    "args = OpenDVCW.Arguments()\n",
    "\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "checkponts_last_path = \"\"\n",
    "checkponts_new_path = \"checkpoints_wavelets_{}_Lmbd_{}_epcs_{}_es_{}_I_QP_{}_{}x{}_CosineDecay_{}/\".format(wavelet_name, lmbda, EPOCHS,  early_stop, I_QP, Width, Height, timestamp)\n",
    "save_name = \"model_save_\" + checkponts_new_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 07:33:49.375044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-29 07:33:49.434327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-29 07:33:49.435557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-29 07:33:49.440743: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-29 07:33:49.449514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-29 07:33:49.450435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-29 07:33:49.451217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-29 07:33:51.794949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-29 07:33:51.796294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-29 07:33:51.797411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-29 07:33:51.798534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10244 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:00:09.0, compute capability: 8.6\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "* [Loading dataset]...\n",
      "Loading weights\n"
     ]
    }
   ],
   "source": [
    "model = OpenDVCW.OpenDVC(width=Width, height=Height, batch_size=BATCH_SIZE, num_filters=128, lmbda=lmbda)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=lr_init, decay_steps=EPOCHS*(STEPS_PER_EPOCH), alpha=1e-8, name=\"lr_CosineDecay\")\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),)\n",
    "print(\"* [Model compiled]...\")\n",
    "\n",
    "print(\"* [Loading dataset]...\")\n",
    "data = DataGen.DataVimeo90kGenerator(\"folder_cloud_test.npy\", \n",
    "                                    BATCH_SIZE,\n",
    "                                    (Height,Width,Channel),\n",
    "                                    Channel,\n",
    "                                    True, \n",
    "                                    I_QP,\n",
    "                                    True)\n",
    "\n",
    "print(\"Loading weights\")\n",
    "if not checkponts_last_path == \"\":\n",
    "    model.load_weights(checkponts_last_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].trainable = False\n",
    "# model.layers[1].trainable = False\n",
    "# model.layers[2].trainable = True\n",
    "# model.layers[3].trainable = True\n",
    "# model.layers[4].trainable = False\n",
    "# model.layers[5].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv_analysis True\n",
      "mv_synthesis True\n",
      "res_analysis True\n",
      "res_synthesis True\n",
      "wavelets_optical_flow True\n",
      "motion_compensation True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 07:34:26.629396: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-04-29 07:35:39.772407: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 48.3705 - bpp: 5.3371 - mse: 0.0210\n",
      "Epoch 1: loss improved from inf to 48.37046, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 137s 348ms/step - loss: 48.3705 - bpp: 5.3371 - mse: 0.0210\n",
      "Epoch 2/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 16.5180 - bpp: 5.2672 - mse: 0.0055\n",
      "Epoch 2: loss improved from 48.37046 to 16.51802, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 36s 350ms/step - loss: 16.5180 - bpp: 5.2672 - mse: 0.0055\n",
      "Epoch 3/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 12.5311 - bpp: 5.1977 - mse: 0.0036\n",
      "Epoch 3: loss improved from 16.51802 to 12.53113, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 38s 378ms/step - loss: 12.5311 - bpp: 5.1977 - mse: 0.0036\n",
      "Epoch 4/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 10.2328 - bpp: 5.1292 - mse: 0.0025\n",
      "Epoch 4: loss improved from 12.53113 to 10.23276, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 37s 363ms/step - loss: 10.2328 - bpp: 5.1292 - mse: 0.0025\n",
      "Epoch 5/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 9.1274 - bpp: 5.0618 - mse: 0.0020\n",
      "Epoch 5: loss improved from 10.23276 to 9.12738, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 9.1274 - bpp: 5.0618 - mse: 0.0020\n",
      "Epoch 6/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 9.2702 - bpp: 4.9957 - mse: 0.0021\n",
      "Epoch 6: loss did not improve from 9.12738\n",
      "100/100 [==============================] - 35s 343ms/step - loss: 9.2702 - bpp: 4.9957 - mse: 0.0021\n",
      "Epoch 7/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 8.4276 - bpp: 4.9303 - mse: 0.0017\n",
      "Epoch 7: loss improved from 9.12738 to 8.42760, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 36s 352ms/step - loss: 8.4276 - bpp: 4.9303 - mse: 0.0017\n",
      "Epoch 8/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 8.4793 - bpp: 4.8657 - mse: 0.0018\n",
      "Epoch 8: loss did not improve from 8.42760\n",
      "100/100 [==============================] - 34s 340ms/step - loss: 8.4793 - bpp: 4.8657 - mse: 0.0018\n",
      "Epoch 9/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 8.0188 - bpp: 4.8011 - mse: 0.0016\n",
      "Epoch 9: loss improved from 8.42760 to 8.01882, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 34s 340ms/step - loss: 8.0188 - bpp: 4.8011 - mse: 0.0016\n",
      "Epoch 10/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 7.6710 - bpp: 4.7370 - mse: 0.0014\n",
      "Epoch 10: loss improved from 8.01882 to 7.67104, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 36s 351ms/step - loss: 7.6710 - bpp: 4.7370 - mse: 0.0014\n",
      "Epoch 11/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 7.1020 - bpp: 4.6733 - mse: 0.0012\n",
      "Epoch 11: loss improved from 7.67104 to 7.10201, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 35s 342ms/step - loss: 7.1020 - bpp: 4.6733 - mse: 0.0012\n",
      "Epoch 12/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 6.8031 - bpp: 4.6104 - mse: 0.0011\n",
      "Epoch 12: loss improved from 7.10201 to 6.80306, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 36s 360ms/step - loss: 6.8031 - bpp: 4.6104 - mse: 0.0011\n",
      "Epoch 13/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 6.4977 - bpp: 4.5478 - mse: 9.5213e-04\n",
      "Epoch 13: loss improved from 6.80306 to 6.49774, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 39s 390ms/step - loss: 6.4977 - bpp: 4.5478 - mse: 9.5213e-04\n",
      "Epoch 14/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 6.3451 - bpp: 4.4867 - mse: 9.0741e-04\n",
      "Epoch 14: loss improved from 6.49774 to 6.34510, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 35s 341ms/step - loss: 6.3451 - bpp: 4.4867 - mse: 9.0741e-04\n",
      "Epoch 15/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 5.9203 - bpp: 4.4248 - mse: 7.3025e-04\n",
      "Epoch 15: loss improved from 6.34510 to 5.92035, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 35s 342ms/step - loss: 5.9203 - bpp: 4.4248 - mse: 7.3025e-04\n",
      "Epoch 16/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 6.0490 - bpp: 4.3636 - mse: 8.2294e-04\n",
      "Epoch 16: loss did not improve from 5.92035\n",
      "100/100 [==============================] - 33s 327ms/step - loss: 6.0490 - bpp: 4.3636 - mse: 8.2294e-04\n",
      "Epoch 17/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 5.5868 - bpp: 4.3031 - mse: 6.2680e-04\n",
      "Epoch 17: loss improved from 5.92035 to 5.58680, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 35s 342ms/step - loss: 5.5868 - bpp: 4.3031 - mse: 6.2680e-04\n",
      "Epoch 18/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 5.6249 - bpp: 4.2435 - mse: 6.7451e-04\n",
      "Epoch 18: loss did not improve from 5.58680\n",
      "100/100 [==============================] - 29s 282ms/step - loss: 5.6249 - bpp: 4.2435 - mse: 6.7451e-04\n",
      "Epoch 19/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 5.5780 - bpp: 4.1851 - mse: 6.8008e-04\n",
      "Epoch 19: loss improved from 5.58680 to 5.57796, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 33s 326ms/step - loss: 5.5780 - bpp: 4.1851 - mse: 6.8008e-04\n",
      "Epoch 20/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 5.6500 - bpp: 4.1280 - mse: 7.4314e-04\n",
      "Epoch 20: loss did not improve from 5.57796\n",
      "100/100 [==============================] - 36s 354ms/step - loss: 5.6500 - bpp: 4.1280 - mse: 7.4314e-04\n",
      "Epoch 21/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 5.2689 - bpp: 4.0689 - mse: 5.8594e-04\n",
      "Epoch 21: loss improved from 5.57796 to 5.26894, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 5.2689 - bpp: 4.0689 - mse: 5.8594e-04\n",
      "Epoch 22/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 5.1889 - bpp: 4.0120 - mse: 5.7464e-04\n",
      "Epoch 22: loss improved from 5.26894 to 5.18891, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 30s 290ms/step - loss: 5.1889 - bpp: 4.0120 - mse: 5.7464e-04\n",
      "Epoch 23/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.9448 - bpp: 3.9557 - mse: 4.8295e-04\n",
      "Epoch 23: loss improved from 5.18891 to 4.94477, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 28s 277ms/step - loss: 4.9448 - bpp: 3.9557 - mse: 4.8295e-04\n",
      "Epoch 24/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 5.2241 - bpp: 3.9020 - mse: 6.4557e-04\n",
      "Epoch 24: loss did not improve from 4.94477\n",
      "100/100 [==============================] - 30s 293ms/step - loss: 5.2241 - bpp: 3.9020 - mse: 6.4557e-04\n",
      "Epoch 25/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 5.0843 - bpp: 3.8460 - mse: 6.0468e-04\n",
      "Epoch 25: loss did not improve from 4.94477\n",
      "100/100 [==============================] - 28s 277ms/step - loss: 5.0843 - bpp: 3.8460 - mse: 6.0468e-04\n",
      "Epoch 26/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.7020 - bpp: 3.7881 - mse: 4.4623e-04\n",
      "Epoch 26: loss improved from 4.94477 to 4.70199, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 4.7020 - bpp: 3.7881 - mse: 4.4623e-04\n",
      "Epoch 27/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.7247 - bpp: 3.7348 - mse: 4.8338e-04\n",
      "Epoch 27: loss did not improve from 4.70199\n",
      "100/100 [==============================] - 30s 293ms/step - loss: 4.7247 - bpp: 3.7348 - mse: 4.8338e-04\n",
      "Epoch 28/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.7146 - bpp: 3.6815 - mse: 5.0442e-04\n",
      "Epoch 28: loss did not improve from 4.70199\n",
      "100/100 [==============================] - 29s 285ms/step - loss: 4.7146 - bpp: 3.6815 - mse: 5.0442e-04\n",
      "Epoch 29/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.5759 - bpp: 3.6289 - mse: 4.6241e-04\n",
      "Epoch 29: loss improved from 4.70199 to 4.57590, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 30s 291ms/step - loss: 4.5759 - bpp: 3.6289 - mse: 4.6241e-04\n",
      "Epoch 30/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.3574 - bpp: 3.5733 - mse: 3.8283e-04\n",
      "Epoch 30: loss improved from 4.57590 to 4.35736, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 31s 302ms/step - loss: 4.3574 - bpp: 3.5733 - mse: 3.8283e-04\n",
      "Epoch 31/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.5291 - bpp: 3.5249 - mse: 4.9033e-04\n",
      "Epoch 31: loss did not improve from 4.35736\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 4.5291 - bpp: 3.5249 - mse: 4.9033e-04\n",
      "Epoch 32/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.2741 - bpp: 3.4687 - mse: 3.9326e-04\n",
      "Epoch 32: loss improved from 4.35736 to 4.27405, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 31s 299ms/step - loss: 4.2741 - bpp: 3.4687 - mse: 3.9326e-04\n",
      "Epoch 33/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.2912 - bpp: 3.4197 - mse: 4.2555e-04\n",
      "Epoch 33: loss did not improve from 4.27405\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 4.2912 - bpp: 3.4197 - mse: 4.2555e-04\n",
      "Epoch 34/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.2998 - bpp: 3.3722 - mse: 4.5295e-04\n",
      "Epoch 34: loss did not improve from 4.27405\n",
      "100/100 [==============================] - 30s 297ms/step - loss: 4.2998 - bpp: 3.3722 - mse: 4.5295e-04\n",
      "Epoch 35/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.1190 - bpp: 3.3191 - mse: 3.9056e-04\n",
      "Epoch 35: loss improved from 4.27405 to 4.11898, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 28s 277ms/step - loss: 4.1190 - bpp: 3.3191 - mse: 3.9056e-04\n",
      "Epoch 36/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.0371 - bpp: 3.2709 - mse: 3.7409e-04\n",
      "Epoch 36: loss improved from 4.11898 to 4.03706, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 4.0371 - bpp: 3.2709 - mse: 3.7409e-04\n",
      "Epoch 37/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.0217 - bpp: 3.2239 - mse: 3.8953e-04\n",
      "Epoch 37: loss improved from 4.03706 to 4.02168, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 30s 293ms/step - loss: 4.0217 - bpp: 3.2239 - mse: 3.8953e-04\n",
      "Epoch 38/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.9931 - bpp: 3.1747 - mse: 3.9960e-04\n",
      "Epoch 38: loss improved from 4.02168 to 3.99312, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 28s 275ms/step - loss: 3.9931 - bpp: 3.1747 - mse: 3.9960e-04\n",
      "Epoch 39/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.0260 - bpp: 3.1311 - mse: 4.3699e-04\n",
      "Epoch 39: loss did not improve from 3.99312\n",
      "100/100 [==============================] - 30s 296ms/step - loss: 4.0260 - bpp: 3.1311 - mse: 4.3699e-04\n",
      "Epoch 40/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.7554 - bpp: 3.0747 - mse: 3.3235e-04\n",
      "Epoch 40: loss improved from 3.99312 to 3.75539, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 30s 296ms/step - loss: 3.7554 - bpp: 3.0747 - mse: 3.3235e-04\n",
      "Epoch 41/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.7091 - bpp: 3.0259 - mse: 3.3355e-04\n",
      "Epoch 41: loss improved from 3.75539 to 3.70906, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 31s 304ms/step - loss: 3.7091 - bpp: 3.0259 - mse: 3.3355e-04\n",
      "Epoch 42/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.7532 - bpp: 2.9896 - mse: 3.7286e-04\n",
      "Epoch 42: loss did not improve from 3.70906\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 3.7532 - bpp: 2.9896 - mse: 3.7286e-04\n",
      "Epoch 43/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.5661 - bpp: 2.9359 - mse: 3.0775e-04\n",
      "Epoch 43: loss improved from 3.70906 to 3.56614, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 3.5661 - bpp: 2.9359 - mse: 3.0775e-04\n",
      "Epoch 44/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.5801 - bpp: 2.8939 - mse: 3.3510e-04\n",
      "Epoch 44: loss did not improve from 3.56614\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 3.5801 - bpp: 2.8939 - mse: 3.3510e-04\n",
      "Epoch 45/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.5347 - bpp: 2.8503 - mse: 3.3415e-04\n",
      "Epoch 45: loss improved from 3.56614 to 3.53469, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 30s 295ms/step - loss: 3.5347 - bpp: 2.8503 - mse: 3.3415e-04\n",
      "Epoch 46/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.4344 - bpp: 2.8012 - mse: 3.0920e-04\n",
      "Epoch 46: loss improved from 3.53469 to 3.43442, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 32s 310ms/step - loss: 3.4344 - bpp: 2.8012 - mse: 3.0920e-04\n",
      "Epoch 47/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.5077 - bpp: 2.7704 - mse: 3.6001e-04\n",
      "Epoch 47: loss did not improve from 3.43442\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 3.5077 - bpp: 2.7704 - mse: 3.6001e-04\n",
      "Epoch 48/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.2378 - bpp: 2.7101 - mse: 2.5767e-04\n",
      "Epoch 48: loss improved from 3.43442 to 3.23784, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 31s 304ms/step - loss: 3.2378 - bpp: 2.7101 - mse: 2.5767e-04\n",
      "Epoch 49/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.3418 - bpp: 2.6747 - mse: 3.2574e-04\n",
      "Epoch 49: loss did not improve from 3.23784\n",
      "100/100 [==============================] - 30s 298ms/step - loss: 3.3418 - bpp: 2.6747 - mse: 3.2574e-04\n",
      "Epoch 50/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.3494 - bpp: 2.6415 - mse: 3.4564e-04\n",
      "Epoch 50: loss did not improve from 3.23784\n",
      "100/100 [==============================] - 28s 277ms/step - loss: 3.3494 - bpp: 2.6415 - mse: 3.4564e-04\n",
      "Epoch 51/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.2086 - bpp: 2.5905 - mse: 3.0182e-04\n",
      "Epoch 51: loss improved from 3.23784 to 3.20859, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 3.2086 - bpp: 2.5905 - mse: 3.0182e-04\n",
      "Epoch 52/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.3056 - bpp: 2.5598 - mse: 3.6417e-04\n",
      "Epoch 52: loss did not improve from 3.20859\n",
      "100/100 [==============================] - 30s 295ms/step - loss: 3.3056 - bpp: 2.5598 - mse: 3.6417e-04\n",
      "Epoch 53/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.0822 - bpp: 2.5058 - mse: 2.8146e-04\n",
      "Epoch 53: loss improved from 3.20859 to 3.08221, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 28s 273ms/step - loss: 3.0822 - bpp: 2.5058 - mse: 2.8146e-04\n",
      "Epoch 54/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.1219 - bpp: 2.4751 - mse: 3.1580e-04\n",
      "Epoch 54: loss did not improve from 3.08221\n",
      "100/100 [==============================] - 30s 296ms/step - loss: 3.1219 - bpp: 2.4751 - mse: 3.1580e-04\n",
      "Epoch 55/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.0148 - bpp: 2.4300 - mse: 2.8552e-04\n",
      "Epoch 55: loss improved from 3.08221 to 3.01476, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 3.0148 - bpp: 2.4300 - mse: 2.8552e-04\n",
      "Epoch 56/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.1547 - bpp: 2.4063 - mse: 3.6545e-04\n",
      "Epoch 56: loss did not improve from 3.01476\n",
      "100/100 [==============================] - 30s 293ms/step - loss: 3.1547 - bpp: 2.4063 - mse: 3.6545e-04\n",
      "Epoch 57/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 9.9042 - bpp: 2.4251 - mse: 0.0037\n",
      "Epoch 57: loss did not improve from 3.01476\n",
      "100/100 [==============================] - 28s 273ms/step - loss: 9.9042 - bpp: 2.4251 - mse: 0.0037\n",
      "Epoch 58/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 5.3114 - bpp: 2.3631 - mse: 0.0014\n",
      "Epoch 58: loss did not improve from 3.01476\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 5.3114 - bpp: 2.3631 - mse: 0.0014\n",
      "Epoch 59/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.9243 - bpp: 2.3132 - mse: 7.8666e-04\n",
      "Epoch 59: loss did not improve from 3.01476\n",
      "100/100 [==============================] - 30s 294ms/step - loss: 3.9243 - bpp: 2.3132 - mse: 7.8666e-04\n",
      "Epoch 60/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.4765 - bpp: 2.2858 - mse: 5.8139e-04\n",
      "Epoch 60: loss did not improve from 3.01476\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 3.4765 - bpp: 2.2858 - mse: 5.8139e-04\n",
      "Epoch 61/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.3232 - bpp: 2.2286 - mse: 5.3450e-04\n",
      "Epoch 61: loss did not improve from 3.01476\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 3.3232 - bpp: 2.2286 - mse: 5.3450e-04\n",
      "Epoch 62/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.9560 - bpp: 2.1800 - mse: 3.7891e-04\n",
      "Epoch 62: loss improved from 3.01476 to 2.95605, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 30s 295ms/step - loss: 2.9560 - bpp: 2.1800 - mse: 3.7891e-04\n",
      "Epoch 63/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.9570 - bpp: 2.1385 - mse: 3.9962e-04\n",
      "Epoch 63: loss did not improve from 2.95605\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 2.9570 - bpp: 2.1385 - mse: 3.9962e-04\n",
      "Epoch 64/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.8081 - bpp: 2.0927 - mse: 3.4935e-04\n",
      "Epoch 64: loss improved from 2.95605 to 2.80813, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 32s 314ms/step - loss: 2.8081 - bpp: 2.0927 - mse: 3.4935e-04\n",
      "Epoch 65/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.6720 - bpp: 2.0596 - mse: 2.9904e-04\n",
      "Epoch 65: loss improved from 2.80813 to 2.67204, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 33s 325ms/step - loss: 2.6720 - bpp: 2.0596 - mse: 2.9904e-04\n",
      "Epoch 66/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.9102 - bpp: 2.0417 - mse: 4.2410e-04\n",
      "Epoch 66: loss did not improve from 2.67204\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 2.9102 - bpp: 2.0417 - mse: 4.2410e-04\n",
      "Epoch 67/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.6580 - bpp: 1.9948 - mse: 3.2383e-04\n",
      "Epoch 67: loss improved from 2.67204 to 2.65798, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 32s 312ms/step - loss: 2.6580 - bpp: 1.9948 - mse: 3.2383e-04\n",
      "Epoch 68/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.5179 - bpp: 1.9609 - mse: 2.7197e-04\n",
      "Epoch 68: loss improved from 2.65798 to 2.51793, saving model to checkpoints_wavelets_haar_Lmbd_2048_epcs_800_es_100_I_QP_27_240x240_CosineDecay_20220429-073347/\n",
      "100/100 [==============================] - 32s 314ms/step - loss: 2.5179 - bpp: 1.9609 - mse: 2.7197e-04\n",
      "Epoch 69/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.6798 - bpp: 1.9592 - mse: 3.5188e-04\n",
      "Epoch 69: loss did not improve from 2.51793\n",
      "100/100 [==============================] - 36s 350ms/step - loss: 2.6798 - bpp: 1.9592 - mse: 3.5188e-04\n",
      "Epoch 70/800\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.5410 - bpp: 1.9079 - mse: 3.0913e-04\n",
      "Epoch 70: loss did not improve from 2.51793\n",
      "100/100 [==============================] - 32s 313ms/step - loss: 2.5410 - bpp: 1.9079 - mse: 3.0913e-04\n"
     ]
    }
   ],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "hist = model.fit(x=data, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE,\n",
    "                callbacks=[\n",
    "                    # Callbacks.MemoryCallback(),\n",
    "                    # Callbacks.LearningRateReducer(),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(filepath=checkponts_new_path, save_weights_only=True, save_freq='epoch', monitor=\"loss\", mode='min',  save_best_only=True, verbose=1), \n",
    "                    tf.keras.callbacks.TerminateOnNaN(),\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=early_stop),\n",
    "                    tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, update_freq=\"epoch\"),            \n",
    "                    ],\n",
    "\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/WindowsDev/DataSets/vimeo_septuplet/sequences/00063/0236/im1.png\n",
      "compress\n",
      "in the compress\n",
      "decompress\n",
      "in decompress\n"
     ]
    }
   ],
   "source": [
    "path = load.load_random_path(\"folder_cloud_test.npy\")\n",
    "i=0\n",
    "out_bin = \"Test_com/test{}.bin\".format(i)\n",
    "out_decom = \"Test_com/testdcom{}.png\".format(i)\n",
    "p_on_test = \"Test_com/test_p_frame{}.png\".format(i)\n",
    "i_on_test = \"Test_com/test_i_frame{}.png\".format(i)\n",
    "\n",
    "i_frame = path + 'im1' + '.png'\n",
    "p_frame = path + 'im2' + '.png'\n",
    "print(i_frame)\n",
    "\n",
    "OpenDVCW.write_png(p_on_test, OpenDVCW.read_png_crop(p_frame, 240, 240))\n",
    "OpenDVCW.write_png(i_on_test, OpenDVCW.read_png_crop(i_frame, 240, 240))\n",
    "\n",
    "OpenDVCW.compress(model, i_frame, p_frame, out_bin, 240, 240)\n",
    "OpenDVCW.decompress(model, i_frame, out_bin, out_decom, 240, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the compress\n",
      "in decompress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as optical_flow_loss_1_layer_call_fn, optical_flow_loss_1_layer_call_and_return_conditional_losses, dwt_1_layer_call_fn, dwt_1_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_sym3_Lmbd_2048_epcs_600_es_100_I_QP_27_240x240_CosineDecay_20220426-130123/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_sym3_Lmbd_2048_epcs_600_es_100_I_QP_27_240x240_CosineDecay_20220426-130123/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(save_name, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/WindowsDev/DataSets/vimeo_septuplet/sequences/00049/0289/im1.png\n",
      "/mnt/WindowsDev/DataSets/vimeo_septuplet/sequences/00049/0289/im2.png\n",
      "compress\n",
      "decompress\n",
      "bin size:  2735 psnr:  38.89004578336327\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from math import log10, sqrt\n",
    "\n",
    "def PSNR(original, compressed):\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal .\n",
    "                  # Therefore PSNR have no importance.\n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * log10(max_pixel / sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "\n",
    "path = load.load_path_n(\"folder_cloud_test.npy\", 0)\n",
    "p_frame_out_bin = \"Test_com/dvcw/p_frame_dvcw.bin\"\n",
    "out_decom = \"Test_com/dvcw/frame1.png\"\n",
    "i_on_test = \"Test_com/frame0.png\"\n",
    "p_on_test = \"Test_com/frame1.png\"\n",
    "\n",
    "i_frame = path + 'im1' + '.png'\n",
    "p_frame = path + 'im2' + '.png'\n",
    "print(i_frame)\n",
    "print(p_frame)\n",
    "\n",
    "# write inputs to disk\n",
    "OpenDVCW.write_png(p_on_test, OpenDVCW.read_png_crop(p_frame, 240, 240))\n",
    "OpenDVCW.write_png(i_on_test, OpenDVCW.read_png_crop(i_frame, 240, 240))\n",
    "\n",
    "\n",
    "OpenDVCW.compress(model, i_frame, p_frame, p_frame_out_bin, 240, 240)\n",
    "OpenDVCW.decompress(model, i_frame, p_frame_out_bin, out_decom, 240, 240)\n",
    "\n",
    "\n",
    "original = cv2.imread(p_on_test)\n",
    "compressed = cv2.imread(out_decom)\n",
    "bin_size = os.path.getsize(p_frame_out_bin)\n",
    "value = PSNR(original, compressed)\n",
    "print(\"bin size: \", bin_size , \"psnr: \", value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
