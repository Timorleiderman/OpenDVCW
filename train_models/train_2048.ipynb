{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/OpenDVCW'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/OpenDVCW\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/ubu-admin/Developer/tensorflow-wavelets\n",
    "%cd /workspaces/OpenDVCW\n",
    "import OpenDVCW\n",
    "import numpy as np\n",
    "import load\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import DataGen\n",
    "import Callbacks\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 700\n",
    "STEPS_PER_EPOCH = 200\n",
    "Height = 240\n",
    "Width = 240\n",
    "Channel = 3\n",
    "lmbda = 2048\n",
    "lr_init = 1e-4\n",
    "early_stop = 15\n",
    "I_QP=27\n",
    "\n",
    "args = OpenDVCW.Arguments()\n",
    "last = 4\n",
    "checkponts_last_path = \"checkpoints_wavelets_L_{}_{}_{}x{}/\".format(lmbda, last, Width, Height)\n",
    "checkponts_new_path = \"checkpoints_wavelets_L_{}_{}_{}x{}/\".format(lmbda, last+1, Width, Height)\n",
    "save_name = \"model_save_wavelets_L_{}_{}_{}x{}\".format(lmbda, last+1, Width, Height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 20:35:21.275973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-30 20:35:21.297335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-30 20:35:21.298131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-30 20:35:21.303090: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-30 20:35:21.305507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-30 20:35:21.306317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-30 20:35:21.307060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-30 20:35:23.947902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-30 20:35:23.948842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-30 20:35:23.949625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-30 20:35:23.950478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10244 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:00:09.0, compute capability: 8.6\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "* [Loading dataset]...\n"
     ]
    }
   ],
   "source": [
    "model = OpenDVCW.OpenDVC(width=Width, height=Height, batch_size=BATCH_SIZE, num_filters=128, lmbda=lmbda)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_init),)\n",
    "print(\"* [Model compiled]...\")\n",
    "\n",
    "print(\"* [Loading dataset]...\")\n",
    "data = DataGen.DataVimeo90kGenerator(\"folder_cloud_test.npy\", \n",
    "                                    BATCH_SIZE,\n",
    "                                    (Height,Width,Channel),\n",
    "                                    Channel,\n",
    "                                    True, \n",
    "                                    I_QP,\n",
    "                                    True)\n",
    "\n",
    "# print(\"Loading weights\")\n",
    "# model.load_weights(checkponts_last_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].trainable = False\n",
    "# model.layers[1].trainable = False\n",
    "# model.layers[2].trainable = True\n",
    "# model.layers[3].trainable = True\n",
    "# model.layers[4].trainable = False\n",
    "# model.layers[5].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv_analysis True\n",
      "mv_synthesis True\n",
      "res_analysis True\n",
      "res_synthesis True\n",
      "wavelets_optical_flow True\n",
      "motion_compensation True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 20:35:49.775696: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-03-30 20:36:28.891546: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/200 [..............................] - ETA: 44s - loss: 427.2516 - bpp: 5.3771 - mse: 0.2060WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2191s vs `on_train_batch_end` time: 0.2858s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2191s vs `on_train_batch_end` time: 0.2858s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA: 0s - loss: 47.4100 - bpp: 5.3080 - mse: 0.0206[MemoryCallback]:  4831464\n",
      "\n",
      "Epoch 1: loss improved from inf to 47.40997, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 101s 223ms/step - loss: 47.4100 - bpp: 5.3080 - mse: 0.0206\n",
      "Epoch 2/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 12.3965 - bpp: 5.1690 - mse: 0.0035[MemoryCallback]:  4993036\n",
      "\n",
      "Epoch 2: loss improved from 47.40997 to 12.39649, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 12.3965 - bpp: 5.1690 - mse: 0.0035\n",
      "Epoch 3/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 10.7611 - bpp: 5.0336 - mse: 0.0028[MemoryCallback]:  5003108\n",
      "\n",
      "Epoch 3: loss improved from 12.39649 to 10.76110, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 10.7611 - bpp: 5.0336 - mse: 0.0028\n",
      "Epoch 4/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 8.6620 - bpp: 4.9006 - mse: 0.0018[MemoryCallback]:  5102112\n",
      "\n",
      "Epoch 4: loss improved from 10.76110 to 8.66196, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 47s 232ms/step - loss: 8.6620 - bpp: 4.9006 - mse: 0.0018\n",
      "Epoch 5/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 8.1428 - bpp: 4.7700 - mse: 0.0016[MemoryCallback]:  5198552\n",
      "\n",
      "Epoch 5: loss improved from 8.66196 to 8.14276, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 8.1428 - bpp: 4.7700 - mse: 0.0016\n",
      "Epoch 6/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 8.3978 - bpp: 4.6420 - mse: 0.0018[MemoryCallback]:  5199352\n",
      "\n",
      "Epoch 6: loss did not improve from 8.14276\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 8.3978 - bpp: 4.6420 - mse: 0.0018\n",
      "Epoch 7/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 7.4553 - bpp: 4.5156 - mse: 0.0014[MemoryCallback]:  5200300\n",
      "\n",
      "Epoch 7: loss improved from 8.14276 to 7.45532, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 43s 212ms/step - loss: 7.4553 - bpp: 4.5156 - mse: 0.0014\n",
      "Epoch 8/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 7.4174 - bpp: 4.3921 - mse: 0.0015[MemoryCallback]:  5296848\n",
      "\n",
      "Epoch 8: loss improved from 7.45532 to 7.41742, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 7.4174 - bpp: 4.3921 - mse: 0.0015\n",
      "Epoch 9/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 6.9566 - bpp: 4.2705 - mse: 0.0013[MemoryCallback]:  5346140\n",
      "\n",
      "Epoch 9: loss improved from 7.41742 to 6.95664, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 6.9566 - bpp: 4.2705 - mse: 0.0013\n",
      "Epoch 10/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 7.0260 - bpp: 4.1506 - mse: 0.0014[MemoryCallback]:  5394020\n",
      "\n",
      "Epoch 10: loss did not improve from 6.95664\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 7.0260 - bpp: 4.1506 - mse: 0.0014\n",
      "Epoch 11/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 6.1292 - bpp: 4.0329 - mse: 0.0010[MemoryCallback]:  5485788\n",
      "\n",
      "Epoch 11: loss improved from 6.95664 to 6.12922, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 6.1292 - bpp: 4.0329 - mse: 0.0010\n",
      "Epoch 12/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 6.0144 - bpp: 3.9183 - mse: 0.0010[MemoryCallback]:  5487372\n",
      "\n",
      "Epoch 12: loss improved from 6.12922 to 6.01436, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 6.0144 - bpp: 3.9183 - mse: 0.0010\n",
      "Epoch 13/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 6.4836 - bpp: 3.8059 - mse: 0.0013[MemoryCallback]:  5495712\n",
      "\n",
      "Epoch 13: loss did not improve from 6.01436\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 6.4836 - bpp: 3.8059 - mse: 0.0013\n",
      "Epoch 14/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.5133 - bpp: 3.6960 - mse: 8.8734e-04[MemoryCallback]:  5496668\n",
      "\n",
      "Epoch 14: loss improved from 6.01436 to 5.51328, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 5.5133 - bpp: 3.6960 - mse: 8.8734e-04\n",
      "Epoch 15/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.5527 - bpp: 3.5889 - mse: 9.5889e-04[MemoryCallback]:  5544856\n",
      "\n",
      "Epoch 15: loss did not improve from 5.51328\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 5.5527 - bpp: 3.5889 - mse: 9.5889e-04\n",
      "Epoch 16/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.3998 - bpp: 3.4825 - mse: 9.3617e-04[MemoryCallback]:  5595324\n",
      "\n",
      "Epoch 16: loss improved from 5.51328 to 5.39983, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 5.3998 - bpp: 3.4825 - mse: 9.3617e-04\n",
      "Epoch 17/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.3503 - bpp: 3.3786 - mse: 9.6277e-04[MemoryCallback]:  5646896\n",
      "\n",
      "Epoch 17: loss improved from 5.39983 to 5.35033, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 5.3503 - bpp: 3.3786 - mse: 9.6277e-04\n",
      "Epoch 18/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.0510 - bpp: 3.2784 - mse: 8.6550e-04[MemoryCallback]:  5748292\n",
      "\n",
      "Epoch 18: loss improved from 5.35033 to 5.05097, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 5.0510 - bpp: 3.2784 - mse: 8.6550e-04\n",
      "Epoch 19/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.9611 - bpp: 3.1789 - mse: 8.7023e-04[MemoryCallback]:  5748536\n",
      "\n",
      "Epoch 19: loss improved from 5.05097 to 4.96111, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 4.9611 - bpp: 3.1789 - mse: 8.7023e-04\n",
      "Epoch 20/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.4021 - bpp: 3.0803 - mse: 0.0011[MemoryCallback]:  5748536\n",
      "\n",
      "Epoch 20: loss did not improve from 4.96111\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 5.4021 - bpp: 3.0803 - mse: 0.0011\n",
      "Epoch 21/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.1752 - bpp: 2.9949 - mse: 0.0011[MemoryCallback]:  5843268\n",
      "\n",
      "Epoch 21: loss did not improve from 4.96111\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 5.1752 - bpp: 2.9949 - mse: 0.0011\n",
      "Epoch 22/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.7495 - bpp: 2.8972 - mse: 9.0448e-04[MemoryCallback]:  5843816\n",
      "\n",
      "Epoch 22: loss improved from 4.96111 to 4.74954, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 4.7495 - bpp: 2.8972 - mse: 9.0448e-04\n",
      "Epoch 23/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.2681 - bpp: 2.8095 - mse: 7.1222e-04[MemoryCallback]:  5845132\n",
      "\n",
      "Epoch 23: loss improved from 4.74954 to 4.26813, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 45s 221ms/step - loss: 4.2681 - bpp: 2.8095 - mse: 7.1222e-04\n",
      "Epoch 24/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.8820 - bpp: 2.7237 - mse: 0.0011[MemoryCallback]:  5848532\n",
      "\n",
      "Epoch 24: loss did not improve from 4.26813\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 4.8820 - bpp: 2.7237 - mse: 0.0011\n",
      "Epoch 25/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.2589 - bpp: 2.6437 - mse: 7.8869e-04[MemoryCallback]:  5901064\n",
      "\n",
      "Epoch 25: loss improved from 4.26813 to 4.25893, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 42s 209ms/step - loss: 4.2589 - bpp: 2.6437 - mse: 7.8869e-04\n",
      "Epoch 26/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.9240 - bpp: 2.5560 - mse: 6.6798e-04[MemoryCallback]:  5948916\n",
      "\n",
      "Epoch 26: loss improved from 4.25893 to 3.92403, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 49s 242ms/step - loss: 3.9240 - bpp: 2.5560 - mse: 6.6798e-04\n",
      "Epoch 27/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.6845 - bpp: 2.4796 - mse: 5.8830e-04[MemoryCallback]:  6000692\n",
      "\n",
      "Epoch 27: loss improved from 3.92403 to 3.68448, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 50s 250ms/step - loss: 3.6845 - bpp: 2.4796 - mse: 5.8830e-04\n",
      "Epoch 28/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.9519 - bpp: 2.4048 - mse: 7.5541e-04[MemoryCallback]:  6049924\n",
      "\n",
      "Epoch 28: loss did not improve from 3.68448\n",
      "200/200 [==============================] - 51s 252ms/step - loss: 3.9519 - bpp: 2.4048 - mse: 7.5541e-04\n",
      "Epoch 29/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.0337 - bpp: 2.3282 - mse: 8.3276e-04[MemoryCallback]:  6098564\n",
      "\n",
      "Epoch 29: loss did not improve from 3.68448\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 4.0337 - bpp: 2.3282 - mse: 8.3276e-04\n",
      "Epoch 30/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.6834 - bpp: 2.2592 - mse: 6.9541e-04[MemoryCallback]:  6099056\n",
      "\n",
      "Epoch 30: loss improved from 3.68448 to 3.68339, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 3.6834 - bpp: 2.2592 - mse: 6.9541e-04\n",
      "Epoch 31/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.7162 - bpp: 2.1865 - mse: 7.4691e-04[MemoryCallback]:  6149476\n",
      "\n",
      "Epoch 31: loss did not improve from 3.68339\n",
      "200/200 [==============================] - 49s 242ms/step - loss: 3.7162 - bpp: 2.1865 - mse: 7.4691e-04\n",
      "Epoch 32/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.3502 - bpp: 2.1186 - mse: 6.0135e-04[MemoryCallback]:  6153532\n",
      "\n",
      "Epoch 32: loss improved from 3.68339 to 3.35015, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 3.3502 - bpp: 2.1186 - mse: 6.0135e-04\n",
      "Epoch 33/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.4326 - bpp: 2.0568 - mse: 6.7180e-04[MemoryCallback]:  6153744\n",
      "\n",
      "Epoch 33: loss did not improve from 3.35015\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 3.4326 - bpp: 2.0568 - mse: 6.7180e-04\n",
      "Epoch 34/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.3636 - bpp: 1.9980 - mse: 6.6679e-04[MemoryCallback]:  6155468\n",
      "\n",
      "Epoch 34: loss did not improve from 3.35015\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 3.3636 - bpp: 1.9980 - mse: 6.6679e-04\n",
      "Epoch 35/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.4870 - bpp: 1.9372 - mse: 7.5675e-04[MemoryCallback]:  6159376\n",
      "\n",
      "Epoch 35: loss did not improve from 3.35015\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 3.4870 - bpp: 1.9372 - mse: 7.5675e-04\n",
      "Epoch 36/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2922 - bpp: 1.8817 - mse: 6.8873e-04[MemoryCallback]:  6210920\n",
      "\n",
      "Epoch 36: loss improved from 3.35015 to 3.29222, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 3.2922 - bpp: 1.8817 - mse: 6.8873e-04\n",
      "Epoch 37/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.1598 - bpp: 1.8120 - mse: 6.5811e-04[MemoryCallback]:  6262644\n",
      "\n",
      "Epoch 37: loss improved from 3.29222 to 3.15978, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 3.1598 - bpp: 1.8120 - mse: 6.5811e-04\n",
      "Epoch 38/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.5110 - bpp: 1.7911 - mse: 0.0013[MemoryCallback]:  6263300\n",
      "\n",
      "Epoch 38: loss did not improve from 3.15978\n",
      "200/200 [==============================] - 43s 212ms/step - loss: 4.5110 - bpp: 1.7911 - mse: 0.0013\n",
      "Epoch 39/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2486 - bpp: 1.7297 - mse: 7.4165e-04[MemoryCallback]:  6263528\n",
      "\n",
      "Epoch 39: loss did not improve from 3.15978\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 3.2486 - bpp: 1.7297 - mse: 7.4165e-04\n",
      "Epoch 40/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.0372 - bpp: 1.6681 - mse: 6.6850e-04[MemoryCallback]:  6364248\n",
      "\n",
      "Epoch 40: loss improved from 3.15978 to 3.03720, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 3.0372 - bpp: 1.6681 - mse: 6.6850e-04\n",
      "Epoch 41/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.8640 - bpp: 1.6212 - mse: 6.0686e-04[MemoryCallback]:  6365596\n",
      "\n",
      "Epoch 41: loss improved from 3.03720 to 2.86401, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 2.8640 - bpp: 1.6212 - mse: 6.0686e-04\n",
      "Epoch 42/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.8858 - bpp: 1.5712 - mse: 6.4187e-04[MemoryCallback]:  6365752\n",
      "\n",
      "Epoch 42: loss did not improve from 2.86401\n",
      "200/200 [==============================] - 50s 246ms/step - loss: 2.8858 - bpp: 1.5712 - mse: 6.4187e-04\n",
      "Epoch 43/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.1561 - bpp: 1.5349 - mse: 7.9159e-04[MemoryCallback]:  6365752\n",
      "\n",
      "Epoch 43: loss did not improve from 2.86401\n",
      "200/200 [==============================] - 43s 214ms/step - loss: 3.1561 - bpp: 1.5349 - mse: 7.9159e-04\n",
      "Epoch 44/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.7327 - bpp: 1.4807 - mse: 6.1135e-04[MemoryCallback]:  6370904\n",
      "\n",
      "Epoch 44: loss improved from 2.86401 to 2.73272, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 2.7327 - bpp: 1.4807 - mse: 6.1135e-04\n",
      "Epoch 45/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.4424 - bpp: 1.4710 - mse: 9.6256e-04[MemoryCallback]:  6371200\n",
      "\n",
      "Epoch 45: loss did not improve from 2.73272\n",
      "200/200 [==============================] - 48s 238ms/step - loss: 3.4424 - bpp: 1.4710 - mse: 9.6256e-04\n",
      "Epoch 46/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.1181 - bpp: 1.4384 - mse: 8.2017e-04[MemoryCallback]:  6421516\n",
      "\n",
      "Epoch 46: loss did not improve from 2.73272\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 3.1181 - bpp: 1.4384 - mse: 8.2017e-04\n",
      "Epoch 47/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.5313 - bpp: 1.3717 - mse: 5.6620e-04[MemoryCallback]:  6422140\n",
      "\n",
      "Epoch 47: loss improved from 2.73272 to 2.53127, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 47s 230ms/step - loss: 2.5313 - bpp: 1.3717 - mse: 5.6620e-04\n",
      "Epoch 48/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.4177 - bpp: 1.3412 - mse: 5.2566e-04[MemoryCallback]:  6422512\n",
      "\n",
      "Epoch 48: loss improved from 2.53127 to 2.41773, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 50s 249ms/step - loss: 2.4177 - bpp: 1.3412 - mse: 5.2566e-04\n",
      "Epoch 49/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.5294 - bpp: 1.3271 - mse: 5.8705e-04[MemoryCallback]:  6422632\n",
      "\n",
      "Epoch 49: loss did not improve from 2.41773\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 2.5294 - bpp: 1.3271 - mse: 5.8705e-04\n",
      "Epoch 50/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.7771 - bpp: 1.3065 - mse: 7.1809e-04[MemoryCallback]:  6422632\n",
      "\n",
      "Epoch 50: loss did not improve from 2.41773\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 2.7771 - bpp: 1.3065 - mse: 7.1809e-04\n",
      "Epoch 51/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.6038 - bpp: 1.2757 - mse: 6.4851e-04[MemoryCallback]:  6422632\n",
      "\n",
      "Epoch 51: loss did not improve from 2.41773\n",
      "200/200 [==============================] - 48s 238ms/step - loss: 2.6038 - bpp: 1.2757 - mse: 6.4851e-04\n",
      "Epoch 52/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.1836 - bpp: 1.2962 - mse: 0.0014[MemoryCallback]:  6422632\n",
      "\n",
      "Epoch 52: loss did not improve from 2.41773\n",
      "200/200 [==============================] - 45s 219ms/step - loss: 4.1836 - bpp: 1.2962 - mse: 0.0014\n",
      "Epoch 53/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.5787 - bpp: 1.2352 - mse: 6.5602e-04[MemoryCallback]:  6426468\n",
      "\n",
      "Epoch 53: loss did not improve from 2.41773\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 2.5787 - bpp: 1.2352 - mse: 6.5602e-04\n",
      "Epoch 54/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.3808 - bpp: 1.2017 - mse: 5.7577e-04[MemoryCallback]:  6426684\n",
      "\n",
      "Epoch 54: loss improved from 2.41773 to 2.38083, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 2.3808 - bpp: 1.2017 - mse: 5.7577e-04\n",
      "Epoch 55/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.5850 - bpp: 1.1940 - mse: 6.7923e-04[MemoryCallback]:  6427352\n",
      "\n",
      "Epoch 55: loss did not improve from 2.38083\n",
      "200/200 [==============================] - 52s 259ms/step - loss: 2.5850 - bpp: 1.1940 - mse: 6.7923e-04\n",
      "Epoch 56/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.3357 - bpp: 1.1542 - mse: 5.7692e-04[MemoryCallback]:  6428292\n",
      "\n",
      "Epoch 56: loss improved from 2.38083 to 2.33574, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 2.3357 - bpp: 1.1542 - mse: 5.7692e-04\n",
      "Epoch 57/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.6736 - bpp: 1.1611 - mse: 7.3851e-04[MemoryCallback]:  6428544\n",
      "\n",
      "Epoch 57: loss did not improve from 2.33574\n",
      "200/200 [==============================] - 49s 241ms/step - loss: 2.6736 - bpp: 1.1611 - mse: 7.3851e-04\n",
      "Epoch 58/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.2179 - bpp: 1.1175 - mse: 5.3729e-04[MemoryCallback]:  6428580\n",
      "\n",
      "Epoch 58: loss improved from 2.33574 to 2.21786, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 48s 238ms/step - loss: 2.2179 - bpp: 1.1175 - mse: 5.3729e-04\n",
      "Epoch 59/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.1790 - bpp: 1.0787 - mse: 5.3722e-04[MemoryCallback]:  6428608\n",
      "\n",
      "Epoch 59: loss improved from 2.21786 to 2.17895, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 46s 226ms/step - loss: 2.1790 - bpp: 1.0787 - mse: 5.3722e-04\n",
      "Epoch 60/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0711 - bpp: 1.0654 - mse: 4.9107e-04[MemoryCallback]:  6428608\n",
      "\n",
      "Epoch 60: loss improved from 2.17895 to 2.07110, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 2.0711 - bpp: 1.0654 - mse: 4.9107e-04\n",
      "Epoch 61/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.1853 - bpp: 1.0700 - mse: 5.4456e-04[MemoryCallback]:  6429016\n",
      "\n",
      "Epoch 61: loss did not improve from 2.07110\n",
      "200/200 [==============================] - 50s 247ms/step - loss: 2.1853 - bpp: 1.0700 - mse: 5.4456e-04\n",
      "Epoch 62/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.1748 - bpp: 1.0509 - mse: 5.4879e-04[MemoryCallback]:  6429328\n",
      "\n",
      "Epoch 62: loss did not improve from 2.07110\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 2.1748 - bpp: 1.0509 - mse: 5.4879e-04\n",
      "Epoch 63/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.9717 - bpp: 1.0213 - mse: 4.6410e-04[MemoryCallback]:  6429344\n",
      "\n",
      "Epoch 63: loss improved from 2.07110 to 1.97175, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 1.9717 - bpp: 1.0213 - mse: 4.6410e-04\n",
      "Epoch 64/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0690 - bpp: 1.0111 - mse: 5.1654e-04[MemoryCallback]:  6429344\n",
      "\n",
      "Epoch 64: loss did not improve from 1.97175\n",
      "200/200 [==============================] - 48s 241ms/step - loss: 2.0690 - bpp: 1.0111 - mse: 5.1654e-04\n",
      "Epoch 65/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.0941 - bpp: 1.0752 - mse: 9.8580e-04[MemoryCallback]:  6429456\n",
      "\n",
      "Epoch 65: loss did not improve from 1.97175\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 3.0941 - bpp: 1.0752 - mse: 9.8580e-04\n",
      "Epoch 66/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0261 - bpp: 1.0006 - mse: 5.0074e-04[MemoryCallback]:  6429576\n",
      "\n",
      "Epoch 66: loss did not improve from 1.97175\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 2.0261 - bpp: 1.0006 - mse: 5.0074e-04\n",
      "Epoch 67/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.1705 - bpp: 0.9917 - mse: 5.7561e-04[MemoryCallback]:  6432108\n",
      "\n",
      "Epoch 67: loss did not improve from 1.97175\n",
      "200/200 [==============================] - 49s 241ms/step - loss: 2.1705 - bpp: 0.9917 - mse: 5.7561e-04\n",
      "Epoch 68/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.6000 - bpp: 0.9769 - mse: 7.9252e-04[MemoryCallback]:  6432360\n",
      "\n",
      "Epoch 68: loss did not improve from 1.97175\n",
      "200/200 [==============================] - 47s 236ms/step - loss: 2.6000 - bpp: 0.9769 - mse: 7.9252e-04\n",
      "Epoch 69/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0012 - bpp: 0.9586 - mse: 5.0906e-04[MemoryCallback]:  6435380\n",
      "\n",
      "Epoch 69: loss did not improve from 1.97175\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 2.0012 - bpp: 0.9586 - mse: 5.0906e-04\n",
      "Epoch 70/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.1629 - bpp: 0.9813 - mse: 5.7699e-04[MemoryCallback]:  6435380\n",
      "\n",
      "Epoch 70: loss did not improve from 1.97175\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 2.1629 - bpp: 0.9813 - mse: 5.7699e-04\n",
      "Epoch 71/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.1174 - bpp: 0.9711 - mse: 5.5972e-04[MemoryCallback]:  6435380\n",
      "\n",
      "Epoch 71: loss did not improve from 1.97175\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 2.1174 - bpp: 0.9711 - mse: 5.5972e-04\n",
      "Epoch 72/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.1776 - bpp: 0.9792 - mse: 5.8515e-04[MemoryCallback]:  6435380\n",
      "\n",
      "Epoch 72: loss did not improve from 1.97175\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 2.1776 - bpp: 0.9792 - mse: 5.8515e-04\n",
      "Epoch 73/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.3015 - bpp: 0.9456 - mse: 6.6207e-04[MemoryCallback]:  6435380\n",
      "\n",
      "Epoch 73: loss did not improve from 1.97175\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 2.3015 - bpp: 0.9456 - mse: 6.6207e-04\n",
      "Epoch 74/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.9679 - bpp: 0.9322 - mse: 5.0572e-04[MemoryCallback]:  6435380\n",
      "\n",
      "Epoch 74: loss improved from 1.97175 to 1.96795, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 1.9679 - bpp: 0.9322 - mse: 5.0572e-04\n",
      "Epoch 75/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.4324 - bpp: 0.9241 - mse: 7.3649e-04[MemoryCallback]:  6435380\n",
      "\n",
      "Epoch 75: loss did not improve from 1.96795\n",
      "200/200 [==============================] - 47s 232ms/step - loss: 2.4324 - bpp: 0.9241 - mse: 7.3649e-04\n",
      "Epoch 76/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.2388 - bpp: 0.9112 - mse: 6.4822e-04[MemoryCallback]:  6435380\n",
      "\n",
      "Epoch 76: loss did not improve from 1.96795\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 2.2388 - bpp: 0.9112 - mse: 6.4822e-04\n",
      "Epoch 77/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0671 - bpp: 0.8872 - mse: 5.7613e-04[MemoryCallback]:  6435380\n",
      "\n",
      "Epoch 77: loss did not improve from 1.96795\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 2.0671 - bpp: 0.8872 - mse: 5.7613e-04\n",
      "Epoch 78/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8605 - bpp: 0.8865 - mse: 4.7555e-04[MemoryCallback]:  6435380\n",
      "\n",
      "Epoch 78: loss improved from 1.96795 to 1.86047, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 1.8605 - bpp: 0.8865 - mse: 4.7555e-04\n",
      "Epoch 79/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7928 - bpp: 0.8775 - mse: 4.4695e-04[MemoryCallback]:  6435624\n",
      "\n",
      "Epoch 79: loss improved from 1.86047 to 1.79282, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 1.7928 - bpp: 0.8775 - mse: 4.4695e-04\n",
      "Epoch 80/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0404 - bpp: 0.8786 - mse: 5.6729e-04[MemoryCallback]:  6435724\n",
      "\n",
      "Epoch 80: loss did not improve from 1.79282\n",
      "200/200 [==============================] - 48s 236ms/step - loss: 2.0404 - bpp: 0.8786 - mse: 5.6729e-04\n",
      "Epoch 81/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8023 - bpp: 0.8708 - mse: 4.5484e-04[MemoryCallback]:  6435788\n",
      "\n",
      "Epoch 81: loss did not improve from 1.79282\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 1.8023 - bpp: 0.8708 - mse: 4.5484e-04\n",
      "Epoch 82/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7849 - bpp: 0.8527 - mse: 4.5515e-04[MemoryCallback]:  6435808\n",
      "\n",
      "Epoch 82: loss improved from 1.79282 to 1.78488, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 48s 238ms/step - loss: 1.7849 - bpp: 0.8527 - mse: 4.5515e-04\n",
      "Epoch 83/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0282 - bpp: 0.8768 - mse: 5.6223e-04[MemoryCallback]:  6435944\n",
      "\n",
      "Epoch 83: loss did not improve from 1.78488\n",
      "200/200 [==============================] - 50s 247ms/step - loss: 2.0282 - bpp: 0.8768 - mse: 5.6223e-04\n",
      "Epoch 84/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8189 - bpp: 0.8420 - mse: 4.7696e-04[MemoryCallback]:  6436208\n",
      "\n",
      "Epoch 84: loss did not improve from 1.78488\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 1.8189 - bpp: 0.8420 - mse: 4.7696e-04\n",
      "Epoch 85/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8132 - bpp: 0.8368 - mse: 4.7676e-04[MemoryCallback]:  6436328\n",
      "\n",
      "Epoch 85: loss did not improve from 1.78488\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 1.8132 - bpp: 0.8368 - mse: 4.7676e-04\n",
      "Epoch 86/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0077 - bpp: 0.8451 - mse: 5.6766e-04[MemoryCallback]:  6436328\n",
      "\n",
      "Epoch 86: loss did not improve from 1.78488\n",
      "200/200 [==============================] - 47s 232ms/step - loss: 2.0077 - bpp: 0.8451 - mse: 5.6766e-04\n",
      "Epoch 87/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.9829 - bpp: 0.8510 - mse: 5.5266e-04[MemoryCallback]:  6436328\n",
      "\n",
      "Epoch 87: loss did not improve from 1.78488\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 1.9829 - bpp: 0.8510 - mse: 5.5266e-04\n",
      "Epoch 88/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7720 - bpp: 0.7930 - mse: 4.7802e-04[MemoryCallback]:  6436328\n",
      "\n",
      "Epoch 88: loss improved from 1.78488 to 1.77201, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 1.7720 - bpp: 0.7930 - mse: 4.7802e-04\n",
      "Epoch 89/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.4688 - bpp: 0.8873 - mse: 0.0017[MemoryCallback]:  6438020\n",
      "\n",
      "Epoch 89: loss did not improve from 1.77201\n",
      "200/200 [==============================] - 52s 256ms/step - loss: 4.4688 - bpp: 0.8873 - mse: 0.0017\n",
      "Epoch 90/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.9903 - bpp: 0.8426 - mse: 5.6039e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 90: loss did not improve from 1.77201\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 1.9903 - bpp: 0.8426 - mse: 5.6039e-04\n",
      "Epoch 91/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7892 - bpp: 0.8269 - mse: 4.6990e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 91: loss did not improve from 1.77201\n",
      "200/200 [==============================] - 49s 242ms/step - loss: 1.7892 - bpp: 0.8269 - mse: 4.6990e-04\n",
      "Epoch 92/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7583 - bpp: 0.8119 - mse: 4.6208e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 92: loss improved from 1.77201 to 1.75827, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 47s 235ms/step - loss: 1.7583 - bpp: 0.8119 - mse: 4.6208e-04\n",
      "Epoch 93/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7152 - bpp: 0.7958 - mse: 4.4891e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 93: loss improved from 1.75827 to 1.71518, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 51s 250ms/step - loss: 1.7152 - bpp: 0.7958 - mse: 4.4891e-04\n",
      "Epoch 94/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8210 - bpp: 0.7936 - mse: 5.0167e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 94: loss did not improve from 1.71518\n",
      "200/200 [==============================] - 51s 251ms/step - loss: 1.8210 - bpp: 0.7936 - mse: 5.0167e-04\n",
      "Epoch 95/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.9826 - bpp: 0.8045 - mse: 5.7524e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 95: loss did not improve from 1.71518\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 1.9826 - bpp: 0.8045 - mse: 5.7524e-04\n",
      "Epoch 96/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8234 - bpp: 0.8121 - mse: 4.9382e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 96: loss did not improve from 1.71518\n",
      "200/200 [==============================] - 50s 246ms/step - loss: 1.8234 - bpp: 0.8121 - mse: 4.9382e-04\n",
      "Epoch 97/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6784 - bpp: 0.7728 - mse: 4.4219e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 97: loss improved from 1.71518 to 1.67840, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 47s 236ms/step - loss: 1.6784 - bpp: 0.7728 - mse: 4.4219e-04\n",
      "Epoch 98/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8171 - bpp: 0.7952 - mse: 4.9901e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 98: loss did not improve from 1.67840\n",
      "200/200 [==============================] - 51s 255ms/step - loss: 1.8171 - bpp: 0.7952 - mse: 4.9901e-04\n",
      "Epoch 99/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8549 - bpp: 0.7555 - mse: 5.3681e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 99: loss did not improve from 1.67840\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 1.8549 - bpp: 0.7555 - mse: 5.3681e-04\n",
      "Epoch 100/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8749 - bpp: 0.7835 - mse: 5.3291e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 100: loss did not improve from 1.67840\n",
      "200/200 [==============================] - 47s 232ms/step - loss: 1.8749 - bpp: 0.7835 - mse: 5.3291e-04\n",
      "Epoch 101/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7407 - bpp: 0.7521 - mse: 4.8268e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch 101: loss did not improve from 1.67840\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 1.7407 - bpp: 0.7521 - mse: 4.8268e-04\n",
      "Epoch 102/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7034 - bpp: 0.7522 - mse: 4.6447e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 101. Reducing Learning Rate from 9.999999747378752e-05 to 9.899999713525176e-05\n",
      "\n",
      "Epoch 102: loss did not improve from 1.67840\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 1.7034 - bpp: 0.7522 - mse: 4.6447e-04\n",
      "Epoch 103/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7101 - bpp: 0.7576 - mse: 4.6512e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 102. Reducing Learning Rate from 9.899999713525176e-05 to 9.801000123843551e-05\n",
      "\n",
      "Epoch 103: loss did not improve from 1.67840\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 1.7101 - bpp: 0.7576 - mse: 4.6512e-04\n",
      "Epoch 104/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8451 - bpp: 0.7766 - mse: 5.2171e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 103. Reducing Learning Rate from 9.801000123843551e-05 to 9.702990064397454e-05\n",
      "\n",
      "Epoch 104: loss did not improve from 1.67840\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 1.8451 - bpp: 0.7766 - mse: 5.2171e-04\n",
      "Epoch 105/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6769 - bpp: 0.7402 - mse: 4.5739e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 104. Reducing Learning Rate from 9.702990064397454e-05 to 9.605960076441988e-05\n",
      "\n",
      "Epoch 105: loss improved from 1.67840 to 1.67690, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 47s 232ms/step - loss: 1.6769 - bpp: 0.7402 - mse: 4.5739e-04\n",
      "Epoch 106/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7497 - bpp: 0.7448 - mse: 4.9064e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 105. Reducing Learning Rate from 9.605960076441988e-05 to 9.509900701232255e-05\n",
      "\n",
      "Epoch 106: loss did not improve from 1.67690\n",
      "200/200 [==============================] - 51s 254ms/step - loss: 1.7497 - bpp: 0.7448 - mse: 4.9064e-04\n",
      "Epoch 107/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6075 - bpp: 0.7215 - mse: 4.3265e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 106. Reducing Learning Rate from 9.509900701232255e-05 to 9.414801752427593e-05\n",
      "\n",
      "Epoch 107: loss improved from 1.67690 to 1.60753, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 48s 236ms/step - loss: 1.6075 - bpp: 0.7215 - mse: 4.3265e-04\n",
      "Epoch 108/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7592 - bpp: 0.7484 - mse: 4.9356e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 107. Reducing Learning Rate from 9.414801752427593e-05 to 9.320653771283105e-05\n",
      "\n",
      "Epoch 108: loss did not improve from 1.60753\n",
      "200/200 [==============================] - 47s 232ms/step - loss: 1.7592 - bpp: 0.7484 - mse: 4.9356e-04\n",
      "Epoch 109/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6276 - bpp: 0.7316 - mse: 4.3750e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 108. Reducing Learning Rate from 9.320653771283105e-05 to 9.227447299053892e-05\n",
      "\n",
      "Epoch 109: loss did not improve from 1.60753\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 1.6276 - bpp: 0.7316 - mse: 4.3750e-04\n",
      "Epoch 110/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5927 - bpp: 0.6985 - mse: 4.3660e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 109. Reducing Learning Rate from 9.227447299053892e-05 to 9.135172876995057e-05\n",
      "\n",
      "Epoch 110: loss improved from 1.60753 to 1.59270, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 1.5927 - bpp: 0.6985 - mse: 4.3660e-04\n",
      "Epoch 111/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8285 - bpp: 0.7397 - mse: 5.3165e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 110. Reducing Learning Rate from 9.135172876995057e-05 to 9.0438210463617e-05\n",
      "\n",
      "Epoch 111: loss did not improve from 1.59270\n",
      "200/200 [==============================] - 48s 238ms/step - loss: 1.8285 - bpp: 0.7397 - mse: 5.3165e-04\n",
      "Epoch 112/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5795 - bpp: 0.7116 - mse: 4.2380e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 111. Reducing Learning Rate from 9.0438210463617e-05 to 8.953383076004684e-05\n",
      "\n",
      "Epoch 112: loss improved from 1.59270 to 1.57952, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 48s 238ms/step - loss: 1.5795 - bpp: 0.7116 - mse: 4.2380e-04\n",
      "Epoch 113/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5967 - bpp: 0.7103 - mse: 4.3285e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 112. Reducing Learning Rate from 8.953383076004684e-05 to 8.863849507179111e-05\n",
      "\n",
      "Epoch 113: loss did not improve from 1.57952\n",
      "200/200 [==============================] - 52s 257ms/step - loss: 1.5967 - bpp: 0.7103 - mse: 4.3285e-04\n",
      "Epoch 114/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4944 - bpp: 0.6983 - mse: 3.8873e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 113. Reducing Learning Rate from 8.863849507179111e-05 to 8.775210881140083e-05\n",
      "\n",
      "Epoch 114: loss improved from 1.57952 to 1.49442, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 1.4944 - bpp: 0.6983 - mse: 3.8873e-04\n",
      "Epoch 115/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7700 - bpp: 0.7174 - mse: 5.1397e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 114. Reducing Learning Rate from 8.775210881140083e-05 to 8.687459194334224e-05\n",
      "\n",
      "Epoch 115: loss did not improve from 1.49442\n",
      "200/200 [==============================] - 50s 246ms/step - loss: 1.7700 - bpp: 0.7174 - mse: 5.1397e-04\n",
      "Epoch 116/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.2945 - bpp: 0.7044 - mse: 7.7643e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 115. Reducing Learning Rate from 8.687459194334224e-05 to 8.600584988016635e-05\n",
      "\n",
      "Epoch 116: loss did not improve from 1.49442\n",
      "200/200 [==============================] - 47s 235ms/step - loss: 2.2945 - bpp: 0.7044 - mse: 7.7643e-04\n",
      "Epoch 117/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6852 - bpp: 0.7046 - mse: 4.7885e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 116. Reducing Learning Rate from 8.600584988016635e-05 to 8.51457953103818e-05\n",
      "\n",
      "Epoch 117: loss did not improve from 1.49442\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 1.6852 - bpp: 0.7046 - mse: 4.7885e-04\n",
      "Epoch 118/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5569 - bpp: 0.6908 - mse: 4.2292e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 117. Reducing Learning Rate from 8.51457953103818e-05 to 8.429434092249721e-05\n",
      "\n",
      "Epoch 118: loss did not improve from 1.49442\n",
      "200/200 [==============================] - 50s 246ms/step - loss: 1.5569 - bpp: 0.6908 - mse: 4.2292e-04\n",
      "Epoch 119/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6532 - bpp: 0.6949 - mse: 4.6789e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 118. Reducing Learning Rate from 8.429434092249721e-05 to 8.345139940502122e-05\n",
      "\n",
      "Epoch 119: loss did not improve from 1.49442\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 1.6532 - bpp: 0.6949 - mse: 4.6789e-04\n",
      "Epoch 120/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6195 - bpp: 0.6929 - mse: 4.5243e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 119. Reducing Learning Rate from 8.345139940502122e-05 to 8.261688344646245e-05\n",
      "\n",
      "Epoch 120: loss did not improve from 1.49442\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 1.6195 - bpp: 0.6929 - mse: 4.5243e-04\n",
      "Epoch 121/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5661 - bpp: 0.6860 - mse: 4.2976e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 120. Reducing Learning Rate from 8.261688344646245e-05 to 8.179071301128715e-05\n",
      "\n",
      "Epoch 121: loss did not improve from 1.49442\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 1.5661 - bpp: 0.6860 - mse: 4.2976e-04\n",
      "Epoch 122/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4072 - bpp: 0.6633 - mse: 3.6321e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 121. Reducing Learning Rate from 8.179071301128715e-05 to 8.097280806396157e-05\n",
      "\n",
      "Epoch 122: loss improved from 1.49442 to 1.40715, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 50s 247ms/step - loss: 1.4072 - bpp: 0.6633 - mse: 3.6321e-04\n",
      "Epoch 123/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3999 - bpp: 0.6571 - mse: 3.6268e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 122. Reducing Learning Rate from 8.097280806396157e-05 to 8.016308129299432e-05\n",
      "\n",
      "Epoch 123: loss improved from 1.40715 to 1.39987, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 54s 265ms/step - loss: 1.3999 - bpp: 0.6571 - mse: 3.6268e-04\n",
      "Epoch 124/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5332 - bpp: 0.6662 - mse: 4.2337e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 123. Reducing Learning Rate from 8.016308129299432e-05 to 7.936145266285166e-05\n",
      "\n",
      "Epoch 124: loss did not improve from 1.39987\n",
      "200/200 [==============================] - 49s 246ms/step - loss: 1.5332 - bpp: 0.6662 - mse: 4.2337e-04\n",
      "Epoch 125/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6248 - bpp: 0.6866 - mse: 4.5811e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 124. Reducing Learning Rate from 7.936145266285166e-05 to 7.856784213799983e-05\n",
      "\n",
      "Epoch 125: loss did not improve from 1.39987\n",
      "200/200 [==============================] - 49s 242ms/step - loss: 1.6248 - bpp: 0.6866 - mse: 4.5811e-04\n",
      "Epoch 126/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5791 - bpp: 0.6766 - mse: 4.4067e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 125. Reducing Learning Rate from 7.856784213799983e-05 to 7.778216240694746e-05\n",
      "\n",
      "Epoch 126: loss did not improve from 1.39987\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 1.5791 - bpp: 0.6766 - mse: 4.4067e-04\n",
      "Epoch 127/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5120 - bpp: 0.6578 - mse: 4.1711e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 126. Reducing Learning Rate from 7.778216240694746e-05 to 7.700434071011841e-05\n",
      "\n",
      "Epoch 127: loss did not improve from 1.39987\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 1.5120 - bpp: 0.6578 - mse: 4.1711e-04\n",
      "Epoch 128/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6202 - bpp: 0.6870 - mse: 4.5565e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 127. Reducing Learning Rate from 7.700434071011841e-05 to 7.623429701197892e-05\n",
      "\n",
      "Epoch 128: loss did not improve from 1.39987\n",
      "200/200 [==============================] - 49s 242ms/step - loss: 1.6202 - bpp: 0.6870 - mse: 4.5565e-04\n",
      "Epoch 129/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4891 - bpp: 0.6615 - mse: 4.0411e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 128. Reducing Learning Rate from 7.623429701197892e-05 to 7.547195127699524e-05\n",
      "\n",
      "Epoch 129: loss did not improve from 1.39987\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 1.4891 - bpp: 0.6615 - mse: 4.0411e-04\n",
      "Epoch 130/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3875 - bpp: 0.6383 - mse: 3.6583e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 129. Reducing Learning Rate from 7.547195127699524e-05 to 7.471723074559122e-05\n",
      "\n",
      "Epoch 130: loss improved from 1.39987 to 1.38752, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 1.3875 - bpp: 0.6383 - mse: 3.6583e-04\n",
      "Epoch 131/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7133 - bpp: 0.6892 - mse: 5.0005e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 130. Reducing Learning Rate from 7.471723074559122e-05 to 7.397006265819073e-05\n",
      "\n",
      "Epoch 131: loss did not improve from 1.38752\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 1.7133 - bpp: 0.6892 - mse: 5.0005e-04\n",
      "Epoch 132/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5579 - bpp: 0.6763 - mse: 4.3049e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 131. Reducing Learning Rate from 7.397006265819073e-05 to 7.323035970330238e-05\n",
      "\n",
      "Epoch 132: loss did not improve from 1.38752\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 1.5579 - bpp: 0.6763 - mse: 4.3049e-04\n",
      "Epoch 133/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8795 - bpp: 0.6687 - mse: 5.9120e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 132. Reducing Learning Rate from 7.323035970330238e-05 to 7.249805639730766e-05\n",
      "\n",
      "Epoch 133: loss did not improve from 1.38752\n",
      "200/200 [==============================] - 47s 233ms/step - loss: 1.8795 - bpp: 0.6687 - mse: 5.9120e-04\n",
      "Epoch 134/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6424 - bpp: 0.6820 - mse: 4.6896e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 133. Reducing Learning Rate from 7.249805639730766e-05 to 7.177307998063043e-05\n",
      "\n",
      "Epoch 134: loss did not improve from 1.38752\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 1.6424 - bpp: 0.6820 - mse: 4.6896e-04\n",
      "Epoch 135/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5943 - bpp: 0.6532 - mse: 4.5953e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 134. Reducing Learning Rate from 7.177307998063043e-05 to 7.105535041773692e-05\n",
      "\n",
      "Epoch 135: loss did not improve from 1.38752\n",
      "200/200 [==============================] - 49s 240ms/step - loss: 1.5943 - bpp: 0.6532 - mse: 4.5953e-04\n",
      "Epoch 136/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5988 - bpp: 0.6598 - mse: 4.5849e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 135. Reducing Learning Rate from 7.105535041773692e-05 to 7.034479494905099e-05\n",
      "\n",
      "Epoch 136: loss did not improve from 1.38752\n",
      "200/200 [==============================] - 52s 260ms/step - loss: 1.5988 - bpp: 0.6598 - mse: 4.5849e-04\n",
      "Epoch 137/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4627 - bpp: 0.6489 - mse: 3.9739e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 136. Reducing Learning Rate from 7.034479494905099e-05 to 6.964134809095412e-05\n",
      "\n",
      "Epoch 137: loss did not improve from 1.38752\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 1.4627 - bpp: 0.6489 - mse: 3.9739e-04\n",
      "Epoch 138/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5309 - bpp: 0.6606 - mse: 4.2498e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 137. Reducing Learning Rate from 6.964134809095412e-05 to 6.894493708387017e-05\n",
      "\n",
      "Epoch 138: loss did not improve from 1.38752\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 1.5309 - bpp: 0.6606 - mse: 4.2498e-04\n",
      "Epoch 139/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5013 - bpp: 0.6574 - mse: 4.1206e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 138. Reducing Learning Rate from 6.894493708387017e-05 to 6.8255489168223e-05\n",
      "\n",
      "Epoch 139: loss did not improve from 1.38752\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 1.5013 - bpp: 0.6574 - mse: 4.1206e-04\n",
      "Epoch 140/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.9099 - bpp: 0.6696 - mse: 6.0559e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 139. Reducing Learning Rate from 6.8255489168223e-05 to 6.757293158443645e-05\n",
      "\n",
      "Epoch 140: loss did not improve from 1.38752\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 1.9099 - bpp: 0.6696 - mse: 6.0559e-04\n",
      "Epoch 141/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5968 - bpp: 0.6666 - mse: 4.5422e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 140. Reducing Learning Rate from 6.757293158443645e-05 to 6.689720612484962e-05\n",
      "\n",
      "Epoch 141: loss did not improve from 1.38752\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 1.5968 - bpp: 0.6666 - mse: 4.5422e-04\n",
      "Epoch 142/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3844 - bpp: 0.6366 - mse: 3.6516e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 141. Reducing Learning Rate from 6.689720612484962e-05 to 6.622823275392875e-05\n",
      "\n",
      "Epoch 142: loss improved from 1.38752 to 1.38442, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 47s 230ms/step - loss: 1.3844 - bpp: 0.6366 - mse: 3.6516e-04\n",
      "Epoch 143/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3322 - bpp: 0.6159 - mse: 3.4974e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 142. Reducing Learning Rate from 6.622823275392875e-05 to 6.556595326401293e-05\n",
      "\n",
      "Epoch 143: loss improved from 1.38442 to 1.33221, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 50s 249ms/step - loss: 1.3322 - bpp: 0.6159 - mse: 3.4974e-04\n",
      "Epoch 144/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4338 - bpp: 0.6473 - mse: 3.8402e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 143. Reducing Learning Rate from 6.556595326401293e-05 to 6.491029489552602e-05\n",
      "\n",
      "Epoch 144: loss did not improve from 1.33221\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 1.4338 - bpp: 0.6473 - mse: 3.8402e-04\n",
      "Epoch 145/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3231 - bpp: 0.6189 - mse: 3.4383e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 144. Reducing Learning Rate from 6.491029489552602e-05 to 6.426119216484949e-05\n",
      "\n",
      "Epoch 145: loss improved from 1.33221 to 1.32305, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 1.3231 - bpp: 0.6189 - mse: 3.4383e-04\n",
      "Epoch 146/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4775 - bpp: 0.6382 - mse: 4.0981e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 145. Reducing Learning Rate from 6.426119216484949e-05 to 6.361857958836481e-05\n",
      "\n",
      "Epoch 146: loss did not improve from 1.32305\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 1.4775 - bpp: 0.6382 - mse: 4.0981e-04\n",
      "Epoch 147/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4578 - bpp: 0.6354 - mse: 4.0156e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 146. Reducing Learning Rate from 6.361857958836481e-05 to 6.298239168245345e-05\n",
      "\n",
      "Epoch 147: loss did not improve from 1.32305\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 1.4578 - bpp: 0.6354 - mse: 4.0156e-04\n",
      "Epoch 148/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3531 - bpp: 0.6223 - mse: 3.5685e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 147. Reducing Learning Rate from 6.298239168245345e-05 to 6.235257023945451e-05\n",
      "\n",
      "Epoch 148: loss did not improve from 1.32305\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 1.3531 - bpp: 0.6223 - mse: 3.5685e-04\n",
      "Epoch 149/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3406 - bpp: 0.6132 - mse: 3.5515e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 148. Reducing Learning Rate from 6.235257023945451e-05 to 6.172904249979183e-05\n",
      "\n",
      "Epoch 149: loss did not improve from 1.32305\n",
      "200/200 [==============================] - 50s 245ms/step - loss: 1.3406 - bpp: 0.6132 - mse: 3.5515e-04\n",
      "Epoch 150/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4210 - bpp: 0.6277 - mse: 3.8737e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 149. Reducing Learning Rate from 6.172904249979183e-05 to 6.111175025580451e-05\n",
      "\n",
      "Epoch 150: loss did not improve from 1.32305\n",
      "200/200 [==============================] - 44s 214ms/step - loss: 1.4210 - bpp: 0.6277 - mse: 3.8737e-04\n",
      "Epoch 151/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3506 - bpp: 0.6229 - mse: 3.5534e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 150. Reducing Learning Rate from 6.111175025580451e-05 to 6.050063166185282e-05\n",
      "\n",
      "Epoch 151: loss did not improve from 1.32305\n",
      "200/200 [==============================] - 43s 212ms/step - loss: 1.3506 - bpp: 0.6229 - mse: 3.5534e-04\n",
      "Epoch 152/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4743 - bpp: 0.6382 - mse: 4.0822e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 151. Reducing Learning Rate from 6.050063166185282e-05 to 5.989562487229705e-05\n",
      "\n",
      "Epoch 152: loss did not improve from 1.32305\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 1.4743 - bpp: 0.6382 - mse: 4.0822e-04\n",
      "Epoch 153/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4366 - bpp: 0.6310 - mse: 3.9335e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 152. Reducing Learning Rate from 5.989562487229705e-05 to 5.929666804149747e-05\n",
      "\n",
      "Epoch 153: loss did not improve from 1.32305\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 1.4366 - bpp: 0.6310 - mse: 3.9335e-04\n",
      "Epoch 154/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3838 - bpp: 0.6163 - mse: 3.7476e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 153. Reducing Learning Rate from 5.929666804149747e-05 to 5.870370296179317e-05\n",
      "\n",
      "Epoch 154: loss did not improve from 1.32305\n",
      "200/200 [==============================] - 43s 214ms/step - loss: 1.3838 - bpp: 0.6163 - mse: 3.7476e-04\n",
      "Epoch 155/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3755 - bpp: 0.6066 - mse: 3.7544e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 154. Reducing Learning Rate from 5.870370296179317e-05 to 5.811666778754443e-05\n",
      "\n",
      "Epoch 155: loss did not improve from 1.32305\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 1.3755 - bpp: 0.6066 - mse: 3.7544e-04\n",
      "Epoch 156/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3714 - bpp: 0.6195 - mse: 3.6715e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 155. Reducing Learning Rate from 5.811666778754443e-05 to 5.753550067311153e-05\n",
      "\n",
      "Epoch 156: loss did not improve from 1.32305\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 1.3714 - bpp: 0.6195 - mse: 3.6715e-04\n",
      "Epoch 157/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2798 - bpp: 0.5814 - mse: 3.4101e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 156. Reducing Learning Rate from 5.753550067311153e-05 to 5.696014704881236e-05\n",
      "\n",
      "Epoch 157: loss improved from 1.32305 to 1.27979, saving model to checkpoints_wavelets_L_2048_5_240x240/\n",
      "200/200 [==============================] - 44s 217ms/step - loss: 1.2798 - bpp: 0.5814 - mse: 3.4101e-04\n",
      "Epoch 158/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6027 - bpp: 0.6140 - mse: 4.8276e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 157. Reducing Learning Rate from 5.696014704881236e-05 to 5.63905450690072e-05\n",
      "\n",
      "Epoch 158: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 1.6027 - bpp: 0.6140 - mse: 4.8276e-04\n",
      "Epoch 159/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3250 - bpp: 0.5991 - mse: 3.5447e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 158. Reducing Learning Rate from 5.63905450690072e-05 to 5.582664016401395e-05\n",
      "\n",
      "Epoch 159: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 1.3250 - bpp: 0.5991 - mse: 3.5447e-04\n",
      "Epoch 160/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3473 - bpp: 0.6015 - mse: 3.6415e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 159. Reducing Learning Rate from 5.582664016401395e-05 to 5.526837412617169e-05\n",
      "\n",
      "Epoch 160: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 1.3473 - bpp: 0.6015 - mse: 3.6415e-04\n",
      "Epoch 161/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3214 - bpp: 0.6061 - mse: 3.4926e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 160. Reducing Learning Rate from 5.526837412617169e-05 to 5.471569238579832e-05\n",
      "\n",
      "Epoch 161: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 1.3214 - bpp: 0.6061 - mse: 3.4926e-04\n",
      "Epoch 162/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3921 - bpp: 0.6195 - mse: 3.7727e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 161. Reducing Learning Rate from 5.471569238579832e-05 to 5.416853673523292e-05\n",
      "\n",
      "Epoch 162: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 1.3921 - bpp: 0.6195 - mse: 3.7727e-04\n",
      "Epoch 163/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3074 - bpp: 0.6038 - mse: 3.4356e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 162. Reducing Learning Rate from 5.416853673523292e-05 to 5.3626852604793385e-05\n",
      "\n",
      "Epoch 163: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 1.3074 - bpp: 0.6038 - mse: 3.4356e-04\n",
      "Epoch 164/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3771 - bpp: 0.6139 - mse: 3.7263e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 163. Reducing Learning Rate from 5.3626852604793385e-05 to 5.309058542479761e-05\n",
      "\n",
      "Epoch 164: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 1.3771 - bpp: 0.6139 - mse: 3.7263e-04\n",
      "Epoch 165/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4723 - bpp: 0.6186 - mse: 4.1687e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 164. Reducing Learning Rate from 5.309058542479761e-05 to 5.255968062556349e-05\n",
      "\n",
      "Epoch 165: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 1.4723 - bpp: 0.6186 - mse: 4.1687e-04\n",
      "Epoch 166/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4802 - bpp: 0.6096 - mse: 4.2512e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 165. Reducing Learning Rate from 5.255968062556349e-05 to 5.203408363740891e-05\n",
      "\n",
      "Epoch 166: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 1.4802 - bpp: 0.6096 - mse: 4.2512e-04\n",
      "Epoch 167/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3508 - bpp: 0.6154 - mse: 3.5905e-04[MemoryCallback]:  6438040\n",
      "\n",
      "Epoch: 166. Reducing Learning Rate from 5.203408363740891e-05 to 5.1513743528630584e-05\n",
      "\n",
      "Epoch 167: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 1.3508 - bpp: 0.6154 - mse: 3.5905e-04\n",
      "Epoch 168/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4595 - bpp: 0.6060 - mse: 4.1675e-04[MemoryCallback]:  6472460\n",
      "\n",
      "Epoch: 167. Reducing Learning Rate from 5.1513743528630584e-05 to 5.09986057295464e-05\n",
      "\n",
      "Epoch 168: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 1.4595 - bpp: 0.6060 - mse: 4.1675e-04\n",
      "Epoch 169/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4842 - bpp: 0.6213 - mse: 4.2131e-04[MemoryCallback]:  6475836\n",
      "\n",
      "Epoch: 168. Reducing Learning Rate from 5.09986057295464e-05 to 5.048861930845305e-05\n",
      "\n",
      "Epoch 169: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 1.4842 - bpp: 0.6213 - mse: 4.2131e-04\n",
      "Epoch 170/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4204 - bpp: 0.6142 - mse: 3.9365e-04[MemoryCallback]:  6475888\n",
      "\n",
      "Epoch: 169. Reducing Learning Rate from 5.048861930845305e-05 to 4.998373333364725e-05\n",
      "\n",
      "Epoch 170: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 47s 236ms/step - loss: 1.4204 - bpp: 0.6142 - mse: 3.9365e-04\n",
      "Epoch 171/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2966 - bpp: 0.5941 - mse: 3.4303e-04[MemoryCallback]:  6475888\n",
      "\n",
      "Epoch: 170. Reducing Learning Rate from 4.998373333364725e-05 to 4.948389687342569e-05\n",
      "\n",
      "Epoch 171: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 1.2966 - bpp: 0.5941 - mse: 3.4303e-04\n",
      "Epoch 172/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3558 - bpp: 0.5981 - mse: 3.6998e-04[MemoryCallback]:  6475888\n",
      "\n",
      "Epoch: 171. Reducing Learning Rate from 4.948389687342569e-05 to 4.898905899608508e-05\n",
      "\n",
      "Epoch 172: loss did not improve from 1.27979\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 1.3558 - bpp: 0.5981 - mse: 3.6998e-04\n"
     ]
    }
   ],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "hist = model.fit(x=data, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE,\n",
    "                callbacks=[\n",
    "                    Callbacks.MemoryCallback(),\n",
    "                    Callbacks.LearningRateReducer(),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(filepath=checkponts_new_path, save_weights_only=True, save_freq='epoch', monitor=\"loss\", mode='min',  save_best_only=True, verbose=1), \n",
    "                    tf.keras.callbacks.TerminateOnNaN(),\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=early_stop),\n",
    "                    tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, update_freq=\"epoch\"),            \n",
    "                    ],\n",
    "\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/WindowsDev/DataSets/vimeo_septuplet/sequences/00059/0223/im1.png\n",
      "compress\n",
      "in the compress\n",
      "in decompress\n"
     ]
    }
   ],
   "source": [
    "path = load.load_random_path(\"folder_cloud_test.npy\")\n",
    "i=0\n",
    "out_bin = \"Test_com/test{}.bin\".format(i)\n",
    "out_decom = \"Test_com/testdcom{}.png\".format(i)\n",
    "p_on_test = \"Test_com/test_p_frame{}.png\".format(i)\n",
    "i_on_test = \"Test_com/test_i_frame{}.png\".format(i)\n",
    "\n",
    "i_frame = path + 'im1' + '.png'\n",
    "p_frame = path + 'im2' + '.png'\n",
    "print(i_frame)\n",
    "\n",
    "OpenDVCW.write_png(p_on_test, OpenDVCW.read_png_crop(p_frame, 240, 240))\n",
    "OpenDVCW.write_png(i_on_test, OpenDVCW.read_png_crop(i_frame, 240, 240))\n",
    "\n",
    "OpenDVCW.compress(model, i_frame, p_frame, out_bin, 240, 240)\n",
    "OpenDVCW.decompress(model, i_frame, out_bin, out_decom, 240, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the compress\n",
      "in decompress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as optical_flow_loss_layer_call_fn, optical_flow_loss_layer_call_and_return_conditional_losses, dwt_layer_call_fn, dwt_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_wavelets_L__2048_5_240x240/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_wavelets_L__2048_5_240x240/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(save_name, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
