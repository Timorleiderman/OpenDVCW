{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/OpenDVCW/train_models'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/OpenDVCW\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/ubu-admin/Developer/tensorflow-wavelets\n",
    "%cd /workspaces/OpenDVCW\n",
    "import OpenDVCW\n",
    "import numpy as np\n",
    "import load\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import DataGen\n",
    "import Callbacks\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 700\n",
    "STEPS_PER_EPOCH = 200\n",
    "Height = 240\n",
    "Width = 240\n",
    "Channel = 3\n",
    "lmbda = 512\n",
    "lr_init = 1e-4\n",
    "early_stop = 15\n",
    "I_QP=27\n",
    "\n",
    "args = OpenDVCW.Arguments()\n",
    "last = 0\n",
    "checkponts_last_path = \"checkpoints_wavelets_L_{}_{}_{}x{}/\".format(lmbda, last, Width, Height)\n",
    "checkponts_new_path = \"checkpoints_wavelets_L_{}_{}_{}x{}/\".format(lmbda, last+1, Width, Height)\n",
    "save_name = \"model_save_wavelets_L_{}_{}_{}x{}\".format(lmbda, last+1, Width, Height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 19:24:43.112398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 19:24:43.180316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 19:24:43.181432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 19:24:43.186398: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-04 19:24:43.189679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 19:24:43.190552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 19:24:43.191300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 19:24:46.535564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 19:24:46.536856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 19:24:46.537897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 19:24:46.539168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10244 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:00:09.0, compute capability: 8.6\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "* [Loading dataset]...\n"
     ]
    }
   ],
   "source": [
    "model = OpenDVCW.OpenDVC(width=Width, height=Height, batch_size=BATCH_SIZE, num_filters=128, lmbda=lmbda)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_init),)\n",
    "print(\"* [Model compiled]...\")\n",
    "\n",
    "print(\"* [Loading dataset]...\")\n",
    "data = DataGen.DataVimeo90kGenerator(\"folder_cloud_test.npy\", \n",
    "                                    BATCH_SIZE,\n",
    "                                    (Height,Width,Channel),\n",
    "                                    Channel,\n",
    "                                    True, \n",
    "                                    I_QP,\n",
    "                                    True)\n",
    "\n",
    "# print(\"Loading weights\")\n",
    "# model.load_weights(checkponts_last_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].trainable = False\n",
    "# model.layers[1].trainable = False\n",
    "# model.layers[2].trainable = True\n",
    "# model.layers[3].trainable = True\n",
    "# model.layers[4].trainable = False\n",
    "# model.layers[5].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv_analysis True\n",
      "mv_synthesis True\n",
      "res_analysis True\n",
      "res_synthesis True\n",
      "wavelets_optical_flow True\n",
      "motion_compensation True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 19:25:23.821558: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-04-04 19:25:55.334797: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA: 0s - loss: 15.8961 - bpp: 5.3103 - mse: 0.0207[MemoryCallback]:  4841388\n",
      "\n",
      "Epoch 1: loss improved from inf to 15.89609, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 86s 177ms/step - loss: 15.8961 - bpp: 5.3103 - mse: 0.0207\n",
      "Epoch 2/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 7.1465 - bpp: 5.1699 - mse: 0.0039[MemoryCallback]:  4991856\n",
      "\n",
      "Epoch 2: loss improved from 15.89609 to 7.14648, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 37s 181ms/step - loss: 7.1465 - bpp: 5.1699 - mse: 0.0039\n",
      "Epoch 3/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 6.2360 - bpp: 5.0332 - mse: 0.0023[MemoryCallback]:  5122928\n",
      "\n",
      "Epoch 3: loss improved from 7.14648 to 6.23604, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 6.2360 - bpp: 5.0332 - mse: 0.0023\n",
      "Epoch 4/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.8792 - bpp: 4.8996 - mse: 0.0019[MemoryCallback]:  5224460\n",
      "\n",
      "Epoch 4: loss improved from 6.23604 to 5.87921, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 34s 171ms/step - loss: 5.8792 - bpp: 4.8996 - mse: 0.0019\n",
      "Epoch 5/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.7142 - bpp: 4.7687 - mse: 0.0018[MemoryCallback]:  5322900\n",
      "\n",
      "Epoch 5: loss improved from 5.87921 to 5.71415, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 38s 190ms/step - loss: 5.7142 - bpp: 4.7687 - mse: 0.0018\n",
      "Epoch 6/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.3763 - bpp: 4.6391 - mse: 0.0014[MemoryCallback]:  5325228\n",
      "\n",
      "Epoch 6: loss improved from 5.71415 to 5.37632, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 37s 181ms/step - loss: 5.3763 - bpp: 4.6391 - mse: 0.0014\n",
      "Epoch 7/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.1874 - bpp: 4.5129 - mse: 0.0013[MemoryCallback]:  5373860\n",
      "\n",
      "Epoch 7: loss improved from 5.37632 to 5.18743, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 5.1874 - bpp: 4.5129 - mse: 0.0013\n",
      "Epoch 8/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.1485 - bpp: 4.3910 - mse: 0.0015[MemoryCallback]:  5421608\n",
      "\n",
      "Epoch 8: loss improved from 5.18743 to 5.14852, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 36s 177ms/step - loss: 5.1485 - bpp: 4.3910 - mse: 0.0015\n",
      "Epoch 9/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.9891 - bpp: 4.2720 - mse: 0.0014[MemoryCallback]:  5473060\n",
      "\n",
      "Epoch 9: loss improved from 5.14852 to 4.98909, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 39s 188ms/step - loss: 4.9891 - bpp: 4.2720 - mse: 0.0014\n",
      "Epoch 10/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.6294 - bpp: 4.1514 - mse: 9.3370e-04[MemoryCallback]:  5525384\n",
      "\n",
      "Epoch 10: loss improved from 4.98909 to 4.62943, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 36s 177ms/step - loss: 4.6294 - bpp: 4.1514 - mse: 9.3370e-04\n",
      "Epoch 11/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.5903 - bpp: 4.0354 - mse: 0.0011[MemoryCallback]:  5525384\n",
      "\n",
      "Epoch 11: loss improved from 4.62943 to 4.59029, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 35s 174ms/step - loss: 4.5903 - bpp: 4.0354 - mse: 0.0011\n",
      "Epoch 12/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.3339 - bpp: 3.9185 - mse: 8.1133e-04[MemoryCallback]:  5573144\n",
      "\n",
      "Epoch 12: loss improved from 4.59029 to 4.33391, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 4.3339 - bpp: 3.9185 - mse: 8.1133e-04\n",
      "Epoch 13/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.2277 - bpp: 3.8065 - mse: 8.2264e-04[MemoryCallback]:  5576008\n",
      "\n",
      "Epoch 13: loss improved from 4.33391 to 4.22765, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 4.2277 - bpp: 3.8065 - mse: 8.2264e-04\n",
      "Epoch 14/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.0380 - bpp: 3.6940 - mse: 6.7199e-04[MemoryCallback]:  5576008\n",
      "\n",
      "Epoch 14: loss improved from 4.22765 to 4.03804, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 37s 181ms/step - loss: 4.0380 - bpp: 3.6940 - mse: 6.7199e-04\n",
      "Epoch 15/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.9186 - bpp: 3.5852 - mse: 6.5107e-04[MemoryCallback]:  5618004\n",
      "\n",
      "Epoch 15: loss improved from 4.03804 to 3.91858, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 35s 175ms/step - loss: 3.9186 - bpp: 3.5852 - mse: 6.5107e-04\n",
      "Epoch 16/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.8672 - bpp: 3.4790 - mse: 7.5814e-04[MemoryCallback]:  5618164\n",
      "\n",
      "Epoch 16: loss improved from 3.91858 to 3.86716, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 35s 174ms/step - loss: 3.8672 - bpp: 3.4790 - mse: 7.5814e-04\n",
      "Epoch 17/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.7220 - bpp: 3.3732 - mse: 6.8130e-04[MemoryCallback]:  5666336\n",
      "\n",
      "Epoch 17: loss improved from 3.86716 to 3.72203, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 3.7220 - bpp: 3.3732 - mse: 6.8130e-04\n",
      "Epoch 18/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.5582 - bpp: 3.2688 - mse: 5.6521e-04[MemoryCallback]:  5715136\n",
      "\n",
      "Epoch 18: loss improved from 3.72203 to 3.55824, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 36s 177ms/step - loss: 3.5582 - bpp: 3.2688 - mse: 5.6521e-04\n",
      "Epoch 19/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.4628 - bpp: 3.1681 - mse: 5.7551e-04[MemoryCallback]:  5715644\n",
      "\n",
      "Epoch 19: loss improved from 3.55824 to 3.46277, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 3.4628 - bpp: 3.1681 - mse: 5.7551e-04\n",
      "Epoch 20/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.3108 - bpp: 3.0636 - mse: 4.8282e-04[MemoryCallback]:  5816484\n",
      "\n",
      "Epoch 20: loss improved from 3.46277 to 3.31080, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 38s 186ms/step - loss: 3.3108 - bpp: 3.0636 - mse: 4.8282e-04\n",
      "Epoch 21/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2290 - bpp: 2.9686 - mse: 5.0876e-04[MemoryCallback]:  5816544\n",
      "\n",
      "Epoch 21: loss improved from 3.31080 to 3.22905, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 39s 192ms/step - loss: 3.2290 - bpp: 2.9686 - mse: 5.0876e-04\n",
      "Epoch 22/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.1380 - bpp: 2.8726 - mse: 5.1839e-04[MemoryCallback]:  5867036\n",
      "\n",
      "Epoch 22: loss improved from 3.22905 to 3.13803, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 3.1380 - bpp: 2.8726 - mse: 5.1839e-04\n",
      "Epoch 23/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.0451 - bpp: 2.7793 - mse: 5.1918e-04[MemoryCallback]:  5867036\n",
      "\n",
      "Epoch 23: loss improved from 3.13803 to 3.04508, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 48s 237ms/step - loss: 3.0451 - bpp: 2.7793 - mse: 5.1918e-04\n",
      "Epoch 24/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.8765 - bpp: 2.6804 - mse: 3.8300e-04[MemoryCallback]:  5867036\n",
      "\n",
      "Epoch 24: loss improved from 3.04508 to 2.87654, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 2.8765 - bpp: 2.6804 - mse: 3.8300e-04\n",
      "Epoch 25/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2116 - bpp: 2.6100 - mse: 0.0012[MemoryCallback]:  5867036\n",
      "\n",
      "Epoch 25: loss did not improve from 2.87654\n",
      "200/200 [==============================] - 55s 273ms/step - loss: 3.2116 - bpp: 2.6100 - mse: 0.0012\n",
      "Epoch 26/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.7696 - bpp: 2.5099 - mse: 5.0714e-04[MemoryCallback]:  5867784\n",
      "\n",
      "Epoch 26: loss improved from 2.87654 to 2.76960, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 2.7696 - bpp: 2.5099 - mse: 5.0714e-04\n",
      "Epoch 27/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.6635 - bpp: 2.4189 - mse: 4.7766e-04[MemoryCallback]:  5916804\n",
      "\n",
      "Epoch 27: loss improved from 2.76960 to 2.66346, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 50s 247ms/step - loss: 2.6635 - bpp: 2.4189 - mse: 4.7766e-04\n",
      "Epoch 28/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.5480 - bpp: 2.3289 - mse: 4.2792e-04[MemoryCallback]:  5917556\n",
      "\n",
      "Epoch 28: loss improved from 2.66346 to 2.54800, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 45s 221ms/step - loss: 2.5480 - bpp: 2.3289 - mse: 4.2792e-04\n",
      "Epoch 29/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.5295 - bpp: 2.2522 - mse: 5.4149e-04[MemoryCallback]:  6013752\n",
      "\n",
      "Epoch 29: loss improved from 2.54800 to 2.52948, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 55s 275ms/step - loss: 2.5295 - bpp: 2.2522 - mse: 5.4149e-04\n",
      "Epoch 30/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.3800 - bpp: 2.1636 - mse: 4.2274e-04[MemoryCallback]:  6065760\n",
      "\n",
      "Epoch 30: loss improved from 2.52948 to 2.38001, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 56s 277ms/step - loss: 2.3800 - bpp: 2.1636 - mse: 4.2274e-04\n",
      "Epoch 31/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.3020 - bpp: 2.0895 - mse: 4.1501e-04[MemoryCallback]:  6114052\n",
      "\n",
      "Epoch 31: loss improved from 2.38001 to 2.30196, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 56s 277ms/step - loss: 2.3020 - bpp: 2.0895 - mse: 4.1501e-04\n",
      "Epoch 32/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.2473 - bpp: 2.0131 - mse: 4.5745e-04[MemoryCallback]:  6117192\n",
      "\n",
      "Epoch 32: loss improved from 2.30196 to 2.24730, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 51s 251ms/step - loss: 2.2473 - bpp: 2.0131 - mse: 4.5745e-04\n",
      "Epoch 33/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.1494 - bpp: 1.9357 - mse: 4.1748e-04[MemoryCallback]:  6120940\n",
      "\n",
      "Epoch 33: loss improved from 2.24730 to 2.14941, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 53s 264ms/step - loss: 2.1494 - bpp: 1.9357 - mse: 4.1748e-04\n",
      "Epoch 34/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0773 - bpp: 1.8602 - mse: 4.2414e-04[MemoryCallback]:  6173464\n",
      "\n",
      "Epoch 34: loss improved from 2.14941 to 2.07732, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 2.0773 - bpp: 1.8602 - mse: 4.2414e-04\n",
      "Epoch 35/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0040 - bpp: 1.7912 - mse: 4.1557e-04[MemoryCallback]:  6173752\n",
      "\n",
      "Epoch 35: loss improved from 2.07732 to 2.00397, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 57s 282ms/step - loss: 2.0040 - bpp: 1.7912 - mse: 4.1557e-04\n",
      "Epoch 36/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8979 - bpp: 1.7081 - mse: 3.7086e-04[MemoryCallback]:  6182720\n",
      "\n",
      "Epoch 36: loss improved from 2.00397 to 1.89793, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 1.8979 - bpp: 1.7081 - mse: 3.7086e-04\n",
      "Epoch 37/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8676 - bpp: 1.6509 - mse: 4.2326e-04[MemoryCallback]:  6182908\n",
      "\n",
      "Epoch 37: loss improved from 1.89793 to 1.86762, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 47s 233ms/step - loss: 1.8676 - bpp: 1.6509 - mse: 4.2326e-04\n",
      "Epoch 38/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8034 - bpp: 1.5850 - mse: 4.2664e-04[MemoryCallback]:  6183236\n",
      "\n",
      "Epoch 38: loss improved from 1.86762 to 1.80345, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 1.8034 - bpp: 1.5850 - mse: 4.2664e-04\n",
      "Epoch 39/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7401 - bpp: 1.5269 - mse: 4.1645e-04[MemoryCallback]:  6187660\n",
      "\n",
      "Epoch 39: loss improved from 1.80345 to 1.74011, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 56s 280ms/step - loss: 1.7401 - bpp: 1.5269 - mse: 4.1645e-04\n",
      "Epoch 40/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7046 - bpp: 1.4698 - mse: 4.5869e-04[MemoryCallback]:  6187920\n",
      "\n",
      "Epoch 40: loss improved from 1.74011 to 1.70463, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 1.7046 - bpp: 1.4698 - mse: 4.5869e-04\n",
      "Epoch 41/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.9755 - bpp: 1.4701 - mse: 0.0029[MemoryCallback]:  6236108\n",
      "\n",
      "Epoch 41: loss did not improve from 1.70463\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 2.9755 - bpp: 1.4701 - mse: 0.0029\n",
      "Epoch 42/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0564 - bpp: 1.3953 - mse: 0.0013[MemoryCallback]:  6237124\n",
      "\n",
      "Epoch 42: loss did not improve from 1.70463\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 2.0564 - bpp: 1.3953 - mse: 0.0013\n",
      "Epoch 43/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8174 - bpp: 1.3398 - mse: 9.3280e-04[MemoryCallback]:  6238632\n",
      "\n",
      "Epoch 43: loss did not improve from 1.70463\n",
      "200/200 [==============================] - 43s 214ms/step - loss: 1.8174 - bpp: 1.3398 - mse: 9.3280e-04\n",
      "Epoch 44/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7800 - bpp: 1.3015 - mse: 9.3471e-04[MemoryCallback]:  6239172\n",
      "\n",
      "Epoch 44: loss did not improve from 1.70463\n",
      "200/200 [==============================] - 44s 217ms/step - loss: 1.7800 - bpp: 1.3015 - mse: 9.3471e-04\n",
      "Epoch 45/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7030 - bpp: 1.2590 - mse: 8.6723e-04[MemoryCallback]:  6239488\n",
      "\n",
      "Epoch 45: loss improved from 1.70463 to 1.70301, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 1.7030 - bpp: 1.2590 - mse: 8.6723e-04\n",
      "Epoch 46/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6209 - bpp: 1.2166 - mse: 7.8959e-04[MemoryCallback]:  6239488\n",
      "\n",
      "Epoch 46: loss improved from 1.70301 to 1.62090, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 1.6209 - bpp: 1.2166 - mse: 7.8959e-04\n",
      "Epoch 47/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5639 - bpp: 1.1679 - mse: 7.7347e-04[MemoryCallback]:  6241228\n",
      "\n",
      "Epoch 47: loss improved from 1.62090 to 1.56395, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 50s 247ms/step - loss: 1.5639 - bpp: 1.1679 - mse: 7.7347e-04\n",
      "Epoch 48/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5538 - bpp: 1.1424 - mse: 8.0350e-04[MemoryCallback]:  6241568\n",
      "\n",
      "Epoch 48: loss improved from 1.56395 to 1.55381, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 1.5538 - bpp: 1.1424 - mse: 8.0350e-04\n",
      "Epoch 49/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4760 - bpp: 1.0947 - mse: 7.4476e-04[MemoryCallback]:  6241568\n",
      "\n",
      "Epoch 49: loss improved from 1.55381 to 1.47601, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 51s 255ms/step - loss: 1.4760 - bpp: 1.0947 - mse: 7.4476e-04\n",
      "Epoch 50/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4751 - bpp: 1.0558 - mse: 8.1890e-04[MemoryCallback]:  6241584\n",
      "\n",
      "Epoch 50: loss improved from 1.47601 to 1.47507, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 1.4751 - bpp: 1.0558 - mse: 8.1890e-04\n",
      "Epoch 51/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4971 - bpp: 1.0365 - mse: 8.9964e-04[MemoryCallback]:  6245404\n",
      "\n",
      "Epoch 51: loss did not improve from 1.47507\n",
      "200/200 [==============================] - 51s 254ms/step - loss: 1.4971 - bpp: 1.0365 - mse: 8.9964e-04\n",
      "Epoch 52/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5253 - bpp: 1.0188 - mse: 9.8916e-04[MemoryCallback]:  6245404\n",
      "\n",
      "Epoch 52: loss did not improve from 1.47507\n",
      "200/200 [==============================] - 47s 230ms/step - loss: 1.5253 - bpp: 1.0188 - mse: 9.8916e-04\n",
      "Epoch 53/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4044 - bpp: 0.9737 - mse: 8.4113e-04[MemoryCallback]:  6246264\n",
      "\n",
      "Epoch 53: loss improved from 1.47507 to 1.40437, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 1.4044 - bpp: 0.9737 - mse: 8.4113e-04\n",
      "Epoch 54/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3500 - bpp: 0.9420 - mse: 7.9683e-04[MemoryCallback]:  6295264\n",
      "\n",
      "Epoch 54: loss improved from 1.40437 to 1.34997, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 47s 231ms/step - loss: 1.3500 - bpp: 0.9420 - mse: 7.9683e-04\n",
      "Epoch 55/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.9458 - bpp: 0.9336 - mse: 0.0020[MemoryCallback]:  6295652\n",
      "\n",
      "Epoch 55: loss did not improve from 1.34997\n",
      "200/200 [==============================] - 63s 311ms/step - loss: 1.9458 - bpp: 0.9336 - mse: 0.0020\n",
      "Epoch 56/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4932 - bpp: 0.9033 - mse: 0.0012[MemoryCallback]:  6296824\n",
      "\n",
      "Epoch 56: loss did not improve from 1.34997\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 1.4932 - bpp: 0.9033 - mse: 0.0012\n",
      "Epoch 57/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4167 - bpp: 0.8843 - mse: 0.0010[MemoryCallback]:  6297340\n",
      "\n",
      "Epoch 57: loss did not improve from 1.34997\n",
      "200/200 [==============================] - 54s 268ms/step - loss: 1.4167 - bpp: 0.8843 - mse: 0.0010\n",
      "Epoch 58/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3356 - bpp: 0.8572 - mse: 9.3447e-04[MemoryCallback]:  6297524\n",
      "\n",
      "Epoch 58: loss improved from 1.34997 to 1.33563, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 1.3356 - bpp: 0.8572 - mse: 9.3447e-04\n",
      "Epoch 59/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2289 - bpp: 0.8241 - mse: 7.9064e-04[MemoryCallback]:  6297928\n",
      "\n",
      "Epoch 59: loss improved from 1.33563 to 1.22895, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 1.2289 - bpp: 0.8241 - mse: 7.9064e-04\n",
      "Epoch 60/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2180 - bpp: 0.8001 - mse: 8.1604e-04[MemoryCallback]:  6298124\n",
      "\n",
      "Epoch 60: loss improved from 1.22895 to 1.21796, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 1.2180 - bpp: 0.8001 - mse: 8.1604e-04\n",
      "Epoch 61/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1722 - bpp: 0.7814 - mse: 7.6337e-04[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 61: loss improved from 1.21796 to 1.17220, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 1.1722 - bpp: 0.7814 - mse: 7.6337e-04\n",
      "Epoch 62/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3171 - bpp: 0.7784 - mse: 0.0011[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 62: loss did not improve from 1.17220\n",
      "200/200 [==============================] - 43s 211ms/step - loss: 1.3171 - bpp: 0.7784 - mse: 0.0011\n",
      "Epoch 63/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1686 - bpp: 0.7413 - mse: 8.3453e-04[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 63: loss improved from 1.17220 to 1.16857, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 1.1686 - bpp: 0.7413 - mse: 8.3453e-04\n",
      "Epoch 64/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1974 - bpp: 0.7426 - mse: 8.8827e-04[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 64: loss did not improve from 1.16857\n",
      "200/200 [==============================] - 43s 211ms/step - loss: 1.1974 - bpp: 0.7426 - mse: 8.8827e-04\n",
      "Epoch 65/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1855 - bpp: 0.7134 - mse: 9.2210e-04[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 65: loss did not improve from 1.16857\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 1.1855 - bpp: 0.7134 - mse: 9.2210e-04\n",
      "Epoch 66/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1429 - bpp: 0.7011 - mse: 8.6292e-04[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 66: loss improved from 1.16857 to 1.14295, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 42s 209ms/step - loss: 1.1429 - bpp: 0.7011 - mse: 8.6292e-04\n",
      "Epoch 67/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1655 - bpp: 0.7034 - mse: 9.0260e-04[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 67: loss did not improve from 1.14295\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 1.1655 - bpp: 0.7034 - mse: 9.0260e-04\n",
      "Epoch 68/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1439 - bpp: 0.6736 - mse: 9.1861e-04[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 68: loss did not improve from 1.14295\n",
      "200/200 [==============================] - 38s 187ms/step - loss: 1.1439 - bpp: 0.6736 - mse: 9.1861e-04\n",
      "Epoch 69/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0340 - bpp: 0.6437 - mse: 7.6230e-04[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 69: loss improved from 1.14295 to 1.03396, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 1.0340 - bpp: 0.6437 - mse: 7.6230e-04\n",
      "Epoch 70/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1395 - bpp: 0.6309 - mse: 9.9331e-04[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 70: loss did not improve from 1.03396\n",
      "200/200 [==============================] - 50s 250ms/step - loss: 1.1395 - bpp: 0.6309 - mse: 9.9331e-04\n",
      "Epoch 71/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0829 - bpp: 0.6279 - mse: 8.8864e-04[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 71: loss did not improve from 1.03396\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 1.0829 - bpp: 0.6279 - mse: 8.8864e-04\n",
      "Epoch 72/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1386 - bpp: 0.6232 - mse: 0.0010[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 72: loss did not improve from 1.03396\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 1.1386 - bpp: 0.6232 - mse: 0.0010\n",
      "Epoch 73/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2537 - bpp: 0.6286 - mse: 0.0012[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 73: loss did not improve from 1.03396\n",
      "200/200 [==============================] - 41s 202ms/step - loss: 1.2537 - bpp: 0.6286 - mse: 0.0012\n",
      "Epoch 74/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0386 - bpp: 0.5932 - mse: 8.6988e-04[MemoryCallback]:  6301916\n",
      "\n",
      "Epoch 74: loss did not improve from 1.03396\n",
      "200/200 [==============================] - 38s 190ms/step - loss: 1.0386 - bpp: 0.5932 - mse: 8.6988e-04\n",
      "Epoch 75/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0252 - bpp: 0.5908 - mse: 8.4847e-04[MemoryCallback]:  6348008\n",
      "\n",
      "Epoch 75: loss improved from 1.03396 to 1.02524, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 39s 190ms/step - loss: 1.0252 - bpp: 0.5908 - mse: 8.4847e-04\n",
      "Epoch 76/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9515 - bpp: 0.5579 - mse: 7.6869e-04[MemoryCallback]:  6348464\n",
      "\n",
      "Epoch 76: loss improved from 1.02524 to 0.95147, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 0.9515 - bpp: 0.5579 - mse: 7.6869e-04\n",
      "Epoch 77/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0080 - bpp: 0.5610 - mse: 8.7300e-04[MemoryCallback]:  6348728\n",
      "\n",
      "Epoch 77: loss did not improve from 0.95147\n",
      "200/200 [==============================] - 41s 199ms/step - loss: 1.0080 - bpp: 0.5610 - mse: 8.7300e-04\n",
      "Epoch 78/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9436 - bpp: 0.5381 - mse: 7.9189e-04[MemoryCallback]:  6396508\n",
      "\n",
      "Epoch 78: loss improved from 0.95147 to 0.94356, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.9436 - bpp: 0.5381 - mse: 7.9189e-04\n",
      "Epoch 79/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9706 - bpp: 0.5466 - mse: 8.2824e-04[MemoryCallback]:  6396668\n",
      "\n",
      "Epoch 79: loss did not improve from 0.94356\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.9706 - bpp: 0.5466 - mse: 8.2824e-04\n",
      "Epoch 80/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6447 - bpp: 0.5790 - mse: 0.0021[MemoryCallback]:  6444328\n",
      "\n",
      "Epoch 80: loss did not improve from 0.94356\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 1.6447 - bpp: 0.5790 - mse: 0.0021\n",
      "Epoch 81/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0586 - bpp: 0.5470 - mse: 9.9931e-04[MemoryCallback]:  6444488\n",
      "\n",
      "Epoch 81: loss did not improve from 0.94356\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 1.0586 - bpp: 0.5470 - mse: 9.9931e-04\n",
      "Epoch 82/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0317 - bpp: 0.5377 - mse: 9.6488e-04[MemoryCallback]:  6444488\n",
      "\n",
      "Epoch 82: loss did not improve from 0.94356\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 1.0317 - bpp: 0.5377 - mse: 9.6488e-04\n",
      "Epoch 83/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9481 - bpp: 0.5102 - mse: 8.5537e-04[MemoryCallback]:  6444696\n",
      "\n",
      "Epoch 83: loss did not improve from 0.94356\n",
      "200/200 [==============================] - 40s 197ms/step - loss: 0.9481 - bpp: 0.5102 - mse: 8.5537e-04\n",
      "Epoch 84/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9620 - bpp: 0.5125 - mse: 8.7785e-04[MemoryCallback]:  6448652\n",
      "\n",
      "Epoch 84: loss did not improve from 0.94356\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.9620 - bpp: 0.5125 - mse: 8.7785e-04\n",
      "Epoch 85/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9408 - bpp: 0.5057 - mse: 8.4966e-04[MemoryCallback]:  6448864\n",
      "\n",
      "Epoch 85: loss improved from 0.94356 to 0.94077, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 0.9408 - bpp: 0.5057 - mse: 8.4966e-04\n",
      "Epoch 86/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8623 - bpp: 0.4791 - mse: 7.4838e-04[MemoryCallback]:  6450060\n",
      "\n",
      "Epoch 86: loss improved from 0.94077 to 0.86225, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.8623 - bpp: 0.4791 - mse: 7.4838e-04\n",
      "Epoch 87/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8888 - bpp: 0.4775 - mse: 8.0324e-04[MemoryCallback]:  6450212\n",
      "\n",
      "Epoch 87: loss did not improve from 0.86225\n",
      "200/200 [==============================] - 41s 201ms/step - loss: 0.8888 - bpp: 0.4775 - mse: 8.0324e-04\n",
      "Epoch 88/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8497 - bpp: 0.4643 - mse: 7.5263e-04[MemoryCallback]:  6452728\n",
      "\n",
      "Epoch 88: loss improved from 0.86225 to 0.84969, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.8497 - bpp: 0.4643 - mse: 7.5263e-04\n",
      "Epoch 89/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8215 - bpp: 0.4627 - mse: 7.0074e-04[MemoryCallback]:  6452868\n",
      "\n",
      "Epoch 89: loss improved from 0.84969 to 0.82149, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 42s 206ms/step - loss: 0.8215 - bpp: 0.4627 - mse: 7.0074e-04\n",
      "Epoch 90/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8413 - bpp: 0.4511 - mse: 7.6212e-04[MemoryCallback]:  6455116\n",
      "\n",
      "Epoch 90: loss did not improve from 0.82149\n",
      "200/200 [==============================] - 39s 192ms/step - loss: 0.8413 - bpp: 0.4511 - mse: 7.6212e-04\n",
      "Epoch 91/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8904 - bpp: 0.4551 - mse: 8.5016e-04[MemoryCallback]:  6455708\n",
      "\n",
      "Epoch 91: loss did not improve from 0.82149\n",
      "200/200 [==============================] - 43s 211ms/step - loss: 0.8904 - bpp: 0.4551 - mse: 8.5016e-04\n",
      "Epoch 92/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8406 - bpp: 0.4433 - mse: 7.7593e-04[MemoryCallback]:  6455828\n",
      "\n",
      "Epoch 92: loss did not improve from 0.82149\n",
      "200/200 [==============================] - 39s 191ms/step - loss: 0.8406 - bpp: 0.4433 - mse: 7.7593e-04\n",
      "Epoch 93/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8302 - bpp: 0.4352 - mse: 7.7162e-04[MemoryCallback]:  6460984\n",
      "\n",
      "Epoch 93: loss did not improve from 0.82149\n",
      "200/200 [==============================] - 39s 193ms/step - loss: 0.8302 - bpp: 0.4352 - mse: 7.7162e-04\n",
      "Epoch 94/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8502 - bpp: 0.4286 - mse: 8.2331e-04[MemoryCallback]:  6461220\n",
      "\n",
      "Epoch 94: loss did not improve from 0.82149\n",
      "200/200 [==============================] - 39s 192ms/step - loss: 0.8502 - bpp: 0.4286 - mse: 8.2331e-04\n",
      "Epoch 95/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8945 - bpp: 0.4338 - mse: 8.9970e-04[MemoryCallback]:  6461220\n",
      "\n",
      "Epoch 95: loss did not improve from 0.82149\n",
      "200/200 [==============================] - 39s 192ms/step - loss: 0.8945 - bpp: 0.4338 - mse: 8.9970e-04\n",
      "Epoch 96/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8128 - bpp: 0.4175 - mse: 7.7201e-04[MemoryCallback]:  6461220\n",
      "\n",
      "Epoch 96: loss improved from 0.82149 to 0.81275, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.8128 - bpp: 0.4175 - mse: 7.7201e-04\n",
      "Epoch 97/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7777 - bpp: 0.4106 - mse: 7.1708e-04[MemoryCallback]:  6461220\n",
      "\n",
      "Epoch 97: loss improved from 0.81275 to 0.77771, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 38s 190ms/step - loss: 0.7777 - bpp: 0.4106 - mse: 7.1708e-04\n",
      "Epoch 98/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8734 - bpp: 0.4179 - mse: 8.8959e-04[MemoryCallback]:  6461220\n",
      "\n",
      "Epoch 98: loss did not improve from 0.77771\n",
      "200/200 [==============================] - 38s 187ms/step - loss: 0.8734 - bpp: 0.4179 - mse: 8.8959e-04\n",
      "Epoch 99/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8147 - bpp: 0.4113 - mse: 7.8791e-04[MemoryCallback]:  6461624\n",
      "\n",
      "Epoch 99: loss did not improve from 0.77771\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 0.8147 - bpp: 0.4113 - mse: 7.8791e-04\n",
      "Epoch 100/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7803 - bpp: 0.3954 - mse: 7.5178e-04[MemoryCallback]:  6465692\n",
      "\n",
      "Epoch 100: loss did not improve from 0.77771\n",
      "200/200 [==============================] - 40s 196ms/step - loss: 0.7803 - bpp: 0.3954 - mse: 7.5178e-04\n",
      "Epoch 101/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7521 - bpp: 0.3911 - mse: 7.0511e-04[MemoryCallback]:  6468948\n",
      "\n",
      "Epoch 101: loss improved from 0.77771 to 0.75214, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 39s 193ms/step - loss: 0.7521 - bpp: 0.3911 - mse: 7.0511e-04\n",
      "Epoch 102/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8005 - bpp: 0.3911 - mse: 7.9952e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 101. Reducing Learning Rate from 9.999999747378752e-05 to 9.899999713525176e-05\n",
      "\n",
      "Epoch 102: loss did not improve from 0.75214\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.8005 - bpp: 0.3911 - mse: 7.9952e-04\n",
      "Epoch 103/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8145 - bpp: 0.3948 - mse: 8.1981e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 102. Reducing Learning Rate from 9.899999713525176e-05 to 9.801000123843551e-05\n",
      "\n",
      "Epoch 103: loss did not improve from 0.75214\n",
      "200/200 [==============================] - 39s 194ms/step - loss: 0.8145 - bpp: 0.3948 - mse: 8.1981e-04\n",
      "Epoch 104/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7709 - bpp: 0.3837 - mse: 7.5628e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 103. Reducing Learning Rate from 9.801000123843551e-05 to 9.702990064397454e-05\n",
      "\n",
      "Epoch 104: loss did not improve from 0.75214\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 0.7709 - bpp: 0.3837 - mse: 7.5628e-04\n",
      "Epoch 105/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8179 - bpp: 0.3843 - mse: 8.4682e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 104. Reducing Learning Rate from 9.702990064397454e-05 to 9.605960076441988e-05\n",
      "\n",
      "Epoch 105: loss did not improve from 0.75214\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 0.8179 - bpp: 0.3843 - mse: 8.4682e-04\n",
      "Epoch 106/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7680 - bpp: 0.3807 - mse: 7.5649e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 105. Reducing Learning Rate from 9.605960076441988e-05 to 9.509900701232255e-05\n",
      "\n",
      "Epoch 106: loss did not improve from 0.75214\n",
      "200/200 [==============================] - 38s 191ms/step - loss: 0.7680 - bpp: 0.3807 - mse: 7.5649e-04\n",
      "Epoch 107/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7915 - bpp: 0.3761 - mse: 8.1136e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 106. Reducing Learning Rate from 9.509900701232255e-05 to 9.414801752427593e-05\n",
      "\n",
      "Epoch 107: loss did not improve from 0.75214\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 0.7915 - bpp: 0.3761 - mse: 8.1136e-04\n",
      "Epoch 108/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8464 - bpp: 0.3763 - mse: 9.1811e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 107. Reducing Learning Rate from 9.414801752427593e-05 to 9.320653771283105e-05\n",
      "\n",
      "Epoch 108: loss did not improve from 0.75214\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 0.8464 - bpp: 0.3763 - mse: 9.1811e-04\n",
      "Epoch 109/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8229 - bpp: 0.3789 - mse: 8.6725e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 108. Reducing Learning Rate from 9.320653771283105e-05 to 9.227447299053892e-05\n",
      "\n",
      "Epoch 109: loss did not improve from 0.75214\n",
      "200/200 [==============================] - 40s 196ms/step - loss: 0.8229 - bpp: 0.3789 - mse: 8.6725e-04\n",
      "Epoch 110/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7543 - bpp: 0.3659 - mse: 7.5868e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 109. Reducing Learning Rate from 9.227447299053892e-05 to 9.135172876995057e-05\n",
      "\n",
      "Epoch 110: loss did not improve from 0.75214\n",
      "200/200 [==============================] - 39s 193ms/step - loss: 0.7543 - bpp: 0.3659 - mse: 7.5868e-04\n",
      "Epoch 111/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7616 - bpp: 0.3668 - mse: 7.7107e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 110. Reducing Learning Rate from 9.135172876995057e-05 to 9.0438210463617e-05\n",
      "\n",
      "Epoch 111: loss did not improve from 0.75214\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 0.7616 - bpp: 0.3668 - mse: 7.7107e-04\n",
      "Epoch 112/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7117 - bpp: 0.3562 - mse: 6.9426e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 111. Reducing Learning Rate from 9.0438210463617e-05 to 8.953383076004684e-05\n",
      "\n",
      "Epoch 112: loss improved from 0.75214 to 0.71168, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 0.7117 - bpp: 0.3562 - mse: 6.9426e-04\n",
      "Epoch 113/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8241 - bpp: 0.3754 - mse: 8.7639e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 112. Reducing Learning Rate from 8.953383076004684e-05 to 8.863849507179111e-05\n",
      "\n",
      "Epoch 113: loss did not improve from 0.71168\n",
      "200/200 [==============================] - 52s 256ms/step - loss: 0.8241 - bpp: 0.3754 - mse: 8.7639e-04\n",
      "Epoch 114/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7421 - bpp: 0.3560 - mse: 7.5406e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 113. Reducing Learning Rate from 8.863849507179111e-05 to 8.775210881140083e-05\n",
      "\n",
      "Epoch 114: loss did not improve from 0.71168\n",
      "200/200 [==============================] - 40s 196ms/step - loss: 0.7421 - bpp: 0.3560 - mse: 7.5406e-04\n",
      "Epoch 115/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6839 - bpp: 0.3416 - mse: 6.6839e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 114. Reducing Learning Rate from 8.775210881140083e-05 to 8.687459194334224e-05\n",
      "\n",
      "Epoch 115: loss improved from 0.71168 to 0.68385, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 48s 236ms/step - loss: 0.6839 - bpp: 0.3416 - mse: 6.6839e-04\n",
      "Epoch 116/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7113 - bpp: 0.3469 - mse: 7.1171e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 115. Reducing Learning Rate from 8.687459194334224e-05 to 8.600584988016635e-05\n",
      "\n",
      "Epoch 116: loss did not improve from 0.68385\n",
      "200/200 [==============================] - 41s 201ms/step - loss: 0.7113 - bpp: 0.3469 - mse: 7.1171e-04\n",
      "Epoch 117/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7598 - bpp: 0.3571 - mse: 7.8662e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 116. Reducing Learning Rate from 8.600584988016635e-05 to 8.51457953103818e-05\n",
      "\n",
      "Epoch 117: loss did not improve from 0.68385\n",
      "200/200 [==============================] - 41s 206ms/step - loss: 0.7598 - bpp: 0.3571 - mse: 7.8662e-04\n",
      "Epoch 118/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7131 - bpp: 0.3468 - mse: 7.1531e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 117. Reducing Learning Rate from 8.51457953103818e-05 to 8.429434092249721e-05\n",
      "\n",
      "Epoch 118: loss did not improve from 0.68385\n",
      "200/200 [==============================] - 39s 192ms/step - loss: 0.7131 - bpp: 0.3468 - mse: 7.1531e-04\n",
      "Epoch 119/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7025 - bpp: 0.3392 - mse: 7.0955e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 118. Reducing Learning Rate from 8.429434092249721e-05 to 8.345139940502122e-05\n",
      "\n",
      "Epoch 119: loss did not improve from 0.68385\n",
      "200/200 [==============================] - 39s 193ms/step - loss: 0.7025 - bpp: 0.3392 - mse: 7.0955e-04\n",
      "Epoch 120/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6756 - bpp: 0.3304 - mse: 6.7410e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 119. Reducing Learning Rate from 8.345139940502122e-05 to 8.261688344646245e-05\n",
      "\n",
      "Epoch 120: loss improved from 0.68385 to 0.67559, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 0.6756 - bpp: 0.3304 - mse: 6.7410e-04\n",
      "Epoch 121/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6772 - bpp: 0.3258 - mse: 6.8635e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 120. Reducing Learning Rate from 8.261688344646245e-05 to 8.179071301128715e-05\n",
      "\n",
      "Epoch 121: loss did not improve from 0.67559\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 0.6772 - bpp: 0.3258 - mse: 6.8635e-04\n",
      "Epoch 122/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7543 - bpp: 0.3509 - mse: 7.8777e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 121. Reducing Learning Rate from 8.179071301128715e-05 to 8.097280806396157e-05\n",
      "\n",
      "Epoch 122: loss did not improve from 0.67559\n",
      "200/200 [==============================] - 40s 197ms/step - loss: 0.7543 - bpp: 0.3509 - mse: 7.8777e-04\n",
      "Epoch 123/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7617 - bpp: 0.3442 - mse: 8.1530e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 122. Reducing Learning Rate from 8.097280806396157e-05 to 8.016308129299432e-05\n",
      "\n",
      "Epoch 123: loss did not improve from 0.67559\n",
      "200/200 [==============================] - 44s 216ms/step - loss: 0.7617 - bpp: 0.3442 - mse: 8.1530e-04\n",
      "Epoch 124/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7368 - bpp: 0.3338 - mse: 7.8724e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 123. Reducing Learning Rate from 8.016308129299432e-05 to 7.936145266285166e-05\n",
      "\n",
      "Epoch 124: loss did not improve from 0.67559\n",
      "200/200 [==============================] - 39s 194ms/step - loss: 0.7368 - bpp: 0.3338 - mse: 7.8724e-04\n",
      "Epoch 125/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6833 - bpp: 0.3326 - mse: 6.8503e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 124. Reducing Learning Rate from 7.936145266285166e-05 to 7.856784213799983e-05\n",
      "\n",
      "Epoch 125: loss did not improve from 0.67559\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 0.6833 - bpp: 0.3326 - mse: 6.8503e-04\n",
      "Epoch 126/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6701 - bpp: 0.3251 - mse: 6.7377e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 125. Reducing Learning Rate from 7.856784213799983e-05 to 7.778216240694746e-05\n",
      "\n",
      "Epoch 126: loss improved from 0.67559 to 0.67011, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 0.6701 - bpp: 0.3251 - mse: 6.7377e-04\n",
      "Epoch 127/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6794 - bpp: 0.3230 - mse: 6.9592e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 126. Reducing Learning Rate from 7.778216240694746e-05 to 7.700434071011841e-05\n",
      "\n",
      "Epoch 127: loss did not improve from 0.67011\n",
      "200/200 [==============================] - 55s 274ms/step - loss: 0.6794 - bpp: 0.3230 - mse: 6.9592e-04\n",
      "Epoch 128/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6532 - bpp: 0.3205 - mse: 6.4973e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 127. Reducing Learning Rate from 7.700434071011841e-05 to 7.623429701197892e-05\n",
      "\n",
      "Epoch 128: loss improved from 0.67011 to 0.65316, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 0.6532 - bpp: 0.3205 - mse: 6.4973e-04\n",
      "Epoch 129/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6495 - bpp: 0.3143 - mse: 6.5468e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 128. Reducing Learning Rate from 7.623429701197892e-05 to 7.547195127699524e-05\n",
      "\n",
      "Epoch 129: loss improved from 0.65316 to 0.64955, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 0.6495 - bpp: 0.3143 - mse: 6.5468e-04\n",
      "Epoch 130/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8074 - bpp: 0.3355 - mse: 9.2163e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 129. Reducing Learning Rate from 7.547195127699524e-05 to 7.471723074559122e-05\n",
      "\n",
      "Epoch 130: loss did not improve from 0.64955\n",
      "200/200 [==============================] - 39s 193ms/step - loss: 0.8074 - bpp: 0.3355 - mse: 9.2163e-04\n",
      "Epoch 131/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6559 - bpp: 0.3177 - mse: 6.6057e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 130. Reducing Learning Rate from 7.471723074559122e-05 to 7.397006265819073e-05\n",
      "\n",
      "Epoch 131: loss did not improve from 0.64955\n",
      "200/200 [==============================] - 40s 197ms/step - loss: 0.6559 - bpp: 0.3177 - mse: 6.6057e-04\n",
      "Epoch 132/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7657 - bpp: 0.3329 - mse: 8.4531e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 131. Reducing Learning Rate from 7.397006265819073e-05 to 7.323035970330238e-05\n",
      "\n",
      "Epoch 132: loss did not improve from 0.64955\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 0.7657 - bpp: 0.3329 - mse: 8.4531e-04\n",
      "Epoch 133/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6507 - bpp: 0.3154 - mse: 6.5497e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 132. Reducing Learning Rate from 7.323035970330238e-05 to 7.249805639730766e-05\n",
      "\n",
      "Epoch 133: loss did not improve from 0.64955\n",
      "200/200 [==============================] - 39s 192ms/step - loss: 0.6507 - bpp: 0.3154 - mse: 6.5497e-04\n",
      "Epoch 134/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6334 - bpp: 0.3055 - mse: 6.4035e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 133. Reducing Learning Rate from 7.249805639730766e-05 to 7.177307998063043e-05\n",
      "\n",
      "Epoch 134: loss improved from 0.64955 to 0.63335, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 42s 205ms/step - loss: 0.6334 - bpp: 0.3055 - mse: 6.4035e-04\n",
      "Epoch 135/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6750 - bpp: 0.3138 - mse: 7.0539e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 134. Reducing Learning Rate from 7.177307998063043e-05 to 7.105535041773692e-05\n",
      "\n",
      "Epoch 135: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 50s 248ms/step - loss: 0.6750 - bpp: 0.3138 - mse: 7.0539e-04\n",
      "Epoch 136/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6943 - bpp: 0.3190 - mse: 7.3308e-04[MemoryCallback]:  6469132\n",
      "\n",
      "Epoch: 135. Reducing Learning Rate from 7.105535041773692e-05 to 7.034479494905099e-05\n",
      "\n",
      "Epoch 136: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 41s 206ms/step - loss: 0.6943 - bpp: 0.3190 - mse: 7.3308e-04\n",
      "Epoch 137/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7287 - bpp: 0.3263 - mse: 7.8582e-04[MemoryCallback]:  6469740\n",
      "\n",
      "Epoch: 136. Reducing Learning Rate from 7.034479494905099e-05 to 6.964134809095412e-05\n",
      "\n",
      "Epoch 137: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 39s 192ms/step - loss: 0.7287 - bpp: 0.3263 - mse: 7.8582e-04\n",
      "Epoch 138/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6846 - bpp: 0.3138 - mse: 7.2411e-04[MemoryCallback]:  6469920\n",
      "\n",
      "Epoch: 137. Reducing Learning Rate from 6.964134809095412e-05 to 6.894493708387017e-05\n",
      "\n",
      "Epoch 138: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 44s 217ms/step - loss: 0.6846 - bpp: 0.3138 - mse: 7.2411e-04\n",
      "Epoch 139/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6644 - bpp: 0.3116 - mse: 6.8901e-04[MemoryCallback]:  6469920\n",
      "\n",
      "Epoch: 138. Reducing Learning Rate from 6.894493708387017e-05 to 6.8255489168223e-05\n",
      "\n",
      "Epoch 139: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 42s 207ms/step - loss: 0.6644 - bpp: 0.3116 - mse: 6.8901e-04\n",
      "Epoch 140/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6923 - bpp: 0.3092 - mse: 7.4819e-04[MemoryCallback]:  6469920\n",
      "\n",
      "Epoch: 139. Reducing Learning Rate from 6.8255489168223e-05 to 6.757293158443645e-05\n",
      "\n",
      "Epoch 140: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 0.6923 - bpp: 0.3092 - mse: 7.4819e-04\n",
      "Epoch 141/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7113 - bpp: 0.3154 - mse: 7.7336e-04[MemoryCallback]:  6469920\n",
      "\n",
      "Epoch: 140. Reducing Learning Rate from 6.757293158443645e-05 to 6.689720612484962e-05\n",
      "\n",
      "Epoch 141: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 0.7113 - bpp: 0.3154 - mse: 7.7336e-04\n",
      "Epoch 142/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6792 - bpp: 0.3155 - mse: 7.1040e-04[MemoryCallback]:  6470064\n",
      "\n",
      "Epoch: 141. Reducing Learning Rate from 6.689720612484962e-05 to 6.622823275392875e-05\n",
      "\n",
      "Epoch 142: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 0.6792 - bpp: 0.3155 - mse: 7.1040e-04\n",
      "Epoch 143/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6371 - bpp: 0.3025 - mse: 6.5344e-04[MemoryCallback]:  6470168\n",
      "\n",
      "Epoch: 142. Reducing Learning Rate from 6.622823275392875e-05 to 6.556595326401293e-05\n",
      "\n",
      "Epoch 143: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 39s 194ms/step - loss: 0.6371 - bpp: 0.3025 - mse: 6.5344e-04\n",
      "Epoch 144/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6830 - bpp: 0.3093 - mse: 7.2993e-04[MemoryCallback]:  6470184\n",
      "\n",
      "Epoch: 143. Reducing Learning Rate from 6.556595326401293e-05 to 6.491029489552602e-05\n",
      "\n",
      "Epoch 144: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 40s 197ms/step - loss: 0.6830 - bpp: 0.3093 - mse: 7.2993e-04\n",
      "Epoch 145/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6523 - bpp: 0.3019 - mse: 6.8439e-04[MemoryCallback]:  6470184\n",
      "\n",
      "Epoch: 144. Reducing Learning Rate from 6.491029489552602e-05 to 6.426119216484949e-05\n",
      "\n",
      "Epoch 145: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 40s 197ms/step - loss: 0.6523 - bpp: 0.3019 - mse: 6.8439e-04\n",
      "Epoch 146/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6795 - bpp: 0.3108 - mse: 7.2015e-04[MemoryCallback]:  6470588\n",
      "\n",
      "Epoch: 145. Reducing Learning Rate from 6.426119216484949e-05 to 6.361857958836481e-05\n",
      "\n",
      "Epoch 146: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 43s 212ms/step - loss: 0.6795 - bpp: 0.3108 - mse: 7.2015e-04\n",
      "Epoch 147/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6610 - bpp: 0.3056 - mse: 6.9409e-04[MemoryCallback]:  6470892\n",
      "\n",
      "Epoch: 146. Reducing Learning Rate from 6.361857958836481e-05 to 6.298239168245345e-05\n",
      "\n",
      "Epoch 147: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 0.6610 - bpp: 0.3056 - mse: 6.9409e-04\n",
      "Epoch 148/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6466 - bpp: 0.3023 - mse: 6.7250e-04[MemoryCallback]:  6470892\n",
      "\n",
      "Epoch: 147. Reducing Learning Rate from 6.298239168245345e-05 to 6.235257023945451e-05\n",
      "\n",
      "Epoch 148: loss did not improve from 0.63335\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 0.6466 - bpp: 0.3023 - mse: 6.7250e-04\n",
      "Epoch 149/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6124 - bpp: 0.2952 - mse: 6.1949e-04[MemoryCallback]:  6471296\n",
      "\n",
      "Epoch: 148. Reducing Learning Rate from 6.235257023945451e-05 to 6.172904249979183e-05\n",
      "\n",
      "Epoch 149: loss improved from 0.63335 to 0.61237, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 0.6124 - bpp: 0.2952 - mse: 6.1949e-04\n",
      "Epoch 150/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6610 - bpp: 0.3018 - mse: 7.0147e-04[MemoryCallback]:  6471620\n",
      "\n",
      "Epoch: 149. Reducing Learning Rate from 6.172904249979183e-05 to 6.111175025580451e-05\n",
      "\n",
      "Epoch 150: loss did not improve from 0.61237\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 0.6610 - bpp: 0.3018 - mse: 7.0147e-04\n",
      "Epoch 151/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6879 - bpp: 0.3060 - mse: 7.4574e-04[MemoryCallback]:  6471620\n",
      "\n",
      "Epoch: 150. Reducing Learning Rate from 6.111175025580451e-05 to 6.050063166185282e-05\n",
      "\n",
      "Epoch 151: loss did not improve from 0.61237\n",
      "200/200 [==============================] - 40s 197ms/step - loss: 0.6879 - bpp: 0.3060 - mse: 7.4574e-04\n",
      "Epoch 152/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6607 - bpp: 0.3002 - mse: 7.0402e-04[MemoryCallback]:  6472008\n",
      "\n",
      "Epoch: 151. Reducing Learning Rate from 6.050063166185282e-05 to 5.989562487229705e-05\n",
      "\n",
      "Epoch 152: loss did not improve from 0.61237\n",
      "200/200 [==============================] - 42s 207ms/step - loss: 0.6607 - bpp: 0.3002 - mse: 7.0402e-04\n",
      "Epoch 153/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6751 - bpp: 0.3065 - mse: 7.1990e-04[MemoryCallback]:  6472396\n",
      "\n",
      "Epoch: 152. Reducing Learning Rate from 5.989562487229705e-05 to 5.929666804149747e-05\n",
      "\n",
      "Epoch 153: loss did not improve from 0.61237\n",
      "200/200 [==============================] - 40s 196ms/step - loss: 0.6751 - bpp: 0.3065 - mse: 7.1990e-04\n",
      "Epoch 154/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6752 - bpp: 0.3033 - mse: 7.2637e-04[MemoryCallback]:  6472516\n",
      "\n",
      "Epoch: 153. Reducing Learning Rate from 5.929666804149747e-05 to 5.870370296179317e-05\n",
      "\n",
      "Epoch 154: loss did not improve from 0.61237\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 0.6752 - bpp: 0.3033 - mse: 7.2637e-04\n",
      "Epoch 155/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5944 - bpp: 0.2877 - mse: 5.9899e-04[MemoryCallback]:  6484536\n",
      "\n",
      "Epoch: 154. Reducing Learning Rate from 5.870370296179317e-05 to 5.811666778754443e-05\n",
      "\n",
      "Epoch 155: loss improved from 0.61237 to 0.59439, saving model to checkpoints_wavelets_L_512_1_240x240/\n",
      "200/200 [==============================] - 41s 202ms/step - loss: 0.5944 - bpp: 0.2877 - mse: 5.9899e-04\n",
      "Epoch 156/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6299 - bpp: 0.2921 - mse: 6.5971e-04[MemoryCallback]:  6484784\n",
      "\n",
      "Epoch: 155. Reducing Learning Rate from 5.811666778754443e-05 to 5.753550067311153e-05\n",
      "\n",
      "Epoch 156: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 0.6299 - bpp: 0.2921 - mse: 6.5971e-04\n",
      "Epoch 157/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6153 - bpp: 0.2890 - mse: 6.3748e-04[MemoryCallback]:  6484904\n",
      "\n",
      "Epoch: 156. Reducing Learning Rate from 5.753550067311153e-05 to 5.696014704881236e-05\n",
      "\n",
      "Epoch 157: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 40s 197ms/step - loss: 0.6153 - bpp: 0.2890 - mse: 6.3748e-04\n",
      "Epoch 158/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6976 - bpp: 0.3009 - mse: 7.7468e-04[MemoryCallback]:  6484904\n",
      "\n",
      "Epoch: 157. Reducing Learning Rate from 5.696014704881236e-05 to 5.63905450690072e-05\n",
      "\n",
      "Epoch 158: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 0.6976 - bpp: 0.3009 - mse: 7.7468e-04\n",
      "Epoch 159/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6384 - bpp: 0.2971 - mse: 6.6660e-04[MemoryCallback]:  6484904\n",
      "\n",
      "Epoch: 158. Reducing Learning Rate from 5.63905450690072e-05 to 5.582664016401395e-05\n",
      "\n",
      "Epoch 159: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 41s 197ms/step - loss: 0.6384 - bpp: 0.2971 - mse: 6.6660e-04\n",
      "Epoch 160/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6229 - bpp: 0.2906 - mse: 6.4901e-04[MemoryCallback]:  6485308\n",
      "\n",
      "Epoch: 159. Reducing Learning Rate from 5.582664016401395e-05 to 5.526837412617169e-05\n",
      "\n",
      "Epoch 160: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 0.6229 - bpp: 0.2906 - mse: 6.4901e-04\n",
      "Epoch 161/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6114 - bpp: 0.2898 - mse: 6.2811e-04[MemoryCallback]:  6485620\n",
      "\n",
      "Epoch: 160. Reducing Learning Rate from 5.526837412617169e-05 to 5.471569238579832e-05\n",
      "\n",
      "Epoch 161: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 40s 197ms/step - loss: 0.6114 - bpp: 0.2898 - mse: 6.2811e-04\n",
      "Epoch 162/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6881 - bpp: 0.2975 - mse: 7.6286e-04[MemoryCallback]:  6489520\n",
      "\n",
      "Epoch: 161. Reducing Learning Rate from 5.471569238579832e-05 to 5.416853673523292e-05\n",
      "\n",
      "Epoch 162: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 0.6881 - bpp: 0.2975 - mse: 7.6286e-04\n",
      "Epoch 163/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6916 - bpp: 0.3043 - mse: 7.5647e-04[MemoryCallback]:  6489520\n",
      "\n",
      "Epoch: 162. Reducing Learning Rate from 5.416853673523292e-05 to 5.3626852604793385e-05\n",
      "\n",
      "Epoch 163: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 0.6916 - bpp: 0.3043 - mse: 7.5647e-04\n",
      "Epoch 164/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6509 - bpp: 0.2953 - mse: 6.9440e-04[MemoryCallback]:  6489520\n",
      "\n",
      "Epoch: 163. Reducing Learning Rate from 5.3626852604793385e-05 to 5.309058542479761e-05\n",
      "\n",
      "Epoch 164: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 40s 196ms/step - loss: 0.6509 - bpp: 0.2953 - mse: 6.9440e-04\n",
      "Epoch 165/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6587 - bpp: 0.2978 - mse: 7.0483e-04[MemoryCallback]:  6489520\n",
      "\n",
      "Epoch: 164. Reducing Learning Rate from 5.309058542479761e-05 to 5.255968062556349e-05\n",
      "\n",
      "Epoch 165: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 42s 211ms/step - loss: 0.6587 - bpp: 0.2978 - mse: 7.0483e-04\n",
      "Epoch 166/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6472 - bpp: 0.2926 - mse: 6.9259e-04[MemoryCallback]:  6489520\n",
      "\n",
      "Epoch: 165. Reducing Learning Rate from 5.255968062556349e-05 to 5.203408363740891e-05\n",
      "\n",
      "Epoch 166: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 0.6472 - bpp: 0.2926 - mse: 6.9259e-04\n",
      "Epoch 167/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6100 - bpp: 0.2870 - mse: 6.3099e-04[MemoryCallback]:  6489520\n",
      "\n",
      "Epoch: 166. Reducing Learning Rate from 5.203408363740891e-05 to 5.1513743528630584e-05\n",
      "\n",
      "Epoch 167: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 0.6100 - bpp: 0.2870 - mse: 6.3099e-04\n",
      "Epoch 168/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6190 - bpp: 0.2899 - mse: 6.4265e-04[MemoryCallback]:  6489520\n",
      "\n",
      "Epoch: 167. Reducing Learning Rate from 5.1513743528630584e-05 to 5.09986057295464e-05\n",
      "\n",
      "Epoch 168: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 0.6190 - bpp: 0.2899 - mse: 6.4265e-04\n",
      "Epoch 169/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6143 - bpp: 0.2873 - mse: 6.3858e-04[MemoryCallback]:  6489520\n",
      "\n",
      "Epoch: 168. Reducing Learning Rate from 5.09986057295464e-05 to 5.048861930845305e-05\n",
      "\n",
      "Epoch 169: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 0.6143 - bpp: 0.2873 - mse: 6.3858e-04\n",
      "Epoch 170/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6009 - bpp: 0.2839 - mse: 6.1919e-04[MemoryCallback]:  6489520\n",
      "\n",
      "Epoch: 169. Reducing Learning Rate from 5.048861930845305e-05 to 4.998373333364725e-05\n",
      "\n",
      "Epoch 170: loss did not improve from 0.59439\n",
      "200/200 [==============================] - 39s 194ms/step - loss: 0.6009 - bpp: 0.2839 - mse: 6.1919e-04\n"
     ]
    }
   ],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "hist = model.fit(x=data, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE,\n",
    "                callbacks=[\n",
    "                    Callbacks.MemoryCallback(),\n",
    "                    Callbacks.LearningRateReducer(),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(filepath=checkponts_new_path, save_weights_only=True, save_freq='epoch', monitor=\"loss\", mode='min',  save_best_only=True, verbose=1), \n",
    "                    tf.keras.callbacks.TerminateOnNaN(),\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=early_stop),\n",
    "                    tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, update_freq=\"epoch\"),            \n",
    "                    ],\n",
    "\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/WindowsDev/DataSets/vimeo_septuplet/sequences/00057/0364/im1.png\n",
      "compress\n",
      "in the compress\n",
      "decompress\n",
      "in decompress\n"
     ]
    }
   ],
   "source": [
    "path = load.load_random_path(\"folder_cloud_test.npy\")\n",
    "i=0\n",
    "out_bin = \"Test_com/test{}.bin\".format(i)\n",
    "out_decom = \"Test_com/testdcom{}.png\".format(i)\n",
    "p_on_test = \"Test_com/test_p_frame{}.png\".format(i)\n",
    "i_on_test = \"Test_com/test_i_frame{}.png\".format(i)\n",
    "\n",
    "i_frame = path + 'im1' + '.png'\n",
    "p_frame = path + 'im2' + '.png'\n",
    "print(i_frame)\n",
    "\n",
    "OpenDVCW.write_png(p_on_test, OpenDVCW.read_png_crop(p_frame, 240, 240))\n",
    "OpenDVCW.write_png(i_on_test, OpenDVCW.read_png_crop(i_frame, 240, 240))\n",
    "\n",
    "OpenDVCW.compress(model, i_frame, p_frame, out_bin, 240, 240)\n",
    "OpenDVCW.decompress(model, i_frame, out_bin, out_decom, 240, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the compress\n",
      "in decompress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as optical_flow_loss_layer_call_fn, optical_flow_loss_layer_call_and_return_conditional_losses, dwt_layer_call_fn, dwt_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_wavelets_L_512_1_240x240/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_wavelets_L_512_1_240x240/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(save_name, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
