{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/OpenDVCW/train_models'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/OpenDVCW\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/ubu-admin/Developer/tensorflow-wavelets\n",
    "%cd /workspaces/OpenDVCW\n",
    "import OpenDVCW\n",
    "import numpy as np\n",
    "import load\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import DataGen\n",
    "import Callbacks\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 700\n",
    "STEPS_PER_EPOCH = 200\n",
    "Height = 240\n",
    "Width = 240\n",
    "Channel = 3\n",
    "lmbda = 256\n",
    "lr_init = 1e-4\n",
    "early_stop = 15\n",
    "I_QP=27\n",
    "\n",
    "args = OpenDVCW.Arguments()\n",
    "last = 0\n",
    "checkponts_last_path = \"checkpoints_wavelets_L_{}_{}_{}x{}/\".format(lmbda, last, Width, Height)\n",
    "checkponts_new_path = \"checkpoints_wavelets_L_{}_{}_{}x{}/\".format(lmbda, last+1, Width, Height)\n",
    "save_name = \"model_save_wavelets_L_{}_{}_{}x{}\".format(lmbda, last+1, Width, Height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 16:46:06.203173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 16:46:06.229222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 16:46:06.230031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 16:46:06.234780: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-04 16:46:06.237418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 16:46:06.238255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 16:46:06.239128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 16:46:07.399718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 16:46:07.400525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 16:46:07.401235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 16:46:07.401980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10244 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:00:09.0, compute capability: 8.6\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "* [Loading dataset]...\n"
     ]
    }
   ],
   "source": [
    "model = OpenDVCW.OpenDVC(width=Width, height=Height, batch_size=BATCH_SIZE, num_filters=128, lmbda=lmbda)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_init),)\n",
    "print(\"* [Model compiled]...\")\n",
    "\n",
    "print(\"* [Loading dataset]...\")\n",
    "data = DataGen.DataVimeo90kGenerator(\"folder_cloud_test.npy\", \n",
    "                                    BATCH_SIZE,\n",
    "                                    (Height,Width,Channel),\n",
    "                                    Channel,\n",
    "                                    True, \n",
    "                                    I_QP,\n",
    "                                    True)\n",
    "\n",
    "# print(\"Loading weights\")\n",
    "# model.load_weights(checkponts_last_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].trainable = False\n",
    "# model.layers[1].trainable = False\n",
    "# model.layers[2].trainable = True\n",
    "# model.layers[3].trainable = True\n",
    "# model.layers[4].trainable = False\n",
    "# model.layers[5].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv_analysis True\n",
      "mv_synthesis True\n",
      "res_analysis True\n",
      "res_synthesis True\n",
      "wavelets_optical_flow True\n",
      "motion_compensation True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 16:46:27.434105: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-04-04 16:47:09.441784: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/200 [..............................] - ETA: 1:05 - loss: 37.3782 - bpp: 5.3746 - mse: 0.1250WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3200s vs `on_train_batch_end` time: 0.4035s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3200s vs `on_train_batch_end` time: 0.4035s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA: 0s - loss: 9.1423 - bpp: 5.3055 - mse: 0.0150[MemoryCallback]:  4847268\n",
      "\n",
      "Epoch 1: loss improved from inf to 9.14235, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 112s 261ms/step - loss: 9.1423 - bpp: 5.3055 - mse: 0.0150\n",
      "Epoch 2/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.7184 - bpp: 5.1633 - mse: 0.0022[MemoryCallback]:  4998336\n",
      "\n",
      "Epoch 2: loss improved from 9.14235 to 5.71835, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 52s 259ms/step - loss: 5.7184 - bpp: 5.1633 - mse: 0.0022\n",
      "Epoch 3/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.4457 - bpp: 5.0214 - mse: 0.0017[MemoryCallback]:  5064236\n",
      "\n",
      "Epoch 3: loss improved from 5.71835 to 5.44570, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 53s 265ms/step - loss: 5.4457 - bpp: 5.0214 - mse: 0.0017\n",
      "Epoch 4/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.2446 - bpp: 4.8920 - mse: 0.0014[MemoryCallback]:  5181124\n",
      "\n",
      "Epoch 4: loss improved from 5.44570 to 5.24461, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 57s 284ms/step - loss: 5.2446 - bpp: 4.8920 - mse: 0.0014\n",
      "Epoch 5/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 5.0666 - bpp: 4.7652 - mse: 0.0012[MemoryCallback]:  5232436\n",
      "\n",
      "Epoch 5: loss improved from 5.24461 to 5.06662, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 57s 281ms/step - loss: 5.0666 - bpp: 4.7652 - mse: 0.0012\n",
      "Epoch 6/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.8811 - bpp: 4.6385 - mse: 9.4763e-04[MemoryCallback]:  5284448\n",
      "\n",
      "Epoch 6: loss improved from 5.06662 to 4.88114, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 59s 290ms/step - loss: 4.8811 - bpp: 4.6385 - mse: 9.4763e-04\n",
      "Epoch 7/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.7246 - bpp: 4.5141 - mse: 8.2208e-04[MemoryCallback]:  5333184\n",
      "\n",
      "Epoch 7: loss improved from 4.88114 to 4.72456, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 57s 284ms/step - loss: 4.7246 - bpp: 4.5141 - mse: 8.2208e-04\n",
      "Epoch 8/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.5990 - bpp: 4.3919 - mse: 8.0908e-04[MemoryCallback]:  5382516\n",
      "\n",
      "Epoch 8: loss improved from 4.72456 to 4.59902, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 57s 283ms/step - loss: 4.5990 - bpp: 4.3919 - mse: 8.0908e-04\n",
      "Epoch 9/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.4597 - bpp: 4.2717 - mse: 7.3440e-04[MemoryCallback]:  5480244\n",
      "\n",
      "Epoch 9: loss improved from 4.59902 to 4.45974, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 58s 290ms/step - loss: 4.4597 - bpp: 4.2717 - mse: 7.3440e-04\n",
      "Epoch 10/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.3321 - bpp: 4.1536 - mse: 6.9716e-04[MemoryCallback]:  5585888\n",
      "\n",
      "Epoch 10: loss improved from 4.45974 to 4.33208, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 57s 284ms/step - loss: 4.3321 - bpp: 4.1536 - mse: 6.9716e-04\n",
      "Epoch 11/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.4309 - bpp: 4.0407 - mse: 0.0015[MemoryCallback]:  5628476\n",
      "\n",
      "Epoch 11: loss did not improve from 4.33208\n",
      "200/200 [==============================] - 56s 277ms/step - loss: 4.4309 - bpp: 4.0407 - mse: 0.0015\n",
      "Epoch 12/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.2605 - bpp: 3.9278 - mse: 0.0013[MemoryCallback]:  5662576\n",
      "\n",
      "Epoch 12: loss improved from 4.33208 to 4.26053, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 57s 281ms/step - loss: 4.2605 - bpp: 3.9278 - mse: 0.0013\n",
      "Epoch 13/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.0022 - bpp: 3.8094 - mse: 7.5339e-04[MemoryCallback]:  5662848\n",
      "\n",
      "Epoch 13: loss improved from 4.26053 to 4.00224, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 58s 290ms/step - loss: 4.0022 - bpp: 3.8094 - mse: 7.5339e-04\n",
      "Epoch 14/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.8587 - bpp: 3.6972 - mse: 6.3100e-04[MemoryCallback]:  5710520\n",
      "\n",
      "Epoch 14: loss improved from 4.00224 to 3.85873, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 60s 297ms/step - loss: 3.8587 - bpp: 3.6972 - mse: 6.3100e-04\n",
      "Epoch 15/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.7166 - bpp: 3.5861 - mse: 5.0982e-04[MemoryCallback]:  5717448\n",
      "\n",
      "Epoch 15: loss improved from 3.85873 to 3.71662, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 57s 285ms/step - loss: 3.7166 - bpp: 3.5861 - mse: 5.0982e-04\n",
      "Epoch 16/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.6375 - bpp: 3.4804 - mse: 6.1342e-04[MemoryCallback]:  5781184\n",
      "\n",
      "Epoch 16: loss improved from 3.71662 to 3.63746, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 59s 293ms/step - loss: 3.6375 - bpp: 3.4804 - mse: 6.1342e-04\n",
      "Epoch 17/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.5083 - bpp: 3.3719 - mse: 5.3308e-04[MemoryCallback]:  5781476\n",
      "\n",
      "Epoch 17: loss improved from 3.63746 to 3.50834, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 55s 272ms/step - loss: 3.5083 - bpp: 3.3719 - mse: 5.3308e-04\n",
      "Epoch 18/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.3891 - bpp: 3.2650 - mse: 4.8490e-04[MemoryCallback]:  5783204\n",
      "\n",
      "Epoch 18: loss improved from 3.50834 to 3.38910, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 55s 275ms/step - loss: 3.3891 - bpp: 3.2650 - mse: 4.8490e-04\n",
      "Epoch 19/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2787 - bpp: 3.1631 - mse: 4.5167e-04[MemoryCallback]:  5832224\n",
      "\n",
      "Epoch 19: loss improved from 3.38910 to 3.27874, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 57s 281ms/step - loss: 3.2787 - bpp: 3.1631 - mse: 4.5167e-04\n",
      "Epoch 20/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.1784 - bpp: 3.0601 - mse: 4.6214e-04[MemoryCallback]:  5832684\n",
      "\n",
      "Epoch 20: loss improved from 3.27874 to 3.17836, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 55s 274ms/step - loss: 3.1784 - bpp: 3.0601 - mse: 4.6214e-04\n",
      "Epoch 21/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.0865 - bpp: 2.9612 - mse: 4.8958e-04[MemoryCallback]:  5832940\n",
      "\n",
      "Epoch 21: loss improved from 3.17836 to 3.08654, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 55s 271ms/step - loss: 3.0865 - bpp: 2.9612 - mse: 4.8958e-04\n",
      "Epoch 22/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.9932 - bpp: 2.8638 - mse: 5.0541e-04[MemoryCallback]:  5833004\n",
      "\n",
      "Epoch 22: loss improved from 3.08654 to 2.99321, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 56s 277ms/step - loss: 2.9932 - bpp: 2.8638 - mse: 5.0541e-04\n",
      "Epoch 23/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.8833 - bpp: 2.7646 - mse: 4.6369e-04[MemoryCallback]:  5881236\n",
      "\n",
      "Epoch 23: loss improved from 2.99321 to 2.88335, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 57s 283ms/step - loss: 2.8833 - bpp: 2.7646 - mse: 4.6369e-04\n",
      "Epoch 24/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.7951 - bpp: 2.6709 - mse: 4.8492e-04[MemoryCallback]:  5881684\n",
      "\n",
      "Epoch 24: loss improved from 2.88335 to 2.79506, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 59s 293ms/step - loss: 2.7951 - bpp: 2.6709 - mse: 4.8492e-04\n",
      "Epoch 25/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.6949 - bpp: 2.5790 - mse: 4.5294e-04[MemoryCallback]:  5930916\n",
      "\n",
      "Epoch 25: loss improved from 2.79506 to 2.69493, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 54s 270ms/step - loss: 2.6949 - bpp: 2.5790 - mse: 4.5294e-04\n",
      "Epoch 26/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.6220 - bpp: 2.4881 - mse: 5.2325e-04[MemoryCallback]:  5979408\n",
      "\n",
      "Epoch 26: loss improved from 2.69493 to 2.62201, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 54s 265ms/step - loss: 2.6220 - bpp: 2.4881 - mse: 5.2325e-04\n",
      "Epoch 27/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.5273 - bpp: 2.4007 - mse: 4.9480e-04[MemoryCallback]:  6077060\n",
      "\n",
      "Epoch 27: loss improved from 2.62201 to 2.52735, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 52s 258ms/step - loss: 2.5273 - bpp: 2.4007 - mse: 4.9480e-04\n",
      "Epoch 28/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.4302 - bpp: 2.3103 - mse: 4.6812e-04[MemoryCallback]:  6077880\n",
      "\n",
      "Epoch 28: loss improved from 2.52735 to 2.43018, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 53s 264ms/step - loss: 2.4302 - bpp: 2.3103 - mse: 4.6812e-04\n",
      "Epoch 29/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.3326 - bpp: 2.2242 - mse: 4.2364e-04[MemoryCallback]:  6081924\n",
      "\n",
      "Epoch 29: loss improved from 2.43018 to 2.33263, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 50s 252ms/step - loss: 2.3326 - bpp: 2.2242 - mse: 4.2364e-04\n",
      "Epoch 30/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.2667 - bpp: 2.1445 - mse: 4.7736e-04[MemoryCallback]:  6130072\n",
      "\n",
      "Epoch 30: loss improved from 2.33263 to 2.26669, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 52s 260ms/step - loss: 2.2667 - bpp: 2.1445 - mse: 4.7736e-04\n",
      "Epoch 31/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.1731 - bpp: 2.0606 - mse: 4.3931e-04[MemoryCallback]:  6217316\n",
      "\n",
      "Epoch 31: loss improved from 2.26669 to 2.17306, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 52s 252ms/step - loss: 2.1731 - bpp: 2.0606 - mse: 4.3931e-04\n",
      "Epoch 32/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0981 - bpp: 1.9806 - mse: 4.5900e-04[MemoryCallback]:  6217512\n",
      "\n",
      "Epoch 32: loss improved from 2.17306 to 2.09807, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 54s 269ms/step - loss: 2.0981 - bpp: 1.9806 - mse: 4.5900e-04\n",
      "Epoch 33/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0115 - bpp: 1.9029 - mse: 4.2416e-04[MemoryCallback]:  6217656\n",
      "\n",
      "Epoch 33: loss improved from 2.09807 to 2.01153, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 55s 274ms/step - loss: 2.0115 - bpp: 1.9029 - mse: 4.2416e-04\n",
      "Epoch 34/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.9844 - bpp: 1.9084 - mse: 0.0120[MemoryCallback]:  6221612\n",
      "\n",
      "Epoch 34: loss did not improve from 2.01153\n",
      "200/200 [==============================] - 52s 260ms/step - loss: 4.9844 - bpp: 1.9084 - mse: 0.0120\n",
      "Epoch 35/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.9464 - bpp: 1.8734 - mse: 0.0042[MemoryCallback]:  6234068\n",
      "\n",
      "Epoch 35: loss did not improve from 2.01153\n",
      "200/200 [==============================] - 50s 247ms/step - loss: 2.9464 - bpp: 1.8734 - mse: 0.0042\n",
      "Epoch 36/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.2845 - bpp: 1.7924 - mse: 0.0019[MemoryCallback]:  6234852\n",
      "\n",
      "Epoch 36: loss did not improve from 2.01153\n",
      "200/200 [==============================] - 51s 256ms/step - loss: 2.2845 - bpp: 1.7924 - mse: 0.0019\n",
      "Epoch 37/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 2.0630 - bpp: 1.6986 - mse: 0.0014[MemoryCallback]:  6235168\n",
      "\n",
      "Epoch 37: loss did not improve from 2.01153\n",
      "200/200 [==============================] - 52s 258ms/step - loss: 2.0630 - bpp: 1.6986 - mse: 0.0014\n",
      "Epoch 38/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.9133 - bpp: 1.6144 - mse: 0.0012[MemoryCallback]:  6235820\n",
      "\n",
      "Epoch 38: loss improved from 2.01153 to 1.91329, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 52s 257ms/step - loss: 1.9133 - bpp: 1.6144 - mse: 0.0012\n",
      "Epoch 39/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8210 - bpp: 1.5469 - mse: 0.0011[MemoryCallback]:  6240544\n",
      "\n",
      "Epoch 39: loss improved from 1.91329 to 1.82099, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 52s 259ms/step - loss: 1.8210 - bpp: 1.5469 - mse: 0.0011\n",
      "Epoch 40/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.7798 - bpp: 1.4925 - mse: 0.0011[MemoryCallback]:  6240560\n",
      "\n",
      "Epoch 40: loss improved from 1.82099 to 1.77976, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 52s 260ms/step - loss: 1.7798 - bpp: 1.4925 - mse: 0.0011\n",
      "Epoch 41/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6806 - bpp: 1.4292 - mse: 9.8196e-04[MemoryCallback]:  6240560\n",
      "\n",
      "Epoch 41: loss improved from 1.77976 to 1.68062, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 52s 257ms/step - loss: 1.6806 - bpp: 1.4292 - mse: 9.8196e-04\n",
      "Epoch 42/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.6391 - bpp: 1.3750 - mse: 0.0010[MemoryCallback]:  6240560\n",
      "\n",
      "Epoch 42: loss improved from 1.68062 to 1.63906, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 52s 258ms/step - loss: 1.6391 - bpp: 1.3750 - mse: 0.0010\n",
      "Epoch 43/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5729 - bpp: 1.3153 - mse: 0.0010[MemoryCallback]:  6240560\n",
      "\n",
      "Epoch 43: loss improved from 1.63906 to 1.57286, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 52s 256ms/step - loss: 1.5729 - bpp: 1.3153 - mse: 0.0010\n",
      "Epoch 44/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.5147 - bpp: 1.2603 - mse: 9.9384e-04[MemoryCallback]:  6240560\n",
      "\n",
      "Epoch 44: loss improved from 1.57286 to 1.51472, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 1.5147 - bpp: 1.2603 - mse: 9.9384e-04\n",
      "Epoch 45/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4455 - bpp: 1.2173 - mse: 8.9153e-04[MemoryCallback]:  6240560\n",
      "\n",
      "Epoch 45: loss improved from 1.51472 to 1.44550, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 1.4455 - bpp: 1.2173 - mse: 8.9153e-04\n",
      "Epoch 46/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4088 - bpp: 1.1678 - mse: 9.4116e-04[MemoryCallback]:  6240560\n",
      "\n",
      "Epoch 46: loss improved from 1.44550 to 1.40876, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 47s 233ms/step - loss: 1.4088 - bpp: 1.1678 - mse: 9.4116e-04\n",
      "Epoch 47/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3491 - bpp: 1.1225 - mse: 8.8530e-04[MemoryCallback]:  6241688\n",
      "\n",
      "Epoch 47: loss improved from 1.40876 to 1.34915, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 1.3491 - bpp: 1.1225 - mse: 8.8530e-04\n",
      "Epoch 48/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3065 - bpp: 1.0752 - mse: 9.0335e-04[MemoryCallback]:  6289708\n",
      "\n",
      "Epoch 48: loss improved from 1.34915 to 1.30646, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 50s 247ms/step - loss: 1.3065 - bpp: 1.0752 - mse: 9.0335e-04\n",
      "Epoch 49/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.3055 - bpp: 1.0524 - mse: 9.8843e-04[MemoryCallback]:  6290272\n",
      "\n",
      "Epoch 49: loss improved from 1.30646 to 1.30546, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 1.3055 - bpp: 1.0524 - mse: 9.8843e-04\n",
      "Epoch 50/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2210 - bpp: 0.9912 - mse: 8.9763e-04[MemoryCallback]:  6290992\n",
      "\n",
      "Epoch 50: loss improved from 1.30546 to 1.22103, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 47s 235ms/step - loss: 1.2210 - bpp: 0.9912 - mse: 8.9763e-04\n",
      "Epoch 51/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2349 - bpp: 0.9755 - mse: 0.0010[MemoryCallback]:  6293276\n",
      "\n",
      "Epoch 51: loss did not improve from 1.22103\n",
      "200/200 [==============================] - 45s 221ms/step - loss: 1.2349 - bpp: 0.9755 - mse: 0.0010\n",
      "Epoch 52/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2328 - bpp: 0.9419 - mse: 0.0011[MemoryCallback]:  6296584\n",
      "\n",
      "Epoch 52: loss did not improve from 1.22103\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 1.2328 - bpp: 0.9419 - mse: 0.0011\n",
      "Epoch 53/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1432 - bpp: 0.8955 - mse: 9.6792e-04[MemoryCallback]:  6297292\n",
      "\n",
      "Epoch 53: loss improved from 1.22103 to 1.14324, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 1.1432 - bpp: 0.8955 - mse: 9.6792e-04\n",
      "Epoch 54/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1097 - bpp: 0.8683 - mse: 9.4278e-04[MemoryCallback]:  6297488\n",
      "\n",
      "Epoch 54: loss improved from 1.14324 to 1.10965, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 1.1097 - bpp: 0.8683 - mse: 9.4278e-04\n",
      "Epoch 55/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1448 - bpp: 0.8481 - mse: 0.0012[MemoryCallback]:  6297532\n",
      "\n",
      "Epoch 55: loss did not improve from 1.10965\n",
      "200/200 [==============================] - 46s 226ms/step - loss: 1.1448 - bpp: 0.8481 - mse: 0.0012\n",
      "Epoch 56/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0598 - bpp: 0.8106 - mse: 9.7361e-04[MemoryCallback]:  6344928\n",
      "\n",
      "Epoch 56: loss improved from 1.10965 to 1.05981, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 1.0598 - bpp: 0.8106 - mse: 9.7361e-04\n",
      "Epoch 57/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0664 - bpp: 0.7927 - mse: 0.0011[MemoryCallback]:  6345172\n",
      "\n",
      "Epoch 57: loss did not improve from 1.05981\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 1.0664 - bpp: 0.7927 - mse: 0.0011\n",
      "Epoch 58/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0037 - bpp: 0.7627 - mse: 9.4120e-04[MemoryCallback]:  6345172\n",
      "\n",
      "Epoch 58: loss improved from 1.05981 to 1.00369, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 47s 231ms/step - loss: 1.0037 - bpp: 0.7627 - mse: 9.4120e-04\n",
      "Epoch 59/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9756 - bpp: 0.7288 - mse: 9.6424e-04[MemoryCallback]:  6345172\n",
      "\n",
      "Epoch 59: loss improved from 1.00369 to 0.97561, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 48s 237ms/step - loss: 0.9756 - bpp: 0.7288 - mse: 9.6424e-04\n",
      "Epoch 60/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0451 - bpp: 0.7120 - mse: 0.0013[MemoryCallback]:  6345172\n",
      "\n",
      "Epoch 60: loss did not improve from 0.97561\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 1.0451 - bpp: 0.7120 - mse: 0.0013\n",
      "Epoch 61/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9930 - bpp: 0.6994 - mse: 0.0011[MemoryCallback]:  6345300\n",
      "\n",
      "Epoch 61: loss did not improve from 0.97561\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 0.9930 - bpp: 0.6994 - mse: 0.0011\n",
      "Epoch 62/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9504 - bpp: 0.6739 - mse: 0.0011[MemoryCallback]:  6347144\n",
      "\n",
      "Epoch 62: loss improved from 0.97561 to 0.95042, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 0.9504 - bpp: 0.6739 - mse: 0.0011\n",
      "Epoch 63/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9061 - bpp: 0.6436 - mse: 0.0010[MemoryCallback]:  6347404\n",
      "\n",
      "Epoch 63: loss improved from 0.95042 to 0.90615, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 0.9061 - bpp: 0.6436 - mse: 0.0010\n",
      "Epoch 64/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9323 - bpp: 0.6335 - mse: 0.0012[MemoryCallback]:  6395608\n",
      "\n",
      "Epoch 64: loss did not improve from 0.90615\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.9323 - bpp: 0.6335 - mse: 0.0012\n",
      "Epoch 65/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9648 - bpp: 0.6214 - mse: 0.0013[MemoryCallback]:  6395956\n",
      "\n",
      "Epoch 65: loss did not improve from 0.90615\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.9648 - bpp: 0.6214 - mse: 0.0013\n",
      "Epoch 66/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0247 - bpp: 0.6067 - mse: 0.0016[MemoryCallback]:  6399792\n",
      "\n",
      "Epoch 66: loss did not improve from 0.90615\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 1.0247 - bpp: 0.6067 - mse: 0.0016\n",
      "Epoch 67/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9414 - bpp: 0.5923 - mse: 0.0014[MemoryCallback]:  6400008\n",
      "\n",
      "Epoch 67: loss did not improve from 0.90615\n",
      "200/200 [==============================] - 47s 235ms/step - loss: 0.9414 - bpp: 0.5923 - mse: 0.0014\n",
      "Epoch 68/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9794 - bpp: 0.5852 - mse: 0.0015[MemoryCallback]:  6400008\n",
      "\n",
      "Epoch 68: loss did not improve from 0.90615\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.9794 - bpp: 0.5852 - mse: 0.0015\n",
      "Epoch 69/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8069 - bpp: 0.5419 - mse: 0.0010[MemoryCallback]:  6400396\n",
      "\n",
      "Epoch 69: loss improved from 0.90615 to 0.80685, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 48s 234ms/step - loss: 0.8069 - bpp: 0.5419 - mse: 0.0010\n",
      "Epoch 70/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7917 - bpp: 0.5302 - mse: 0.0010[MemoryCallback]:  6400712\n",
      "\n",
      "Epoch 70: loss improved from 0.80685 to 0.79168, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 46s 226ms/step - loss: 0.7917 - bpp: 0.5302 - mse: 0.0010\n",
      "Epoch 71/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8462 - bpp: 0.5380 - mse: 0.0012[MemoryCallback]:  6400712\n",
      "\n",
      "Epoch 71: loss did not improve from 0.79168\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.8462 - bpp: 0.5380 - mse: 0.0012\n",
      "Epoch 72/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8310 - bpp: 0.5221 - mse: 0.0012[MemoryCallback]:  6400728\n",
      "\n",
      "Epoch 72: loss did not improve from 0.79168\n",
      "200/200 [==============================] - 45s 221ms/step - loss: 0.8310 - bpp: 0.5221 - mse: 0.0012\n",
      "Epoch 73/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8277 - bpp: 0.5007 - mse: 0.0013[MemoryCallback]:  6453140\n",
      "\n",
      "Epoch 73: loss did not improve from 0.79168\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.8277 - bpp: 0.5007 - mse: 0.0013\n",
      "Epoch 74/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8025 - bpp: 0.4894 - mse: 0.0012[MemoryCallback]:  6453316\n",
      "\n",
      "Epoch 74: loss did not improve from 0.79168\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.8025 - bpp: 0.4894 - mse: 0.0012\n",
      "Epoch 75/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7678 - bpp: 0.4785 - mse: 0.0011[MemoryCallback]:  6453316\n",
      "\n",
      "Epoch 75: loss improved from 0.79168 to 0.76782, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.7678 - bpp: 0.4785 - mse: 0.0011\n",
      "Epoch 76/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7890 - bpp: 0.4643 - mse: 0.0013[MemoryCallback]:  6453316\n",
      "\n",
      "Epoch 76: loss did not improve from 0.76782\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.7890 - bpp: 0.4643 - mse: 0.0013\n",
      "Epoch 77/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8277 - bpp: 0.4572 - mse: 0.0014[MemoryCallback]:  6453444\n",
      "\n",
      "Epoch 77: loss did not improve from 0.76782\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.8277 - bpp: 0.4572 - mse: 0.0014\n",
      "Epoch 78/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7436 - bpp: 0.4454 - mse: 0.0012[MemoryCallback]:  6453548\n",
      "\n",
      "Epoch 78: loss improved from 0.76782 to 0.74356, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 0.7436 - bpp: 0.4454 - mse: 0.0012\n",
      "Epoch 79/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6722 - bpp: 0.4211 - mse: 9.8075e-04[MemoryCallback]:  6453984\n",
      "\n",
      "Epoch 79: loss improved from 0.74356 to 0.67221, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.6722 - bpp: 0.4211 - mse: 9.8075e-04\n",
      "Epoch 80/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7112 - bpp: 0.4197 - mse: 0.0011[MemoryCallback]:  6454212\n",
      "\n",
      "Epoch 80: loss did not improve from 0.67221\n",
      "200/200 [==============================] - 46s 226ms/step - loss: 0.7112 - bpp: 0.4197 - mse: 0.0011\n",
      "Epoch 81/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6994 - bpp: 0.4118 - mse: 0.0011[MemoryCallback]:  6454212\n",
      "\n",
      "Epoch 81: loss did not improve from 0.67221\n",
      "200/200 [==============================] - 45s 222ms/step - loss: 0.6994 - bpp: 0.4118 - mse: 0.0011\n",
      "Epoch 82/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7367 - bpp: 0.4103 - mse: 0.0013[MemoryCallback]:  6454212\n",
      "\n",
      "Epoch 82: loss did not improve from 0.67221\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 0.7367 - bpp: 0.4103 - mse: 0.0013\n",
      "Epoch 83/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6730 - bpp: 0.3970 - mse: 0.0011[MemoryCallback]:  6454356\n",
      "\n",
      "Epoch 83: loss did not improve from 0.67221\n",
      "200/200 [==============================] - 44s 217ms/step - loss: 0.6730 - bpp: 0.3970 - mse: 0.0011\n",
      "Epoch 84/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6956 - bpp: 0.3918 - mse: 0.0012[MemoryCallback]:  6454520\n",
      "\n",
      "Epoch 84: loss did not improve from 0.67221\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.6956 - bpp: 0.3918 - mse: 0.0012\n",
      "Epoch 85/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6323 - bpp: 0.3722 - mse: 0.0010[MemoryCallback]:  6454520\n",
      "\n",
      "Epoch 85: loss improved from 0.67221 to 0.63231, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.6323 - bpp: 0.3722 - mse: 0.0010\n",
      "Epoch 86/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6883 - bpp: 0.3674 - mse: 0.0013[MemoryCallback]:  6454520\n",
      "\n",
      "Epoch 86: loss did not improve from 0.63231\n",
      "200/200 [==============================] - 47s 231ms/step - loss: 0.6883 - bpp: 0.3674 - mse: 0.0013\n",
      "Epoch 87/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6617 - bpp: 0.3611 - mse: 0.0012[MemoryCallback]:  6457036\n",
      "\n",
      "Epoch 87: loss did not improve from 0.63231\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.6617 - bpp: 0.3611 - mse: 0.0012\n",
      "Epoch 88/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6600 - bpp: 0.3531 - mse: 0.0012[MemoryCallback]:  6457580\n",
      "\n",
      "Epoch 88: loss did not improve from 0.63231\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.6600 - bpp: 0.3531 - mse: 0.0012\n",
      "Epoch 89/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7443 - bpp: 0.3600 - mse: 0.0015[MemoryCallback]:  6457892\n",
      "\n",
      "Epoch 89: loss did not improve from 0.63231\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.7443 - bpp: 0.3600 - mse: 0.0015\n",
      "Epoch 90/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7097 - bpp: 0.3573 - mse: 0.0014[MemoryCallback]:  6457892\n",
      "\n",
      "Epoch 90: loss did not improve from 0.63231\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.7097 - bpp: 0.3573 - mse: 0.0014\n",
      "Epoch 91/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6249 - bpp: 0.3426 - mse: 0.0011[MemoryCallback]:  6458036\n",
      "\n",
      "Epoch 91: loss improved from 0.63231 to 0.62486, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 44s 217ms/step - loss: 0.6249 - bpp: 0.3426 - mse: 0.0011\n",
      "Epoch 92/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6439 - bpp: 0.3363 - mse: 0.0012[MemoryCallback]:  6458160\n",
      "\n",
      "Epoch 92: loss did not improve from 0.62486\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.6439 - bpp: 0.3363 - mse: 0.0012\n",
      "Epoch 93/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5890 - bpp: 0.3259 - mse: 0.0010[MemoryCallback]:  6458560\n",
      "\n",
      "Epoch 93: loss improved from 0.62486 to 0.58904, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.5890 - bpp: 0.3259 - mse: 0.0010\n",
      "Epoch 94/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6077 - bpp: 0.3231 - mse: 0.0011[MemoryCallback]:  6473756\n",
      "\n",
      "Epoch 94: loss did not improve from 0.58904\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.6077 - bpp: 0.3231 - mse: 0.0011\n",
      "Epoch 95/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6003 - bpp: 0.3206 - mse: 0.0011[MemoryCallback]:  6474052\n",
      "\n",
      "Epoch 95: loss did not improve from 0.58904\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 0.6003 - bpp: 0.3206 - mse: 0.0011\n",
      "Epoch 96/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6231 - bpp: 0.3156 - mse: 0.0012[MemoryCallback]:  6474052\n",
      "\n",
      "Epoch 96: loss did not improve from 0.58904\n",
      "200/200 [==============================] - 52s 256ms/step - loss: 0.6231 - bpp: 0.3156 - mse: 0.0012\n",
      "Epoch 97/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6252 - bpp: 0.3103 - mse: 0.0012[MemoryCallback]:  6474720\n",
      "\n",
      "Epoch 97: loss did not improve from 0.58904\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.6252 - bpp: 0.3103 - mse: 0.0012\n",
      "Epoch 98/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5915 - bpp: 0.3055 - mse: 0.0011[MemoryCallback]:  6475028\n",
      "\n",
      "Epoch 98: loss did not improve from 0.58904\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 0.5915 - bpp: 0.3055 - mse: 0.0011\n",
      "Epoch 99/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5914 - bpp: 0.2993 - mse: 0.0011[MemoryCallback]:  6475028\n",
      "\n",
      "Epoch 99: loss did not improve from 0.58904\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.5914 - bpp: 0.2993 - mse: 0.0011\n",
      "Epoch 100/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5226 - bpp: 0.2839 - mse: 9.3275e-04[MemoryCallback]:  6475032\n",
      "\n",
      "Epoch 100: loss improved from 0.58904 to 0.52263, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 0.5226 - bpp: 0.2839 - mse: 9.3275e-04\n",
      "Epoch 101/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6310 - bpp: 0.2977 - mse: 0.0013[MemoryCallback]:  6475172\n",
      "\n",
      "Epoch 101: loss did not improve from 0.52263\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.6310 - bpp: 0.2977 - mse: 0.0013\n",
      "Epoch 102/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5844 - bpp: 0.2942 - mse: 0.0011[MemoryCallback]:  6475436\n",
      "\n",
      "Epoch: 101. Reducing Learning Rate from 9.999999747378752e-05 to 9.899999713525176e-05\n",
      "\n",
      "Epoch 102: loss did not improve from 0.52263\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.5844 - bpp: 0.2942 - mse: 0.0011\n",
      "Epoch 103/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5755 - bpp: 0.2825 - mse: 0.0011[MemoryCallback]:  6476488\n",
      "\n",
      "Epoch: 102. Reducing Learning Rate from 9.899999713525176e-05 to 9.801000123843551e-05\n",
      "\n",
      "Epoch 103: loss did not improve from 0.52263\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.5755 - bpp: 0.2825 - mse: 0.0011\n",
      "Epoch 104/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5541 - bpp: 0.2824 - mse: 0.0011[MemoryCallback]:  6476844\n",
      "\n",
      "Epoch: 103. Reducing Learning Rate from 9.801000123843551e-05 to 9.702990064397454e-05\n",
      "\n",
      "Epoch 104: loss did not improve from 0.52263\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.5541 - bpp: 0.2824 - mse: 0.0011\n",
      "Epoch 105/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5318 - bpp: 0.2720 - mse: 0.0010   [MemoryCallback]:  6477236\n",
      "\n",
      "Epoch: 104. Reducing Learning Rate from 9.702990064397454e-05 to 9.605960076441988e-05\n",
      "\n",
      "Epoch 105: loss did not improve from 0.52263\n",
      "200/200 [==============================] - 48s 241ms/step - loss: 0.5318 - bpp: 0.2720 - mse: 0.0010\n",
      "Epoch 106/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5589 - bpp: 0.2750 - mse: 0.0011[MemoryCallback]:  6477452\n",
      "\n",
      "Epoch: 105. Reducing Learning Rate from 9.605960076441988e-05 to 9.509900701232255e-05\n",
      "\n",
      "Epoch 106: loss did not improve from 0.52263\n",
      "200/200 [==============================] - 46s 226ms/step - loss: 0.5589 - bpp: 0.2750 - mse: 0.0011\n",
      "Epoch 107/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5632 - bpp: 0.2738 - mse: 0.0011[MemoryCallback]:  6477512\n",
      "\n",
      "Epoch: 106. Reducing Learning Rate from 9.509900701232255e-05 to 9.414801752427593e-05\n",
      "\n",
      "Epoch 107: loss did not improve from 0.52263\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.5632 - bpp: 0.2738 - mse: 0.0011\n",
      "Epoch 108/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5244 - bpp: 0.2604 - mse: 0.0010[MemoryCallback]:  6477512\n",
      "\n",
      "Epoch: 107. Reducing Learning Rate from 9.414801752427593e-05 to 9.320653771283105e-05\n",
      "\n",
      "Epoch 108: loss did not improve from 0.52263\n",
      "200/200 [==============================] - 48s 237ms/step - loss: 0.5244 - bpp: 0.2604 - mse: 0.0010\n",
      "Epoch 109/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5526 - bpp: 0.2681 - mse: 0.0011[MemoryCallback]:  6477512\n",
      "\n",
      "Epoch: 108. Reducing Learning Rate from 9.320653771283105e-05 to 9.227447299053892e-05\n",
      "\n",
      "Epoch 109: loss did not improve from 0.52263\n",
      "200/200 [==============================] - 49s 241ms/step - loss: 0.5526 - bpp: 0.2681 - mse: 0.0011\n",
      "Epoch 110/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5216 - bpp: 0.2562 - mse: 0.0010[MemoryCallback]:  6477512\n",
      "\n",
      "Epoch: 109. Reducing Learning Rate from 9.227447299053892e-05 to 9.135172876995057e-05\n",
      "\n",
      "Epoch 110: loss improved from 0.52263 to 0.52163, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 0.5216 - bpp: 0.2562 - mse: 0.0010\n",
      "Epoch 111/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5011 - bpp: 0.2531 - mse: 9.6879e-04[MemoryCallback]:  6477516\n",
      "\n",
      "Epoch: 110. Reducing Learning Rate from 9.135172876995057e-05 to 9.0438210463617e-05\n",
      "\n",
      "Epoch 111: loss improved from 0.52163 to 0.50113, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.5011 - bpp: 0.2531 - mse: 9.6879e-04\n",
      "Epoch 112/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5232 - bpp: 0.2488 - mse: 0.0011[MemoryCallback]:  6477516\n",
      "\n",
      "Epoch: 111. Reducing Learning Rate from 9.0438210463617e-05 to 8.953383076004684e-05\n",
      "\n",
      "Epoch 112: loss did not improve from 0.50113\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.5232 - bpp: 0.2488 - mse: 0.0011\n",
      "Epoch 113/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5460 - bpp: 0.2558 - mse: 0.0011[MemoryCallback]:  6479236\n",
      "\n",
      "Epoch: 112. Reducing Learning Rate from 8.953383076004684e-05 to 8.863849507179111e-05\n",
      "\n",
      "Epoch 113: loss did not improve from 0.50113\n",
      "200/200 [==============================] - 50s 250ms/step - loss: 0.5460 - bpp: 0.2558 - mse: 0.0011\n",
      "Epoch 114/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4730 - bpp: 0.2366 - mse: 9.2327e-04[MemoryCallback]:  6479548\n",
      "\n",
      "Epoch: 113. Reducing Learning Rate from 8.863849507179111e-05 to 8.775210881140083e-05\n",
      "\n",
      "Epoch 114: loss improved from 0.50113 to 0.47300, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 48s 237ms/step - loss: 0.4730 - bpp: 0.2366 - mse: 9.2327e-04\n",
      "Epoch 115/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4971 - bpp: 0.2464 - mse: 9.7926e-04[MemoryCallback]:  6479548\n",
      "\n",
      "Epoch: 114. Reducing Learning Rate from 8.775210881140083e-05 to 8.687459194334224e-05\n",
      "\n",
      "Epoch 115: loss did not improve from 0.47300\n",
      "200/200 [==============================] - 45s 221ms/step - loss: 0.4971 - bpp: 0.2464 - mse: 9.7926e-04\n",
      "Epoch 116/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5265 - bpp: 0.2476 - mse: 0.0011[MemoryCallback]:  6479548\n",
      "\n",
      "Epoch: 115. Reducing Learning Rate from 8.687459194334224e-05 to 8.600584988016635e-05\n",
      "\n",
      "Epoch 116: loss did not improve from 0.47300\n",
      "200/200 [==============================] - 47s 233ms/step - loss: 0.5265 - bpp: 0.2476 - mse: 0.0011\n",
      "Epoch 117/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5094 - bpp: 0.2402 - mse: 0.0011[MemoryCallback]:  6479948\n",
      "\n",
      "Epoch: 116. Reducing Learning Rate from 8.600584988016635e-05 to 8.51457953103818e-05\n",
      "\n",
      "Epoch 117: loss did not improve from 0.47300\n",
      "200/200 [==============================] - 46s 226ms/step - loss: 0.5094 - bpp: 0.2402 - mse: 0.0011\n",
      "Epoch 118/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4915 - bpp: 0.2343 - mse: 0.0010    [MemoryCallback]:  6480264\n",
      "\n",
      "Epoch: 117. Reducing Learning Rate from 8.51457953103818e-05 to 8.429434092249721e-05\n",
      "\n",
      "Epoch 118: loss did not improve from 0.47300\n",
      "200/200 [==============================] - 48s 237ms/step - loss: 0.4915 - bpp: 0.2343 - mse: 0.0010\n",
      "Epoch 119/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4926 - bpp: 0.2346 - mse: 0.0010[MemoryCallback]:  6480264\n",
      "\n",
      "Epoch: 118. Reducing Learning Rate from 8.429434092249721e-05 to 8.345139940502122e-05\n",
      "\n",
      "Epoch 119: loss did not improve from 0.47300\n",
      "200/200 [==============================] - 47s 232ms/step - loss: 0.4926 - bpp: 0.2346 - mse: 0.0010\n",
      "Epoch 120/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4493 - bpp: 0.2250 - mse: 8.7621e-04[MemoryCallback]:  6480264\n",
      "\n",
      "Epoch: 119. Reducing Learning Rate from 8.345139940502122e-05 to 8.261688344646245e-05\n",
      "\n",
      "Epoch 120: loss improved from 0.47300 to 0.44927, saving model to checkpoints_wavelets_L_256_1_240x240/\n",
      "200/200 [==============================] - 49s 241ms/step - loss: 0.4493 - bpp: 0.2250 - mse: 8.7621e-04\n",
      "Epoch 121/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5002 - bpp: 0.2315 - mse: 0.0010[MemoryCallback]:  6480264\n",
      "\n",
      "Epoch: 120. Reducing Learning Rate from 8.261688344646245e-05 to 8.179071301128715e-05\n",
      "\n",
      "Epoch 121: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 0.5002 - bpp: 0.2315 - mse: 0.0010\n",
      "Epoch 122/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4875 - bpp: 0.2279 - mse: 0.0010[MemoryCallback]:  6480264\n",
      "\n",
      "Epoch: 121. Reducing Learning Rate from 8.179071301128715e-05 to 8.097280806396157e-05\n",
      "\n",
      "Epoch 122: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 47s 233ms/step - loss: 0.4875 - bpp: 0.2279 - mse: 0.0010\n",
      "Epoch 123/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5511 - bpp: 0.2381 - mse: 0.0012[MemoryCallback]:  6481192\n",
      "\n",
      "Epoch: 122. Reducing Learning Rate from 8.097280806396157e-05 to 8.016308129299432e-05\n",
      "\n",
      "Epoch 123: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 50s 246ms/step - loss: 0.5511 - bpp: 0.2381 - mse: 0.0012\n",
      "Epoch 124/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5114 - bpp: 0.2332 - mse: 0.0011[MemoryCallback]:  6485420\n",
      "\n",
      "Epoch: 123. Reducing Learning Rate from 8.016308129299432e-05 to 7.936145266285166e-05\n",
      "\n",
      "Epoch 124: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.5114 - bpp: 0.2332 - mse: 0.0011\n",
      "Epoch 125/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5043 - bpp: 0.2263 - mse: 0.0011[MemoryCallback]:  6488648\n",
      "\n",
      "Epoch: 124. Reducing Learning Rate from 7.936145266285166e-05 to 7.856784213799983e-05\n",
      "\n",
      "Epoch 125: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 47s 232ms/step - loss: 0.5043 - bpp: 0.2263 - mse: 0.0011\n",
      "Epoch 126/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5051 - bpp: 0.2281 - mse: 0.0011[MemoryCallback]:  6488872\n",
      "\n",
      "Epoch: 125. Reducing Learning Rate from 7.856784213799983e-05 to 7.778216240694746e-05\n",
      "\n",
      "Epoch 126: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 49s 241ms/step - loss: 0.5051 - bpp: 0.2281 - mse: 0.0011\n",
      "Epoch 127/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4661 - bpp: 0.2211 - mse: 9.5708e-04[MemoryCallback]:  6488872\n",
      "\n",
      "Epoch: 126. Reducing Learning Rate from 7.778216240694746e-05 to 7.700434071011841e-05\n",
      "\n",
      "Epoch 127: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 48s 237ms/step - loss: 0.4661 - bpp: 0.2211 - mse: 9.5708e-04\n",
      "Epoch 128/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4768 - bpp: 0.2235 - mse: 9.8951e-04[MemoryCallback]:  6488872\n",
      "\n",
      "Epoch: 127. Reducing Learning Rate from 7.700434071011841e-05 to 7.623429701197892e-05\n",
      "\n",
      "Epoch 128: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 0.4768 - bpp: 0.2235 - mse: 9.8951e-04\n",
      "Epoch 129/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4506 - bpp: 0.2160 - mse: 9.1658e-04[MemoryCallback]:  6488872\n",
      "\n",
      "Epoch: 128. Reducing Learning Rate from 7.623429701197892e-05 to 7.547195127699524e-05\n",
      "\n",
      "Epoch 129: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.4506 - bpp: 0.2160 - mse: 9.1658e-04\n",
      "Epoch 130/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4664 - bpp: 0.2186 - mse: 9.6768e-04[MemoryCallback]:  6488872\n",
      "\n",
      "Epoch: 129. Reducing Learning Rate from 7.547195127699524e-05 to 7.471723074559122e-05\n",
      "\n",
      "Epoch 130: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 46s 226ms/step - loss: 0.4664 - bpp: 0.2186 - mse: 9.6768e-04\n",
      "Epoch 131/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5051 - bpp: 0.2225 - mse: 0.0011[MemoryCallback]:  6488872\n",
      "\n",
      "Epoch: 130. Reducing Learning Rate from 7.471723074559122e-05 to 7.397006265819073e-05\n",
      "\n",
      "Epoch 131: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.5051 - bpp: 0.2225 - mse: 0.0011\n",
      "Epoch 132/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4710 - bpp: 0.2205 - mse: 9.7853e-04[MemoryCallback]:  6488872\n",
      "\n",
      "Epoch: 131. Reducing Learning Rate from 7.397006265819073e-05 to 7.323035970330238e-05\n",
      "\n",
      "Epoch 132: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 0.4710 - bpp: 0.2205 - mse: 9.7853e-04\n",
      "Epoch 133/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4619 - bpp: 0.2139 - mse: 9.6864e-04[MemoryCallback]:  6488872\n",
      "\n",
      "Epoch: 132. Reducing Learning Rate from 7.323035970330238e-05 to 7.249805639730766e-05\n",
      "\n",
      "Epoch 133: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 47s 235ms/step - loss: 0.4619 - bpp: 0.2139 - mse: 9.6864e-04\n",
      "Epoch 134/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4850 - bpp: 0.2188 - mse: 0.0010[MemoryCallback]:  6488872\n",
      "\n",
      "Epoch: 133. Reducing Learning Rate from 7.249805639730766e-05 to 7.177307998063043e-05\n",
      "\n",
      "Epoch 134: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 47s 233ms/step - loss: 0.4850 - bpp: 0.2188 - mse: 0.0010\n",
      "Epoch 135/700\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4775 - bpp: 0.2173 - mse: 0.0010[MemoryCallback]:  6488872\n",
      "\n",
      "Epoch: 134. Reducing Learning Rate from 7.177307998063043e-05 to 7.105535041773692e-05\n",
      "\n",
      "Epoch 135: loss did not improve from 0.44927\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.4775 - bpp: 0.2173 - mse: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "hist = model.fit(x=data, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE,\n",
    "                callbacks=[\n",
    "                    Callbacks.MemoryCallback(),\n",
    "                    Callbacks.LearningRateReducer(),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(filepath=checkponts_new_path, save_weights_only=True, save_freq='epoch', monitor=\"loss\", mode='min',  save_best_only=True, verbose=1), \n",
    "                    tf.keras.callbacks.TerminateOnNaN(),\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=early_stop),\n",
    "                    tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, update_freq=\"epoch\"),            \n",
    "                    ],\n",
    "\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/WindowsDev/DataSets/vimeo_septuplet/sequences/00051/0045/im1.png\n",
      "compress\n",
      "in the compress\n",
      "decompress\n",
      "in decompress\n"
     ]
    }
   ],
   "source": [
    "path = load.load_random_path(\"folder_cloud_test.npy\")\n",
    "i=0\n",
    "out_bin = \"Test_com/test{}.bin\".format(i)\n",
    "out_decom = \"Test_com/testdcom{}.png\".format(i)\n",
    "p_on_test = \"Test_com/test_p_frame{}.png\".format(i)\n",
    "i_on_test = \"Test_com/test_i_frame{}.png\".format(i)\n",
    "\n",
    "i_frame = path + 'im1' + '.png'\n",
    "p_frame = path + 'im2' + '.png'\n",
    "print(i_frame)\n",
    "\n",
    "OpenDVCW.write_png(p_on_test, OpenDVCW.read_png_crop(p_frame, 240, 240))\n",
    "OpenDVCW.write_png(i_on_test, OpenDVCW.read_png_crop(i_frame, 240, 240))\n",
    "\n",
    "OpenDVCW.compress(model, i_frame, p_frame, out_bin, 240, 240)\n",
    "OpenDVCW.decompress(model, i_frame, out_bin, out_decom, 240, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the compress\n",
      "in decompress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as optical_flow_loss_layer_call_fn, optical_flow_loss_layer_call_and_return_conditional_losses, dwt_layer_call_fn, dwt_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_wavelets_L_256_1_240x240/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_wavelets_L_256_1_240x240/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(save_name, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
