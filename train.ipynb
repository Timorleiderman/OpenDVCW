{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/OpenDVCW'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/OpenDVCW\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/ubu-admin/Developer/tensorflow-wavelets\n",
    "%cd /workspaces/OpenDVCW\n",
    "import OpenDVCW\n",
    "import numpy as np\n",
    "import load\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import DataGen\n",
    "import Callbacks\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10000\n",
    "STEPS_PER_EPOCH = 100\n",
    "Height = 240\n",
    "Width = 240\n",
    "Channel = 3\n",
    "lmbda = 2048\n",
    "lr_init = 1e-4\n",
    "early_stop = 7\n",
    "I_QP=27\n",
    "\n",
    "args = OpenDVCW.Arguments()\n",
    "last = 2\n",
    "checkponts_last_path = \"checkpoints_L_{}_{}_{}x{}/\".format(lmbda, last, Width, Height)\n",
    "checkponts_new_path = \"checkpoints_L_{}_{}_{}x{}/\".format(lmbda, last+1, Width, Height)\n",
    "save_name = \"model_save_L_{}_{}_{}x{}\".format(lmbda, last+1, Width, Height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 14:42:31.607586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-25 14:42:31.726634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-25 14:42:31.727460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-25 14:42:31.729578: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-25 14:42:31.732136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-25 14:42:31.732865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-25 14:42:31.733609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-25 14:42:33.134512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-25 14:42:33.135291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-25 14:42:33.135936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-25 14:42:33.136658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10240 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:00:09.0, compute capability: 8.6\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "* [Loading dataset]...\n",
      "Loading weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 14:42:40.199464: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9b0c6018e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OpenDVCW.OpenDVC(width=Width, height=Height, batch_size=BATCH_SIZE, num_filters=128, lmbda=lmbda)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_init),)\n",
    "print(\"* [Model compiled]...\")\n",
    "\n",
    "print(\"* [Loading dataset]...\")\n",
    "data = DataGen.DataVimeo90kGenerator(\"folder_cloud_test.npy\", \n",
    "                                    BATCH_SIZE,\n",
    "                                    (Height,Width,Channel),\n",
    "                                    Channel,\n",
    "                                    True, \n",
    "                                    I_QP,\n",
    "                                    True)\n",
    "\n",
    "print(\"Loading weights\")\n",
    "model.load_weights(checkponts_last_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].trainable = False\n",
    "# model.layers[1].trainable = False\n",
    "# model.layers[2].trainable = True\n",
    "# model.layers[3].trainable = True\n",
    "# model.layers[4].trainable = False\n",
    "# model.layers[5].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv_analysis True\n",
      "mv_synthesis True\n",
      "res_analysis True\n",
      "res_synthesis True\n",
      "optical_flow True\n",
      "motion_compensation True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 14:43:15.406949: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 1.4522 - bpp: 0.8513 - mse: 2.9343e-04- ETA: 1s - loss: 1.4468 - bpp: 0.8470 - m[MemoryCallback]:  4730772\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.45223, saving model to checkpoints_L_2048_3_240x240/\n",
      "100/100 [==============================] - 44s 128ms/step - loss: 1.4522 - bpp: 0.8513 - mse: 2.9343e-04\n",
      "Epoch 2/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5437 - bpp: 0.8686 - mse: 3.2964e-04[MemoryCallback]:  4944904\n",
      "\n",
      "Epoch 00002: loss did not improve from 1.45223\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 1.5437 - bpp: 0.8686 - mse: 3.2964e-04\n",
      "Epoch 3/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5253 - bpp: 0.8362 - mse: 3.3646e-04[MemoryCallback]:  5046284\n",
      "\n",
      "Epoch 00003: loss did not improve from 1.45223\n",
      "100/100 [==============================] - 13s 124ms/step - loss: 1.5253 - bpp: 0.8362 - mse: 3.3646e-04\n",
      "Epoch 4/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5066 - bpp: 0.8385 - mse: 3.2620e-04[MemoryCallback]:  5099716\n",
      "\n",
      "Epoch 00004: loss did not improve from 1.45223\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 1.5066 - bpp: 0.8385 - mse: 3.2620e-04\n",
      "Epoch 5/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5374 - bpp: 0.8551 - mse: 3.3314e-04[MemoryCallback]:  5197580\n",
      "\n",
      "Epoch 00005: loss did not improve from 1.45223\n",
      "100/100 [==============================] - 13s 119ms/step - loss: 1.5374 - bpp: 0.8551 - mse: 3.3314e-04\n",
      "Epoch 6/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5805 - bpp: 0.8476 - mse: 3.5787e-04[MemoryCallback]:  5202568\n",
      "\n",
      "Epoch 00006: loss did not improve from 1.45223\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 1.5805 - bpp: 0.8476 - mse: 3.5787e-04\n",
      "Epoch 7/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5947 - bpp: 0.8501 - mse: 3.6358e-04[MemoryCallback]:  5253776\n",
      "\n",
      "Epoch 00007: loss did not improve from 1.45223\n",
      "100/100 [==============================] - 12s 114ms/step - loss: 1.5947 - bpp: 0.8501 - mse: 3.6358e-04\n",
      "Epoch 8/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.4279 - bpp: 0.8092 - mse: 3.0209e-04[MemoryCallback]:  5305272\n",
      "\n",
      "Epoch 00008: loss improved from 1.45223 to 1.42790, saving model to checkpoints_L_2048_3_240x240/\n",
      "100/100 [==============================] - 14s 134ms/step - loss: 1.4279 - bpp: 0.8092 - mse: 3.0209e-04\n",
      "Epoch 9/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.3841 - bpp: 0.8074 - mse: 2.8160e-04[MemoryCallback]:  5314776\n",
      "\n",
      "Epoch 00009: loss improved from 1.42790 to 1.38413, saving model to checkpoints_L_2048_3_240x240/\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 1.3841 - bpp: 0.8074 - mse: 2.8160e-04\n",
      "Epoch 10/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5175 - bpp: 0.8487 - mse: 3.2656e-04- ETA: 2s - loss: 1.5483 - bpp: 0.8568 - mse[MemoryCallback]:  5314776\n",
      "\n",
      "Epoch 00010: loss did not improve from 1.38413\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 1.5175 - bpp: 0.8487 - mse: 3.2656e-04\n",
      "Epoch 11/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.3340 - bpp: 0.7790 - mse: 2.7098e-04[MemoryCallback]:  5314776\n",
      "\n",
      "Epoch 00011: loss improved from 1.38413 to 1.33398, saving model to checkpoints_L_2048_3_240x240/\n",
      "100/100 [==============================] - 13s 123ms/step - loss: 1.3340 - bpp: 0.7790 - mse: 2.7098e-04\n",
      "Epoch 12/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5475 - bpp: 0.8347 - mse: 3.4803e-04[MemoryCallback]:  5362336\n",
      "\n",
      "Epoch 00012: loss did not improve from 1.33398\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 1.5475 - bpp: 0.8347 - mse: 3.4803e-04\n",
      "Epoch 13/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.5057 - bpp: 0.7968 - mse: 3.4615e-04[MemoryCallback]:  5409400\n",
      "\n",
      "Epoch 00013: loss did not improve from 1.33398\n",
      "100/100 [==============================] - 13s 123ms/step - loss: 1.5057 - bpp: 0.7968 - mse: 3.4615e-04\n",
      "Epoch 14/10000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.3396 - bpp: 0.7597 - mse: 2.8313e-04[MemoryCallback]:  5409560\n",
      "\n",
      "Epoch 00014: loss did not improve from 1.33398\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 1.3396 - bpp: 0.7597 - mse: 2.8313e-04\n",
      "Epoch 15/10000\n",
      " 93/100 [==========================>...] - ETA: 0s - loss: 1.3547 - bpp: 0.7664 - mse: 2.8725e-04"
     ]
    }
   ],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "hist = model.fit(x=data, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE,\n",
    "                callbacks=[\n",
    "                    Callbacks.MemoryCallback(),\n",
    "                    Callbacks.LearningRateReducer(),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(filepath=checkponts_new_path, save_weights_only=True, save_freq='epoch', monitor=\"loss\", mode='min',  save_best_only=True, verbose=1), \n",
    "                    tf.keras.callbacks.TerminateOnNaN(),\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=early_stop),\n",
    "                    tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, update_freq=\"epoch\"),            \n",
    "                    ],\n",
    "\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/WindowsDev/DataSets/vimeo_septuplet/sequences/00035/0959/im1.png\n",
      "compress\n"
     ]
    }
   ],
   "source": [
    "path = load.load_random_path(\"folder_cloud_test.npy\")\n",
    "i=0\n",
    "out_bin = \"Test_com/test{}.bin\".format(i)\n",
    "out_decom = \"Test_com/testdcom{}.png\".format(i)\n",
    "p_on_test = \"Test_com/test_p_frame{}.png\".format(i)\n",
    "i_on_test = \"Test_com/test_i_frame{}.png\".format(i)\n",
    "\n",
    "i_frame = path + 'im1' + '.png'\n",
    "p_frame = path + 'im2' + '.png'\n",
    "print(i_frame)\n",
    "\n",
    "OpenDVCW.write_png(p_on_test, OpenDVCW.read_png_crop(p_frame, 240, 240))\n",
    "OpenDVCW.write_png(i_on_test, OpenDVCW.read_png_crop(i_frame, 240, 240))\n",
    "\n",
    "OpenDVCW.compress(model, i_frame, p_frame, out_bin, 240, 240)\n",
    "OpenDVCW.decompress(model, i_frame, out_bin, out_decom, 240, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the compress\n",
      "in decompress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as optical_flow_loss_layer_call_fn, optical_flow_loss_layer_call_and_return_conditional_losses, mc1_layer_call_fn, mc1_layer_call_and_return_conditional_losses, res_block_layer_call_fn while saving (showing 5 of 135). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_L_1024_2_240x240/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_L_1024_2_240x240/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(save_name, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
