{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/OpenDVCW'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/OpenDVCW\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/ubu-admin/Developer/tensorflow-wavelets\n",
    "%cd /workspaces/OpenDVCW\n",
    "from train import TrainOpenDVCW\n",
    "import numpy as np\n",
    "import load\n",
    "import OpenDVCW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_folder = \"train_set_iqp22.npy\"\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 15\n",
    "# STEPS_PER_EPOCH = len(np.load(np_folder))\n",
    "STEPS_PER_EPOCH = 10000\n",
    "Height = 240\n",
    "Width = 240\n",
    "Channel = 3\n",
    "NUM_FILTERS = 256\n",
    "MV_KERNEL_SIZE=3\n",
    "RES_KERNEL_SIZE=5\n",
    "M=256\n",
    "lmbda = 2048\n",
    "lr_init = 1e-4\n",
    "lr_alpha = 1e-8\n",
    "early_stop = 2\n",
    "I_QP=22\n",
    "wavelet_name = \"haar\"\n",
    "\n",
    "checkponts_prev_path = \"\"\n",
    "checkpoints_target_path = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 21:46:22.894007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-03 21:46:22.934344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-03 21:46:22.935852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-03 21:46:22.940692: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-03 21:46:22.945084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-03 21:46:22.946566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-03 21:46:22.947973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-03 21:46:24.736751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-03 21:46:24.738252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-03 21:46:24.739610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-03 21:46:24.740694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22306 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:00:09.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# img_path = load.load_path_n(\"folder_cloud_test.npy\", 0)\n",
    "img_path = \"/mnt/WindowsDev/DataSets/Beauty_1920x1080_120fps_420_8bit_YUV_RAW/\"\n",
    "i_frame = img_path + 'im0' + '.png'\n",
    "p_frame = img_path + 'im1' + '.png'\n",
    "out_bin = \"Test_com/test{}.bin\".format(0)\n",
    "out_decom = \"Test_com/testdcom{}.png\".format(0)\n",
    "\n",
    "p_on_test = \"Test_com/test_p_frame{}.png\".format(0)\n",
    "i_on_test = \"Test_com/test_i_frame{}.png\".format(0)\n",
    "\n",
    "OpenDVCW.write_png(p_on_test, OpenDVCW.read_png_crop(p_frame, Width, Height))\n",
    "OpenDVCW.write_png(i_on_test, OpenDVCW.read_png_crop(i_frame, Width, Height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 21:47:32.371130: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 0s - loss: 66.7977 - bpp: 5.8566 - mse: 9.2989e-04\n",
      "Epoch 1: loss improved from inf to 66.79774, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 2334s 228ms/step - loss: 66.7977 - bpp: 5.8566 - mse: 9.2989e-04\n",
      "Epoch 2/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 37.9912 - bpp: 2.9181 - mse: 5.3517e-04\n",
      "Epoch 2: loss improved from 66.79774 to 37.99122, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1685s 168ms/step - loss: 37.9912 - bpp: 2.9181 - mse: 5.3517e-04\n",
      "Epoch 3/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 38.9449 - bpp: 2.2947 - mse: 5.5924e-04\n",
      "Epoch 3: loss did not improve from 37.99122\n",
      "10000/10000 [==============================] - 1553s 155ms/step - loss: 38.9449 - bpp: 2.2947 - mse: 5.5924e-04\n",
      "Epoch 4/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 26.1082 - bpp: 2.2697 - mse: 3.6375e-04\n",
      "Epoch 4: loss improved from 37.99122 to 26.10821, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1559s 156ms/step - loss: 26.1082 - bpp: 2.2697 - mse: 3.6375e-04\n",
      "Epoch 5/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 17.3891 - bpp: 2.3110 - mse: 2.3007e-04\n",
      "Epoch 5: loss improved from 26.10821 to 17.38909, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1575s 157ms/step - loss: 17.3891 - bpp: 2.3110 - mse: 2.3007e-04\n",
      "Epoch 6/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 14.0475 - bpp: 2.3896 - mse: 1.7789e-04\n",
      "Epoch 6: loss improved from 17.38909 to 14.04746, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1582s 158ms/step - loss: 14.0475 - bpp: 2.3896 - mse: 1.7789e-04\n",
      "Epoch 7/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 12.2958 - bpp: 2.4787 - mse: 1.4980e-04\n",
      "Epoch 7: loss improved from 14.04746 to 12.29584, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1577s 157ms/step - loss: 12.2958 - bpp: 2.4787 - mse: 1.4980e-04\n",
      "Epoch 8/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 10.0788 - bpp: 2.5372 - mse: 1.1508e-04\n",
      "Epoch 8: loss improved from 12.29584 to 10.07883, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1589s 159ms/step - loss: 10.0788 - bpp: 2.5372 - mse: 1.1508e-04\n",
      "Epoch 9/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 9.0268 - bpp: 2.5785 - mse: 9.8394e-05\n",
      "Epoch 9: loss improved from 10.07883 to 9.02680, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1598s 160ms/step - loss: 9.0268 - bpp: 2.5785 - mse: 9.8394e-05\n",
      "Epoch 10/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 8.1334 - bpp: 2.6304 - mse: 8.3968e-05\n",
      "Epoch 10: loss improved from 9.02680 to 8.13339, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1602s 160ms/step - loss: 8.1334 - bpp: 2.6304 - mse: 8.3968e-05\n",
      "Epoch 11/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 7.4353 - bpp: 2.6542 - mse: 7.2954e-05\n",
      "Epoch 11: loss improved from 8.13339 to 7.43526, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1580s 158ms/step - loss: 7.4353 - bpp: 2.6542 - mse: 7.2954e-05\n",
      "Epoch 12/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 6.9624 - bpp: 2.6798 - mse: 6.5348e-05\n",
      "Epoch 12: loss improved from 7.43526 to 6.96242, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1593s 159ms/step - loss: 6.9624 - bpp: 2.6798 - mse: 6.5348e-05\n",
      "Epoch 13/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 6.7620 - bpp: 2.6884 - mse: 6.2157e-05\n",
      "Epoch 13: loss improved from 6.96242 to 6.76197, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1577s 158ms/step - loss: 6.7620 - bpp: 2.6884 - mse: 6.2157e-05\n",
      "Epoch 14/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 6.6051 - bpp: 2.6978 - mse: 5.9621e-05\n",
      "Epoch 14: loss improved from 6.76197 to 6.60509, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1601s 160ms/step - loss: 6.6051 - bpp: 2.6978 - mse: 5.9621e-05\n",
      "Epoch 15/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 6.6037 - bpp: 2.6981 - mse: 5.9595e-05\n",
      "Epoch 15: loss improved from 6.60509 to 6.60370, saving model to checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/\n",
      "10000/10000 [==============================] - 1595s 159ms/step - loss: 6.6037 - bpp: 2.6981 - mse: 5.9595e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Found untraced functions such as optical_flow_loss_layer_call_fn, optical_flow_loss_layer_call_and_return_conditional_losses, dwt_layer_call_fn, dwt_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 76). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_65536_nfilt_256_epcs_15_stps_10000_I_QP_22_240x240_CosineDecay_20220703-214625/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin size:  18091 psnr:  40.64867650580335 bpp:  0.10469328703703704\n"
     ]
    }
   ],
   "source": [
    "I_QP=22\n",
    "lmbda = 65536\n",
    "np_folder = \"train_set_iqp22.npy\"\n",
    "checkponts_prev_path = \"\"\n",
    "checkpoints_target_path = \"\"\n",
    "trainer_65536 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "                           Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "                           lmbda, lr_init, lr_alpha, early_stop,\n",
    "                           I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "trainer_65536.compile()\n",
    "trainer_65536.fit()\n",
    "trainer_65536.save()\n",
    "trainer_65536.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "Epoch 1/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 25.6844 - bpp: 5.7184 - mse: 0.0012\n",
      "Epoch 1: loss improved from inf to 25.68440, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 1558s 153ms/step - loss: 25.6844 - bpp: 5.7184 - mse: 0.0012\n",
      "Epoch 2/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 14.5555 - bpp: 2.2458 - mse: 7.5133e-04\n",
      "Epoch 2: loss improved from 25.68440 to 14.55549, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 1961s 196ms/step - loss: 14.5555 - bpp: 2.2458 - mse: 7.5133e-04\n",
      "Epoch 3/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 9.1529 - bpp: 1.9818 - mse: 4.3769e-04\n",
      "Epoch 3: loss improved from 14.55549 to 9.15294, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2700s 270ms/step - loss: 9.1529 - bpp: 1.9818 - mse: 4.3769e-04\n",
      "Epoch 4/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 6.7512 - bpp: 1.8575 - mse: 2.9869e-04\n",
      "Epoch 4: loss improved from 9.15294 to 6.75124, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2782s 278ms/step - loss: 6.7512 - bpp: 1.8575 - mse: 2.9869e-04\n",
      "Epoch 5/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 5.4284 - bpp: 1.7732 - mse: 2.2309e-04\n",
      "Epoch 5: loss improved from 6.75124 to 5.42842, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2812s 281ms/step - loss: 5.4284 - bpp: 1.7732 - mse: 2.2309e-04\n",
      "Epoch 6/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 4.6230 - bpp: 1.7158 - mse: 1.7744e-04\n",
      "Epoch 6: loss improved from 5.42842 to 4.62299, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2817s 281ms/step - loss: 4.6230 - bpp: 1.7158 - mse: 1.7744e-04\n",
      "Epoch 7/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 4.1488 - bpp: 1.6761 - mse: 1.5092e-04\n",
      "Epoch 7: loss improved from 4.62299 to 4.14877, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2863s 286ms/step - loss: 4.1488 - bpp: 1.6761 - mse: 1.5092e-04\n",
      "Epoch 8/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 3.8021 - bpp: 1.6478 - mse: 1.3149e-04\n",
      "Epoch 8: loss improved from 4.14877 to 3.80209, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2650s 265ms/step - loss: 3.8021 - bpp: 1.6478 - mse: 1.3149e-04\n",
      "Epoch 9/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 3.5243 - bpp: 1.6173 - mse: 1.1640e-04\n",
      "Epoch 9: loss improved from 3.80209 to 3.52432, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2652s 265ms/step - loss: 3.5243 - bpp: 1.6173 - mse: 1.1640e-04\n",
      "Epoch 10/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 3.2691 - bpp: 1.5820 - mse: 1.0297e-04\n",
      "Epoch 10: loss improved from 3.52432 to 3.26907, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2554s 255ms/step - loss: 3.2691 - bpp: 1.5820 - mse: 1.0297e-04\n",
      "Epoch 11/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 3.1423 - bpp: 1.5706 - mse: 9.5927e-05\n",
      "Epoch 11: loss improved from 3.26907 to 3.14227, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2536s 253ms/step - loss: 3.1423 - bpp: 1.5706 - mse: 9.5927e-05\n",
      "Epoch 12/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 3.0316 - bpp: 1.5515 - mse: 9.0341e-05\n",
      "Epoch 12: loss improved from 3.14227 to 3.03162, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2649s 264ms/step - loss: 3.0316 - bpp: 1.5515 - mse: 9.0341e-05\n",
      "Epoch 13/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 2.9631 - bpp: 1.5423 - mse: 8.6722e-05\n",
      "Epoch 13: loss improved from 3.03162 to 2.96312, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2705s 270ms/step - loss: 2.9631 - bpp: 1.5423 - mse: 8.6722e-05\n",
      "Epoch 14/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 2.9486 - bpp: 1.5438 - mse: 8.5741e-05\n",
      "Epoch 14: loss improved from 2.96312 to 2.94862, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2588s 258ms/step - loss: 2.9486 - bpp: 1.5438 - mse: 8.5741e-05\n",
      "Epoch 15/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 2.9164 - bpp: 1.5394 - mse: 8.4046e-05\n",
      "Epoch 15: loss improved from 2.94862 to 2.91644, saving model to checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/\n",
      "10000/10000 [==============================] - 2621s 262ms/step - loss: 2.9164 - bpp: 1.5394 - mse: 8.4046e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Found untraced functions such as optical_flow_loss_1_layer_call_fn, optical_flow_loss_1_layer_call_and_return_conditional_losses, dwt_1_layer_call_fn, dwt_1_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 76). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_16384_nfilt_256_epcs_15_stps_10000_I_QP_27_240x240_CosineDecay_20220704-044114/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin size:  9533 psnr:  39.58461941315642 bpp:  0.05516782407407408\n"
     ]
    }
   ],
   "source": [
    "I_QP=27\n",
    "lmbda = 16384\n",
    "np_folder = \"train_set_iqp27.npy\"\n",
    "checkponts_prev_path = \"\"\n",
    "checkpoints_target_path = \"\"\n",
    "trainer_16384 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "                           Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "                           lmbda, lr_init, lr_alpha, early_stop,\n",
    "                           I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "trainer_16384.compile()\n",
    "trainer_16384.fit()\n",
    "trainer_16384.save()\n",
    "trainer_16384.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I_QP=22\n",
    "# lmbda = 8192\n",
    "# np_folder = \"train_set_iqp22.npy\"\n",
    "# checkponts_prev_path = \"checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220603-201441/\"\n",
    "# checkpoints_target_path = \"\"\n",
    "# trainer_8192 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "#                            Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "#                            lmbda, lr_init, lr_alpha, early_stop,\n",
    "#                            I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "# trainer_8192.compile()\n",
    "# trainer_8192.fit()\n",
    "# trainer_8192.save()\n",
    "# trainer_8192.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "Epoch 1/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 11.5790 - bpp: 5.6333 - mse: 0.0015\n",
      "Epoch 1: loss improved from inf to 11.57897, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1738s 170ms/step - loss: 11.5790 - bpp: 5.6333 - mse: 0.0015\n",
      "Epoch 2/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 4.3842 - bpp: 1.6860 - mse: 6.5876e-04\n",
      "Epoch 2: loss improved from 11.57897 to 4.38422, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1714s 171ms/step - loss: 4.3842 - bpp: 1.6860 - mse: 6.5876e-04\n",
      "Epoch 3/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 2.9885 - bpp: 1.1522 - mse: 4.4830e-04\n",
      "Epoch 3: loss improved from 4.38422 to 2.98845, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1705s 170ms/step - loss: 2.9885 - bpp: 1.1522 - mse: 4.4830e-04\n",
      "Epoch 4/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 2.4281 - bpp: 0.9616 - mse: 3.5802e-04\n",
      "Epoch 4: loss improved from 2.98845 to 2.42805, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1681s 168ms/step - loss: 2.4281 - bpp: 0.9616 - mse: 3.5802e-04\n",
      "Epoch 5/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 2.1069 - bpp: 0.8825 - mse: 2.9893e-04\n",
      "Epoch 5: loss improved from 2.42805 to 2.10692, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1692s 169ms/step - loss: 2.1069 - bpp: 0.8825 - mse: 2.9893e-04\n",
      "Epoch 6/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.8804 - bpp: 0.8412 - mse: 2.5372e-04\n",
      "Epoch 6: loss improved from 2.10692 to 1.88044, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1684s 168ms/step - loss: 1.8804 - bpp: 0.8412 - mse: 2.5372e-04\n",
      "Epoch 7/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.7432 - bpp: 0.8108 - mse: 2.2763e-04\n",
      "Epoch 7: loss improved from 1.88044 to 1.74318, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1748s 175ms/step - loss: 1.7432 - bpp: 0.8108 - mse: 2.2763e-04\n",
      "Epoch 8/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.6530 - bpp: 0.7955 - mse: 2.0935e-04\n",
      "Epoch 8: loss improved from 1.74318 to 1.65300, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1845s 184ms/step - loss: 1.6530 - bpp: 0.7955 - mse: 2.0935e-04\n",
      "Epoch 9/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.5839 - bpp: 0.7825 - mse: 1.9564e-04\n",
      "Epoch 9: loss improved from 1.65300 to 1.58388, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1832s 183ms/step - loss: 1.5839 - bpp: 0.7825 - mse: 1.9564e-04\n",
      "Epoch 10/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.5046 - bpp: 0.7668 - mse: 1.8012e-04\n",
      "Epoch 10: loss improved from 1.58388 to 1.50456, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1789s 179ms/step - loss: 1.5046 - bpp: 0.7668 - mse: 1.8012e-04\n",
      "Epoch 11/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.4825 - bpp: 0.7624 - mse: 1.7579e-04\n",
      "Epoch 11: loss improved from 1.50456 to 1.48245, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1657s 166ms/step - loss: 1.4825 - bpp: 0.7624 - mse: 1.7579e-04\n",
      "Epoch 12/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.4494 - bpp: 0.7526 - mse: 1.7011e-04\n",
      "Epoch 12: loss improved from 1.48245 to 1.44942, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1660s 166ms/step - loss: 1.4494 - bpp: 0.7526 - mse: 1.7011e-04\n",
      "Epoch 13/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.4236 - bpp: 0.7495 - mse: 1.6458e-04\n",
      "Epoch 13: loss improved from 1.44942 to 1.42364, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1761s 176ms/step - loss: 1.4236 - bpp: 0.7495 - mse: 1.6458e-04\n",
      "Epoch 14/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.4173 - bpp: 0.7451 - mse: 1.6412e-04\n",
      "Epoch 14: loss improved from 1.42364 to 1.41728, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1834s 183ms/step - loss: 1.4173 - bpp: 0.7451 - mse: 1.6412e-04\n",
      "Epoch 15/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.4126 - bpp: 0.7468 - mse: 1.6254e-04\n",
      "Epoch 15: loss improved from 1.41728 to 1.41256, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/\n",
      "10000/10000 [==============================] - 1846s 184ms/step - loss: 1.4126 - bpp: 0.7468 - mse: 1.6254e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Found untraced functions such as optical_flow_loss_2_layer_call_fn, optical_flow_loss_2_layer_call_and_return_conditional_losses, dwt_2_layer_call_fn, dwt_2_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 76). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_15_stps_10000_I_QP_32_240x240_CosineDecay_20220704-152812/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin size:  4024 psnr:  37.772593298637496 bpp:  0.023287037037037037\n"
     ]
    }
   ],
   "source": [
    "I_QP=32\n",
    "lmbda = 4096\n",
    "np_folder = \"train_set_iqp32.npy\"\n",
    "checkponts_prev_path = \"\"\n",
    "checkpoints_target_path = \"\"\n",
    "trainer_4096 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "                           Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "                           lmbda, lr_init, lr_alpha, early_stop,\n",
    "                           I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "trainer_4096.compile()\n",
    "trainer_4096.fit()\n",
    "trainer_4096.save()\n",
    "trainer_4096.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I_QP=30\n",
    "# lmbda = 2048\n",
    "# np_folder = \"train_set_iqp30.npy\"\n",
    "# checkponts_prev_path = \"checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220604-001653/\"\n",
    "# checkpoints_target_path = \"\"\n",
    "# trainer_2048 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "#                            Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "#                            lmbda, lr_init, lr_alpha, early_stop,\n",
    "#                            I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "# trainer_2048.compile()\n",
    "# trainer_2048.fit()\n",
    "# trainer_2048.save()\n",
    "# trainer_2048.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "Epoch 1/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 6.8927 - bpp: 5.5172 - mse: 0.0013\n",
      "Epoch 1: loss improved from inf to 6.89273, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1720s 169ms/step - loss: 6.8927 - bpp: 5.5172 - mse: 0.0013\n",
      "Epoch 2/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.9019 - bpp: 1.1167 - mse: 7.6683e-04\n",
      "Epoch 2: loss improved from 6.89273 to 1.90190, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1691s 169ms/step - loss: 1.9019 - bpp: 1.1167 - mse: 7.6683e-04\n",
      "Epoch 3/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.1455 - bpp: 0.5268 - mse: 6.0419e-04\n",
      "Epoch 3: loss improved from 1.90190 to 1.14549, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1688s 169ms/step - loss: 1.1455 - bpp: 0.5268 - mse: 6.0419e-04\n",
      "Epoch 4/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.9509 - bpp: 0.4251 - mse: 5.1351e-04\n",
      "Epoch 4: loss improved from 1.14549 to 0.95092, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1703s 170ms/step - loss: 0.9509 - bpp: 0.4251 - mse: 5.1351e-04\n",
      "Epoch 5/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.8452 - bpp: 0.3861 - mse: 4.4835e-04\n",
      "Epoch 5: loss improved from 0.95092 to 0.84522, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1707s 171ms/step - loss: 0.8452 - bpp: 0.3861 - mse: 4.4835e-04\n",
      "Epoch 6/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.8014 - bpp: 0.3718 - mse: 4.1954e-04\n",
      "Epoch 6: loss improved from 0.84522 to 0.80140, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1717s 171ms/step - loss: 0.8014 - bpp: 0.3718 - mse: 4.1954e-04\n",
      "Epoch 7/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.7722 - bpp: 0.3638 - mse: 3.9883e-04\n",
      "Epoch 7: loss improved from 0.80140 to 0.77216, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1687s 169ms/step - loss: 0.7722 - bpp: 0.3638 - mse: 3.9883e-04\n",
      "Epoch 8/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.7499 - bpp: 0.3581 - mse: 3.8261e-04\n",
      "Epoch 8: loss improved from 0.77216 to 0.74988, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1686s 168ms/step - loss: 0.7499 - bpp: 0.3581 - mse: 3.8261e-04\n",
      "Epoch 9/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.7358 - bpp: 0.3538 - mse: 3.7306e-04\n",
      "Epoch 9: loss improved from 0.74988 to 0.73579, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1694s 169ms/step - loss: 0.7358 - bpp: 0.3538 - mse: 3.7306e-04\n",
      "Epoch 10/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.7199 - bpp: 0.3512 - mse: 3.6004e-04\n",
      "Epoch 10: loss improved from 0.73579 to 0.71988, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1699s 170ms/step - loss: 0.7199 - bpp: 0.3512 - mse: 3.6004e-04\n",
      "Epoch 11/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.7201 - bpp: 0.3510 - mse: 3.6041e-04\n",
      "Epoch 11: loss did not improve from 0.71988\n",
      "10000/10000 [==============================] - 1709s 171ms/step - loss: 0.7201 - bpp: 0.3510 - mse: 3.6041e-04\n",
      "Epoch 12/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.7041 - bpp: 0.3476 - mse: 3.4809e-04\n",
      "Epoch 12: loss improved from 0.71988 to 0.70405, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1954s 195ms/step - loss: 0.7041 - bpp: 0.3476 - mse: 3.4809e-04\n",
      "Epoch 13/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.7049 - bpp: 0.3469 - mse: 3.4960e-04\n",
      "Epoch 13: loss did not improve from 0.70405\n",
      "10000/10000 [==============================] - 1936s 193ms/step - loss: 0.7049 - bpp: 0.3469 - mse: 3.4960e-04\n",
      "Epoch 14/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.6935 - bpp: 0.3439 - mse: 3.4144e-04\n",
      "Epoch 14: loss improved from 0.70405 to 0.69350, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1923s 192ms/step - loss: 0.6935 - bpp: 0.3439 - mse: 3.4144e-04\n",
      "Epoch 15/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.6850 - bpp: 0.3429 - mse: 3.3410e-04\n",
      "Epoch 15: loss improved from 0.69350 to 0.68499, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/\n",
      "10000/10000 [==============================] - 1864s 186ms/step - loss: 0.6850 - bpp: 0.3429 - mse: 3.3410e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Found untraced functions such as optical_flow_loss_3_layer_call_fn, optical_flow_loss_3_layer_call_and_return_conditional_losses, dwt_3_layer_call_fn, dwt_3_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 76). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_15_stps_10000_I_QP_37_240x240_CosineDecay_20220704-225023/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin size:  1579 psnr:  35.861868667946084 bpp:  0.009137731481481481\n"
     ]
    }
   ],
   "source": [
    "I_QP=37\n",
    "lmbda = 1024\n",
    "np_folder = \"train_set_iqp37.npy\"\n",
    "checkponts_prev_path = \"\"\n",
    "checkpoints_target_path = \"\"\n",
    "trainer_1024 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "                           Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "                           lmbda, lr_init, lr_alpha, early_stop,\n",
    "                           I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "trainer_1024.compile()\n",
    "trainer_1024.fit()\n",
    "trainer_1024.save()\n",
    "trainer_1024.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I_QP=40\n",
    "# lmbda = 512\n",
    "# np_folder = \"train_set_iqp40.npy\"\n",
    "# checkponts_prev_path = \"checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220604-041729/\"\n",
    "# checkpoints_target_path = \"\"\n",
    "# trainer_512 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "#                            Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "#                            lmbda, lr_init, lr_alpha, early_stop,\n",
    "#                            I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "# trainer_512.compile()\n",
    "# trainer_512.fit()\n",
    "# trainer_512.save()\n",
    "# trainer_512.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "Epoch 1/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 5.7667 - bpp: 5.4472 - mse: 0.0012\n",
      "Epoch 1: loss improved from inf to 5.76669, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 1949s 190ms/step - loss: 5.7667 - bpp: 5.4472 - mse: 0.0012\n",
      "Epoch 2/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.1367 - bpp: 0.8595 - mse: 0.0011\n",
      "Epoch 2: loss improved from 5.76669 to 1.13667, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 1981s 198ms/step - loss: 1.1367 - bpp: 0.8595 - mse: 0.0011\n",
      "Epoch 3/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.5482 - bpp: 0.2883 - mse: 0.0010\n",
      "Epoch 3: loss improved from 1.13667 to 0.54820, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 1998s 199ms/step - loss: 0.5482 - bpp: 0.2883 - mse: 0.0010\n",
      "Epoch 4/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.4296 - bpp: 0.1914 - mse: 9.3058e-04\n",
      "Epoch 4: loss improved from 0.54820 to 0.42962, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 2037s 203ms/step - loss: 0.4296 - bpp: 0.1914 - mse: 9.3058e-04\n",
      "Epoch 5/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.3942 - bpp: 0.1695 - mse: 8.7758e-04\n",
      "Epoch 5: loss improved from 0.42962 to 0.39415, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 1922s 192ms/step - loss: 0.3942 - bpp: 0.1695 - mse: 8.7758e-04\n",
      "Epoch 6/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.3687 - bpp: 0.1596 - mse: 8.1674e-04\n",
      "Epoch 6: loss improved from 0.39415 to 0.36865, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 2006s 200ms/step - loss: 0.3687 - bpp: 0.1596 - mse: 8.1674e-04\n",
      "Epoch 7/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.3591 - bpp: 0.1548 - mse: 7.9793e-04\n",
      "Epoch 7: loss improved from 0.36865 to 0.35910, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 2035s 203ms/step - loss: 0.3591 - bpp: 0.1548 - mse: 7.9793e-04\n",
      "Epoch 8/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.3498 - bpp: 0.1510 - mse: 7.7643e-04\n",
      "Epoch 8: loss improved from 0.35910 to 0.34977, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 2076s 207ms/step - loss: 0.3498 - bpp: 0.1510 - mse: 7.7643e-04\n",
      "Epoch 9/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.3460 - bpp: 0.1501 - mse: 7.6553e-04\n",
      "Epoch 9: loss improved from 0.34977 to 0.34603, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 2050s 205ms/step - loss: 0.3460 - bpp: 0.1501 - mse: 7.6553e-04\n",
      "Epoch 10/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.3389 - bpp: 0.1483 - mse: 7.4462e-04\n",
      "Epoch 10: loss improved from 0.34603 to 0.33887, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 2067s 206ms/step - loss: 0.3389 - bpp: 0.1483 - mse: 7.4462e-04\n",
      "Epoch 11/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.3358 - bpp: 0.1472 - mse: 7.3697e-04\n",
      "Epoch 11: loss improved from 0.33887 to 0.33582, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 2123s 212ms/step - loss: 0.3358 - bpp: 0.1472 - mse: 7.3697e-04\n",
      "Epoch 12/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.3317 - bpp: 0.1458 - mse: 7.2617e-04\n",
      "Epoch 12: loss improved from 0.33582 to 0.33169, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 2122s 212ms/step - loss: 0.3317 - bpp: 0.1458 - mse: 7.2617e-04\n",
      "Epoch 13/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.3291 - bpp: 0.1453 - mse: 7.1789e-04\n",
      "Epoch 13: loss improved from 0.33169 to 0.32906, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 2175s 217ms/step - loss: 0.3291 - bpp: 0.1453 - mse: 7.1789e-04\n",
      "Epoch 14/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.3330 - bpp: 0.1465 - mse: 7.2855e-04\n",
      "Epoch 14: loss did not improve from 0.32906\n",
      "10000/10000 [==============================] - 2112s 211ms/step - loss: 0.3330 - bpp: 0.1465 - mse: 7.2855e-04\n",
      "Epoch 15/15\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.3285 - bpp: 0.1452 - mse: 7.1594e-04\n",
      "Epoch 15: loss improved from 0.32906 to 0.32851, saving model to checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/\n",
      "10000/10000 [==============================] - 2129s 213ms/step - loss: 0.3285 - bpp: 0.1452 - mse: 7.1594e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Found untraced functions such as optical_flow_loss_4_layer_call_fn, optical_flow_loss_4_layer_call_and_return_conditional_losses, dwt_4_layer_call_fn, dwt_4_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 76). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_256_nfilt_256_epcs_15_stps_10000_I_QP_42_240x240_CosineDecay_20220705-061459/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function OpenDVCW.compress at 0x7fabfcfacf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function OpenDVCW.compress at 0x7fabfcfacf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function OpenDVCW.decompress at 0x7fac1c28c550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function OpenDVCW.decompress at 0x7fac1c28c550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin size:  656 psnr:  34.53375720404969 bpp:  0.0037962962962962963\n"
     ]
    }
   ],
   "source": [
    "I_QP=42\n",
    "lmbda = 256\n",
    "np_folder = \"train_set_iqp42.npy\"\n",
    "checkponts_prev_path = \"\"\n",
    "checkpoints_target_path = \"\"\n",
    "trainer_256 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "                           Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "                           lmbda, lr_init, lr_alpha, early_stop,\n",
    "                           I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "trainer_256.compile()\n",
    "trainer_256.fit()\n",
    "trainer_256.save()\n",
    "trainer_256.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin size:  18091 psnr:  40.64930483510711 bpp:  0.10469328703703704\n",
      "bin size:  9533 psnr:  39.582951273704765 bpp:  0.05516782407407408\n",
      "bin size:  4024 psnr:  37.772609498763494 bpp:  0.023287037037037037\n",
      "bin size:  1579 psnr:  35.86185227190701 bpp:  0.009137731481481481\n",
      "bin size:  656 psnr:  34.533759399714484 bpp:  0.0037962962962962963\n"
     ]
    }
   ],
   "source": [
    "trainer_65536.test(i_frame, p_frame, out_bin, out_decom)\n",
    "trainer_16384.test(i_frame, p_frame, out_bin, out_decom)\n",
    "# trainer_8192.test(i_frame, p_frame, out_bin, out_decom)\n",
    "trainer_4096.test(i_frame, p_frame, out_bin, out_decom)\n",
    "# trainer_2048.test(i_frame, p_frame, out_bin, out_decom)\n",
    "trainer_1024.test(i_frame, p_frame, out_bin, out_decom)\n",
    "# trainer_512.test(i_frame, p_frame, out_bin, out_decom)\n",
    "trainer_256.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
