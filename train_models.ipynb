{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/OpenDVCW'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/OpenDVCW\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/ubu-admin/Developer/tensorflow-wavelets\n",
    "%cd /workspaces/OpenDVCW\n",
    "from train import TrainOpenDVCW\n",
    "import numpy as np\n",
    "import load\n",
    "import OpenDVCW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 12\n",
    "STEPS_PER_EPOCH = 6000\n",
    "Height = 240\n",
    "Width = 240\n",
    "Channel = 3\n",
    "NUM_FILTERS = 256\n",
    "MV_KERNEL_SIZE=3\n",
    "RES_KERNEL_SIZE=5\n",
    "M=256\n",
    "lmbda = 2048\n",
    "lr_init = 1e-4\n",
    "lr_alpha = 1e-8\n",
    "early_stop = 3\n",
    "I_QP=22\n",
    "wavelet_name = \"haar\"\n",
    "np_folder = \"folder_cloud_test.npy\"\n",
    "checkponts_prev_path = \"\"\n",
    "checkpoints_target_path = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = load.load_path_n(\"folder_cloud_test.npy\", 0)\n",
    "img_path = \"/mnt/WindowsDev/DataSets/Beauty_1920x1080_120fps_420_8bit_YUV_RAW/\"\n",
    "i_frame = img_path + 'im0' + '.png'\n",
    "p_frame = img_path + 'im1' + '.png'\n",
    "out_bin = \"Test_com/test{}.bin\".format(0)\n",
    "out_decom = \"Test_com/testdcom{}.png\".format(0)\n",
    "\n",
    "p_on_test = \"Test_com/test_p_frame{}.png\".format(0)\n",
    "i_on_test = \"Test_com/test_i_frame{}.png\".format(0)\n",
    "\n",
    "OpenDVCW.write_png(p_on_test, OpenDVCW.read_png_crop(p_frame, Width, Height))\n",
    "OpenDVCW.write_png(i_on_test, OpenDVCW.read_png_crop(i_frame, Width, Height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "Epoch 1/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 17.9237 - bpp: 7.2441 - mse: 0.0013\n",
      "Epoch 1: loss improved from inf to 17.92374, saving model to checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/\n",
      "6000/6000 [==============================] - 910s 147ms/step - loss: 17.9237 - bpp: 7.2441 - mse: 0.0013\n",
      "Epoch 2/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 6.9798 - bpp: 2.9476 - mse: 4.9222e-04\n",
      "Epoch 2: loss improved from 17.92374 to 6.97984, saving model to checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/\n",
      "6000/6000 [==============================] - 882s 147ms/step - loss: 6.9798 - bpp: 2.9476 - mse: 4.9222e-04\n",
      "Epoch 3/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 4.0372 - bpp: 1.7857 - mse: 2.7484e-04\n",
      "Epoch 3: loss improved from 6.97984 to 4.03720, saving model to checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/\n",
      "6000/6000 [==============================] - 882s 147ms/step - loss: 4.0372 - bpp: 1.7857 - mse: 2.7484e-04\n",
      "Epoch 4/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 3.3970 - bpp: 1.5009 - mse: 2.3147e-04\n",
      "Epoch 4: loss improved from 4.03720 to 3.39705, saving model to checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/\n",
      "6000/6000 [==============================] - 883s 147ms/step - loss: 3.3970 - bpp: 1.5009 - mse: 2.3147e-04\n",
      "Epoch 5/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 2.7170 - bpp: 1.2579 - mse: 1.7812e-04\n",
      "Epoch 5: loss improved from 3.39705 to 2.71703, saving model to checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/\n",
      "6000/6000 [==============================] - 882s 147ms/step - loss: 2.7170 - bpp: 1.2579 - mse: 1.7812e-04\n",
      "Epoch 6/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 2.3184 - bpp: 1.1209 - mse: 1.4618e-04\n",
      "Epoch 6: loss improved from 2.71703 to 2.31837, saving model to checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/\n",
      "6000/6000 [==============================] - 879s 146ms/step - loss: 2.3184 - bpp: 1.1209 - mse: 1.4618e-04\n",
      "Epoch 7/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 2.1579 - bpp: 1.0518 - mse: 1.3501e-04\n",
      "Epoch 7: loss improved from 2.31837 to 2.15786, saving model to checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/\n",
      "6000/6000 [==============================] - 881s 147ms/step - loss: 2.1579 - bpp: 1.0518 - mse: 1.3501e-04\n",
      "Epoch 8/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 2.0096 - bpp: 1.0055 - mse: 1.2257e-04\n",
      "Epoch 8: loss improved from 2.15786 to 2.00963, saving model to checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/\n",
      "6000/6000 [==============================] - 878s 146ms/step - loss: 2.0096 - bpp: 1.0055 - mse: 1.2257e-04\n",
      "Epoch 9/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.9346 - bpp: 0.9834 - mse: 1.1612e-04\n",
      "Epoch 9: loss improved from 2.00963 to 1.93464, saving model to checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/\n",
      "6000/6000 [==============================] - 883s 147ms/step - loss: 1.9346 - bpp: 0.9834 - mse: 1.1612e-04\n",
      "Epoch 10/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.8297 - bpp: 0.9508 - mse: 1.0728e-04\n",
      "Epoch 10: loss improved from 1.93464 to 1.82972, saving model to checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/\n",
      "6000/6000 [==============================] - 882s 147ms/step - loss: 1.8297 - bpp: 0.9508 - mse: 1.0728e-04\n",
      "Epoch 11/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.8508 - bpp: 0.9546 - mse: 1.0940e-04\n",
      "Epoch 11: loss did not improve from 1.82972\n",
      "6000/6000 [==============================] - 879s 146ms/step - loss: 1.8508 - bpp: 0.9546 - mse: 1.0940e-04\n",
      "Epoch 12/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.8326 - bpp: 0.9505 - mse: 1.0769e-04\n",
      "Epoch 12: loss did not improve from 1.82972\n",
      "6000/6000 [==============================] - 880s 146ms/step - loss: 1.8326 - bpp: 0.9505 - mse: 1.0769e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Found untraced functions such as optical_flow_loss_5_layer_call_fn, optical_flow_loss_5_layer_call_and_return_conditional_losses, dwt_5_layer_call_fn, dwt_5_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_8192_nfilt_256_epcs_12_stps_6000_I_QP_22_240x240_CosineDecay_20220602-201330/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compress\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function OpenDVCW.compress at 0x7f4e000f8940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function OpenDVCW.compress at 0x7f4e000f8940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decompress\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function OpenDVCW.decompress at 0x7f4e00d3e8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function OpenDVCW.decompress at 0x7f4e00d3e8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin size:  3697 psnr:  38.98808230330149 bpp:  0.021394675925925925\n"
     ]
    }
   ],
   "source": [
    "I_QP=22\n",
    "lmbda = 4096*2\n",
    "np_folder = \"train_set_iqp22.npy\"\n",
    "checkponts_prev_path = \"\" \n",
    "checkpoints_target_path = \"\"\n",
    "trainer_11 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "                           Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "                           lmbda, lr_init, lr_alpha, early_stop,\n",
    "                           I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "trainer_11.compile()\n",
    "trainer_11.fit()\n",
    "trainer_11.save()\n",
    "trainer_11.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "Epoch 1/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 14.1188 - bpp: 7.2322 - mse: 0.0017\n",
      "Epoch 1: loss improved from inf to 14.11879, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/\n",
      "6000/6000 [==============================] - 884s 143ms/step - loss: 14.1188 - bpp: 7.2322 - mse: 0.0017\n",
      "Epoch 2/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 5.5577 - bpp: 2.9106 - mse: 6.4626e-04\n",
      "Epoch 2: loss improved from 14.11879 to 5.55765, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/\n",
      "6000/6000 [==============================] - 858s 143ms/step - loss: 5.5577 - bpp: 2.9106 - mse: 6.4626e-04\n",
      "Epoch 3/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 3.5615 - bpp: 1.6374 - mse: 4.6973e-04\n",
      "Epoch 3: loss improved from 5.55765 to 3.56146, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/\n",
      "6000/6000 [==============================] - 860s 143ms/step - loss: 3.5615 - bpp: 1.6374 - mse: 4.6973e-04\n",
      "Epoch 4/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 2.9441 - bpp: 1.2535 - mse: 4.1276e-04\n",
      "Epoch 4: loss improved from 3.56146 to 2.94415, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/\n",
      "6000/6000 [==============================] - 862s 144ms/step - loss: 2.9441 - bpp: 1.2535 - mse: 4.1276e-04\n",
      "Epoch 5/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 2.5852 - bpp: 1.0854 - mse: 3.6618e-04\n",
      "Epoch 5: loss improved from 2.94415 to 2.58523, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/\n",
      "6000/6000 [==============================] - 860s 143ms/step - loss: 2.5852 - bpp: 1.0854 - mse: 3.6618e-04\n",
      "Epoch 6/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 2.1857 - bpp: 0.9804 - mse: 2.9425e-04\n",
      "Epoch 6: loss improved from 2.58523 to 2.18570, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/\n",
      "6000/6000 [==============================] - 861s 143ms/step - loss: 2.1857 - bpp: 0.9804 - mse: 2.9425e-04\n",
      "Epoch 7/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 2.0223 - bpp: 0.9343 - mse: 2.6564e-04\n",
      "Epoch 7: loss improved from 2.18570 to 2.02233, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/\n",
      "6000/6000 [==============================] - 861s 143ms/step - loss: 2.0223 - bpp: 0.9343 - mse: 2.6564e-04\n",
      "Epoch 8/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.8994 - bpp: 0.9021 - mse: 2.4349e-04\n",
      "Epoch 8: loss improved from 2.02233 to 1.89944, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/\n",
      "6000/6000 [==============================] - 863s 144ms/step - loss: 1.8994 - bpp: 0.9021 - mse: 2.4349e-04\n",
      "Epoch 9/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.7979 - bpp: 0.8818 - mse: 2.2365e-04\n",
      "Epoch 9: loss improved from 1.89944 to 1.79787, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/\n",
      "6000/6000 [==============================] - 859s 143ms/step - loss: 1.7979 - bpp: 0.8818 - mse: 2.2365e-04\n",
      "Epoch 10/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.7297 - bpp: 0.8652 - mse: 2.1106e-04\n",
      "Epoch 10: loss improved from 1.79787 to 1.72971, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/\n",
      "6000/6000 [==============================] - 859s 143ms/step - loss: 1.7297 - bpp: 0.8652 - mse: 2.1106e-04\n",
      "Epoch 11/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.6890 - bpp: 0.8585 - mse: 2.0277e-04\n",
      "Epoch 11: loss improved from 1.72971 to 1.68902, saving model to checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/\n",
      "6000/6000 [==============================] - 859s 143ms/step - loss: 1.6890 - bpp: 0.8585 - mse: 2.0277e-04\n",
      "Epoch 12/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.7179 - bpp: 0.8650 - mse: 2.0823e-04\n",
      "Epoch 12: loss did not improve from 1.68902\n",
      "6000/6000 [==============================] - 857s 143ms/step - loss: 1.7179 - bpp: 0.8650 - mse: 2.0823e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Found untraced functions such as optical_flow_loss_6_layer_call_fn, optical_flow_loss_6_layer_call_and_return_conditional_losses, dwt_6_layer_call_fn, dwt_6_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_4096_nfilt_256_epcs_12_stps_6000_I_QP_27_240x240_CosineDecay_20220602-231542/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compress\n",
      "decompress\n",
      "bin size:  4517 psnr:  37.48429368763955 bpp:  0.026140046296296297\n"
     ]
    }
   ],
   "source": [
    "I_QP=27\n",
    "lmbda = 2048*2\n",
    "np_folder = \"train_set_iqp27.npy\"\n",
    "checkponts_prev_path = \"\" \n",
    "checkpoints_target_path = \"\"\n",
    "trainer_12 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "                           Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "                           lmbda, lr_init, lr_alpha, early_stop,\n",
    "                           I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "trainer_12.compile()\n",
    "trainer_12.fit()\n",
    "trainer_12.save()\n",
    "trainer_12.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "Epoch 1/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 10.0751 - bpp: 7.2317 - mse: 0.0014\n",
      "Epoch 1: loss improved from inf to 10.07510, saving model to checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/\n",
      "6000/6000 [==============================] - 880s 142ms/step - loss: 10.0751 - bpp: 7.2317 - mse: 0.0014\n",
      "Epoch 2/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 3.6487 - bpp: 2.6880 - mse: 4.6910e-04\n",
      "Epoch 2: loss improved from 10.07510 to 3.64868, saving model to checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/\n",
      "6000/6000 [==============================] - 858s 143ms/step - loss: 3.6487 - bpp: 2.6880 - mse: 4.6910e-04\n",
      "Epoch 3/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 2.0576 - bpp: 1.1765 - mse: 4.3022e-04\n",
      "Epoch 3: loss improved from 3.64868 to 2.05760, saving model to checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/\n",
      "6000/6000 [==============================] - 857s 143ms/step - loss: 2.0576 - bpp: 1.1765 - mse: 4.3022e-04\n",
      "Epoch 4/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.4967 - bpp: 0.7856 - mse: 3.4726e-04\n",
      "Epoch 4: loss improved from 2.05760 to 1.49675, saving model to checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/\n",
      "6000/6000 [==============================] - 859s 143ms/step - loss: 1.4967 - bpp: 0.7856 - mse: 3.4726e-04\n",
      "Epoch 5/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.2527 - bpp: 0.6338 - mse: 3.0221e-04\n",
      "Epoch 5: loss improved from 1.49675 to 1.25272, saving model to checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/\n",
      "6000/6000 [==============================] - 859s 143ms/step - loss: 1.2527 - bpp: 0.6338 - mse: 3.0221e-04\n",
      "Epoch 6/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.1157 - bpp: 0.5648 - mse: 2.6899e-04\n",
      "Epoch 6: loss improved from 1.25272 to 1.11569, saving model to checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/\n",
      "6000/6000 [==============================] - 861s 143ms/step - loss: 1.1157 - bpp: 0.5648 - mse: 2.6899e-04\n",
      "Epoch 7/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.0621 - bpp: 0.5332 - mse: 2.5827e-04\n",
      "Epoch 7: loss improved from 1.11569 to 1.06214, saving model to checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/\n",
      "6000/6000 [==============================] - 859s 143ms/step - loss: 1.0621 - bpp: 0.5332 - mse: 2.5827e-04\n",
      "Epoch 8/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.9850 - bpp: 0.5025 - mse: 2.3559e-04\n",
      "Epoch 8: loss improved from 1.06214 to 0.98499, saving model to checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/\n",
      "6000/6000 [==============================] - 857s 143ms/step - loss: 0.9850 - bpp: 0.5025 - mse: 2.3559e-04\n",
      "Epoch 9/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.9564 - bpp: 0.4920 - mse: 2.2678e-04\n",
      "Epoch 9: loss improved from 0.98499 to 0.95643, saving model to checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/\n",
      "6000/6000 [==============================] - 857s 143ms/step - loss: 0.9564 - bpp: 0.4920 - mse: 2.2678e-04\n",
      "Epoch 10/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.9438 - bpp: 0.4845 - mse: 2.2427e-04\n",
      "Epoch 10: loss improved from 0.95643 to 0.94379, saving model to checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/\n",
      "6000/6000 [==============================] - 860s 143ms/step - loss: 0.9438 - bpp: 0.4845 - mse: 2.2427e-04\n",
      "Epoch 11/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.9179 - bpp: 0.4763 - mse: 2.1562e-04\n",
      "Epoch 11: loss improved from 0.94379 to 0.91788, saving model to checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/\n",
      "6000/6000 [==============================] - 860s 143ms/step - loss: 0.9179 - bpp: 0.4763 - mse: 2.1562e-04\n",
      "Epoch 12/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.9375 - bpp: 0.4827 - mse: 2.2205e-04\n",
      "Epoch 12: loss did not improve from 0.91788\n",
      "6000/6000 [==============================] - 860s 143ms/step - loss: 0.9375 - bpp: 0.4827 - mse: 2.2205e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Found untraced functions such as optical_flow_loss_7_layer_call_fn, optical_flow_loss_7_layer_call_and_return_conditional_losses, dwt_7_layer_call_fn, dwt_7_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_2048_nfilt_256_epcs_12_stps_6000_I_QP_30_240x240_CosineDecay_20220603-021042/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compress\n",
      "decompress\n",
      "bin size:  1310 psnr:  37.64010916306803 bpp:  0.007581018518518518\n"
     ]
    }
   ],
   "source": [
    "I_QP=30\n",
    "lmbda = 1024*2\n",
    "np_folder = \"train_set_iqp30.npy\"\n",
    "checkponts_prev_path = \"\" \n",
    "checkpoints_target_path = \"\"\n",
    "trainer_13 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "                           Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "                           lmbda, lr_init, lr_alpha, early_stop,\n",
    "                           I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "trainer_13.compile()\n",
    "trainer_13.fit()\n",
    "trainer_13.save()\n",
    "trainer_13.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "Epoch 1/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 8.6454 - bpp: 7.2203 - mse: 0.0014\n",
      "Epoch 1: loss improved from inf to 8.64544, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/\n",
      "6000/6000 [==============================] - 885s 143ms/step - loss: 8.6454 - bpp: 7.2203 - mse: 0.0014\n",
      "Epoch 2/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 3.3624 - bpp: 2.6160 - mse: 7.2891e-04\n",
      "Epoch 2: loss improved from 8.64544 to 3.36241, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/\n",
      "6000/6000 [==============================] - 863s 144ms/step - loss: 3.3624 - bpp: 2.6160 - mse: 7.2891e-04\n",
      "Epoch 3/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.7803 - bpp: 1.0756 - mse: 6.8824e-04\n",
      "Epoch 3: loss improved from 3.36241 to 1.78032, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/\n",
      "6000/6000 [==============================] - 863s 144ms/step - loss: 1.7803 - bpp: 1.0756 - mse: 6.8824e-04\n",
      "Epoch 4/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.3051 - bpp: 0.6623 - mse: 6.2773e-04\n",
      "Epoch 4: loss improved from 1.78032 to 1.30514, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/\n",
      "6000/6000 [==============================] - 862s 143ms/step - loss: 1.3051 - bpp: 0.6623 - mse: 6.2773e-04\n",
      "Epoch 5/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.0737 - bpp: 0.5181 - mse: 5.4249e-04\n",
      "Epoch 5: loss improved from 1.30514 to 1.07365, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/\n",
      "6000/6000 [==============================] - 861s 143ms/step - loss: 1.0737 - bpp: 0.5181 - mse: 5.4249e-04\n",
      "Epoch 6/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.9512 - bpp: 0.4525 - mse: 4.8703e-04\n",
      "Epoch 6: loss improved from 1.07365 to 0.95117, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/\n",
      "6000/6000 [==============================] - 861s 143ms/step - loss: 0.9512 - bpp: 0.4525 - mse: 4.8703e-04\n",
      "Epoch 7/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.8793 - bpp: 0.4242 - mse: 4.4448e-04\n",
      "Epoch 7: loss improved from 0.95117 to 0.87934, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/\n",
      "6000/6000 [==============================] - 865s 144ms/step - loss: 0.8793 - bpp: 0.4242 - mse: 4.4448e-04\n",
      "Epoch 8/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.8420 - bpp: 0.4085 - mse: 4.2333e-04\n",
      "Epoch 8: loss improved from 0.87934 to 0.84197, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/\n",
      "6000/6000 [==============================] - 863s 144ms/step - loss: 0.8420 - bpp: 0.4085 - mse: 4.2333e-04\n",
      "Epoch 9/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.8004 - bpp: 0.3947 - mse: 3.9613e-04\n",
      "Epoch 9: loss improved from 0.84197 to 0.80038, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/\n",
      "6000/6000 [==============================] - 861s 143ms/step - loss: 0.8004 - bpp: 0.3947 - mse: 3.9613e-04\n",
      "Epoch 10/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.7925 - bpp: 0.3907 - mse: 3.9235e-04\n",
      "Epoch 10: loss improved from 0.80038 to 0.79248, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/\n",
      "6000/6000 [==============================] - 866s 144ms/step - loss: 0.7925 - bpp: 0.3907 - mse: 3.9235e-04\n",
      "Epoch 11/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.7847 - bpp: 0.3893 - mse: 3.8614e-04\n",
      "Epoch 11: loss improved from 0.79248 to 0.78468, saving model to checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/\n",
      "6000/6000 [==============================] - 862s 144ms/step - loss: 0.7847 - bpp: 0.3893 - mse: 3.8614e-04\n",
      "Epoch 12/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.7892 - bpp: 0.3898 - mse: 3.9009e-04\n",
      "Epoch 12: loss did not improve from 0.78468\n",
      "6000/6000 [==============================] - 861s 143ms/step - loss: 0.7892 - bpp: 0.3898 - mse: 3.9009e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Found untraced functions such as optical_flow_loss_8_layer_call_fn, optical_flow_loss_8_layer_call_and_return_conditional_losses, dwt_8_layer_call_fn, dwt_8_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_1024_nfilt_256_epcs_12_stps_6000_I_QP_35_240x240_CosineDecay_20220603-050441/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compress\n",
      "decompress\n",
      "bin size:  1744 psnr:  35.86675893961802 bpp:  0.010092592592592592\n"
     ]
    }
   ],
   "source": [
    "I_QP=35\n",
    "lmbda = 512*2\n",
    "np_folder = \"train_set_iqp35.npy\"\n",
    "checkponts_prev_path = \"\" \n",
    "checkpoints_target_path = \"\"\n",
    "trainer_14 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "                           Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "                           lmbda, lr_init, lr_alpha, early_stop,\n",
    "                           I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "trainer_14.compile()\n",
    "trainer_14.fit()\n",
    "trainer_14.save()\n",
    "trainer_14.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [Model compiled]...\n",
      "Epoch 1/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 7.8850 - bpp: 7.1940 - mse: 0.0013\n",
      "Epoch 1: loss improved from inf to 7.88498, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 890s 144ms/step - loss: 7.8850 - bpp: 7.1940 - mse: 0.0013\n",
      "Epoch 2/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 2.7269 - bpp: 2.4437 - mse: 5.5315e-04\n",
      "Epoch 2: loss improved from 7.88498 to 2.72690, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 873s 145ms/step - loss: 2.7269 - bpp: 2.4437 - mse: 5.5315e-04\n",
      "Epoch 3/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 1.2420 - bpp: 0.8814 - mse: 7.0434e-04\n",
      "Epoch 3: loss improved from 2.72690 to 1.24202, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 884s 147ms/step - loss: 1.2420 - bpp: 0.8814 - mse: 7.0434e-04\n",
      "Epoch 4/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.8053 - bpp: 0.4733 - mse: 6.4828e-04\n",
      "Epoch 4: loss improved from 1.24202 to 0.80526, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 881s 147ms/step - loss: 0.8053 - bpp: 0.4733 - mse: 6.4828e-04\n",
      "Epoch 5/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.6545 - bpp: 0.3318 - mse: 6.3041e-04\n",
      "Epoch 5: loss improved from 0.80526 to 0.65453, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 885s 147ms/step - loss: 0.6545 - bpp: 0.3318 - mse: 6.3041e-04\n",
      "Epoch 6/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.5575 - bpp: 0.2711 - mse: 5.5950e-04\n",
      "Epoch 6: loss improved from 0.65453 to 0.55752, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 882s 147ms/step - loss: 0.5575 - bpp: 0.2711 - mse: 5.5950e-04\n",
      "Epoch 7/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.5060 - bpp: 0.2434 - mse: 5.1285e-04\n",
      "Epoch 7: loss improved from 0.55752 to 0.50599, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 876s 146ms/step - loss: 0.5060 - bpp: 0.2434 - mse: 5.1285e-04\n",
      "Epoch 8/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.4826 - bpp: 0.2295 - mse: 4.9429e-04\n",
      "Epoch 8: loss improved from 0.50599 to 0.48259, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 877s 146ms/step - loss: 0.4826 - bpp: 0.2295 - mse: 4.9429e-04\n",
      "Epoch 9/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.4635 - bpp: 0.2209 - mse: 4.7380e-04\n",
      "Epoch 9: loss improved from 0.48259 to 0.46351, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 877s 146ms/step - loss: 0.4635 - bpp: 0.2209 - mse: 4.7380e-04\n",
      "Epoch 10/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.4597 - bpp: 0.2183 - mse: 4.7138e-04\n",
      "Epoch 10: loss improved from 0.46351 to 0.45968, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 876s 146ms/step - loss: 0.4597 - bpp: 0.2183 - mse: 4.7138e-04\n",
      "Epoch 11/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.4482 - bpp: 0.2137 - mse: 4.5798e-04\n",
      "Epoch 11: loss improved from 0.45968 to 0.44818, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 878s 146ms/step - loss: 0.4482 - bpp: 0.2137 - mse: 4.5798e-04\n",
      "Epoch 12/12\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 0.4476 - bpp: 0.2144 - mse: 4.5529e-04\n",
      "Epoch 12: loss improved from 0.44818 to 0.44756, saving model to checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/\n",
      "6000/6000 [==============================] - 877s 146ms/step - loss: 0.4476 - bpp: 0.2144 - mse: 4.5529e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n",
      "WARNING:absl:Found untraced functions such as optical_flow_loss_9_layer_call_fn, optical_flow_loss_9_layer_call_and_return_conditional_losses, dwt_9_layer_call_fn, dwt_9_layer_call_and_return_conditional_losses, mc1_layer_call_fn while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_checkpoints_wavelets_haar_Lmbd_512_nfilt_256_epcs_12_stps_6000_I_QP_40_240x240_CosineDecay_20220603-080247/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compress\n",
      "decompress\n",
      "bin size:  463 psnr:  36.695193114730024 bpp:  0.002679398148148148\n"
     ]
    }
   ],
   "source": [
    "I_QP=40\n",
    "lmbda = 256*2\n",
    "np_folder = \"train_set_iqp40.npy\"\n",
    "checkponts_prev_path = \"\" \n",
    "checkpoints_target_path = \"\"\n",
    "trainer_15 = TrainOpenDVCW(BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH,\n",
    "                           Height, Width, Channel, NUM_FILTERS, MV_KERNEL_SIZE, RES_KERNEL_SIZE, M,\n",
    "                           lmbda, lr_init, lr_alpha, early_stop,\n",
    "                           I_QP, wavelet_name, checkponts_prev_path, checkpoints_target_path, np_folder)\n",
    "trainer_15.compile()\n",
    "trainer_15.fit()\n",
    "trainer_15.save()\n",
    "trainer_15.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compress\n",
      "decompress\n",
      "bin size:  3696 psnr:  38.98687920576718 bpp:  0.021388888888888888\n",
      "compress\n",
      "decompress\n",
      "bin size:  4517 psnr:  37.48376529958876 bpp:  0.026140046296296297\n",
      "compress\n",
      "decompress\n",
      "bin size:  1310 psnr:  37.64011589737207 bpp:  0.007581018518518518\n",
      "compress\n",
      "decompress\n",
      "bin size:  1744 psnr:  35.86676043184944 bpp:  0.010092592592592592\n",
      "compress\n",
      "decompress\n",
      "bin size:  463 psnr:  36.695214784876825 bpp:  0.002679398148148148\n"
     ]
    }
   ],
   "source": [
    "trainer_11.test(i_frame, p_frame, out_bin, out_decom)\n",
    "trainer_12.test(i_frame, p_frame, out_bin, out_decom)\n",
    "trainer_13.test(i_frame, p_frame, out_bin, out_decom)\n",
    "trainer_14.test(i_frame, p_frame, out_bin, out_decom)\n",
    "trainer_15.test(i_frame, p_frame, out_bin, out_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
